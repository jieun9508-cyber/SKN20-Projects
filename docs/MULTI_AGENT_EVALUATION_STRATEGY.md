# 시스템 아키텍처 평가 - 멀티에이전트 전략 가이드

## 1. 현재 시스템 분석

### 현재 구조
```
사용자 아키텍처 설계
       ↓
   설명 입력
       ↓
질문 생성 (3개 영역)
       ↓
   답변 수집
       ↓
단일 LLM 평가 (gpt-4o-mini)
       ↓
    결과 출력
```

### 6대 평가 지표 (Pillars)
| 지표 | 영문명 | txt 파일 |
|------|--------|----------|
| 신뢰성 | Reliability | 신뢰성.txt |
| 최적화 | Performance | 최적화.txt |
| 운영 | Operational Excellence | 운영유용성.txt |
| 비용 | Cost Optimization | 비용.txt |
| 보안 | Security | 보안.txt |
| 지속가능성 | Sustainability | 지속가능성.txt |

### 현재 방식의 한계
- **단일 관점**: 하나의 LLM이 모든 지표를 한번에 평가 → 깊이 부족
- **Context 제한**: 6개 원칙을 한 프롬프트에 담으면 품질 저하
- **일관성 문제**: 같은 답변에 대해 평가 결과가 달라질 수 있음

---

## 2. 멀티에이전트 아키텍처 옵션

### 옵션 A: 병렬 전문가 에이전트 (Parallel Specialist Agents)

```
                    ┌─────────────────┐
                    │  Orchestrator   │
                    │   (조율 에이전트)  │
                    └────────┬────────┘
                             │
        ┌────────┬───────┬───┴───┬───────┬────────┐
        ▼        ▼       ▼       ▼       ▼        ▼
   ┌────────┐┌────────┐┌────────┐┌────────┐┌────────┐┌────────┐
   │ 신뢰성  ││ 성능   ││ 운영   ││ 비용   ││ 보안   ││지속가능│
   │ Agent  ││ Agent  ││ Agent  ││ Agent  ││ Agent  ││ Agent  │
   └────┬───┘└────┬───┘└────┬───┘└────┬───┘└────┬───┘└────┬───┘
        │        │       │       │       │        │
        └────────┴───────┴───┬───┴───────┴────────┘
                             ▼
                    ┌─────────────────┐
                    │   Aggregator    │
                    │  (종합 에이전트)  │
                    └─────────────────┘
```

**장점**
- 각 지표별 전문성 확보 (System Prompt 최적화)
- 병렬 처리로 속도 향상
- 모듈화로 개별 지표 튜닝 용이

**단점**
- API 호출 6배 증가 → 비용 상승
- 지표 간 상충 관계 파악 어려움

**구현 복잡도**: ★★☆☆☆

---

### 옵션 B: 토론/피드백 기반 멀티에이전트 (Debate-style Multi-Agent)

```
    ┌─────────────────────────────────────────────────┐
    │             Round 1: 초기 평가                   │
    │  ┌────────┐        ┌────────┐        ┌────────┐ │
    │  │AWS-SA  │        │GCP-SA  │        │Azure-SA│ │
    │  │Persona │        │Persona │        │Persona │ │
    │  └────┬───┘        └────┬───┘        └────┬───┘ │
    └───────┼────────────────┼────────────────┼──────┘
            ▼                ▼                ▼
    ┌─────────────────────────────────────────────────┐
    │             Round 2: 상호 비평                   │
    │  "AWS 관점에서 GCP 평가의 약점 지적"              │
    │  "GCP 관점에서 Azure 평가 보완"                  │
    └───────────────────────┬─────────────────────────┘
                            ▼
    ┌─────────────────────────────────────────────────┐
    │             Round 3: 합의 도출                   │
    │         ┌───────────────────────┐               │
    │         │    Moderator Agent    │               │
    │         │  (다양한 의견 종합)     │               │
    │         └───────────────────────┘               │
    └─────────────────────────────────────────────────┘
```

**장점**
- 다양한 관점에서 검증 → 편향 감소
- 평가의 신뢰성/깊이 향상
- 실제 기업 리뷰 프로세스와 유사

**단점**
- 가장 높은 API 비용 (3+ 라운드)
- 처리 시간 길어짐
- 합의 실패 시 처리 로직 필요

**구현 복잡도**: ★★★★☆

---

### 옵션 C: 계층적 에이전트 (Hierarchical Multi-Agent)

```
                    ┌─────────────────┐
                    │  Chief Architect│ ← 최종 의사결정
                    │    (Level 3)    │
                    └────────┬────────┘
                             │
           ┌─────────────────┼─────────────────┐
           ▼                 ▼                 ▼
    ┌─────────────┐   ┌─────────────┐   ┌─────────────┐
    │ Reliability │   │ Performance │   │  Security   │
    │   Lead      │   │    Lead     │   │    Lead     │
    │  (Level 2)  │   │  (Level 2)  │   │  (Level 2)  │
    └──────┬──────┘   └──────┬──────┘   └──────┬──────┘
           │                 │                 │
     ┌─────┴─────┐     ┌─────┴─────┐     ┌─────┴─────┐
     ▼           ▼     ▼           ▼     ▼           ▼
┌────────┐ ┌────────┐ ┌────────┐ ┌────────┐ ┌────────┐ ┌────────┐
│HA 분석 │ │DR 분석 │ │캐싱분석│ │스케일링│ │암호화  │ │접근제어│
│Worker  │ │Worker  │ │Worker  │ │ Worker │ │ Worker │ │ Worker │
│(Lv 1)  │ │(Lv 1)  │ │(Lv 1)  │ │(Lv 1)  │ │(Lv 1)  │ │(Lv 1)  │
└────────┘ └────────┘ └────────┘ └────────┘ └────────┘ └────────┘
```

**장점**
- 세분화된 전문성 (매우 상세한 평가)
- 상위 에이전트가 하위 결과 검증
- 확장성 좋음

**단점**
- 가장 복잡한 구현
- 많은 API 호출
- 과도한 엔지니어링 위험

**구현 복잡도**: ★★★★★

---

### 옵션 D: 순차적 파이프라인 에이전트 (Sequential Pipeline)

```
┌────────────┐   ┌────────────┐   ┌────────────┐
│  분석 단계  │ → │  평가 단계  │ → │  검증 단계  │
└────────────┘   └────────────┘   └────────────┘
      │               │                │
      ▼               ▼                ▼
 아키텍처 분석    6개 지표 평가      결과 검증
 컴포넌트 식별    점수 산정         일관성 체크
 데이터 흐름      피드백 생성       최종 보정
```

**장점**
- 구현 단순
- 각 단계 출력이 다음 단계 입력
- 디버깅 용이

**단점**
- 순차 실행으로 속도 느림
- 단계 간 정보 손실 가능

**구현 복잡도**: ★★☆☆☆

---

### 옵션 E: 리뷰어-리바이저 패턴 (Reviewer-Reviser Pattern) ⭐ 추천

```
┌─────────────────────────────────────────────────────────────┐
│                        Phase 1: 초기 평가                    │
│  ┌─────────────────────────────────────────────────────┐    │
│  │              Primary Evaluator                       │    │
│  │  (6개 지표 동시 평가, 모범답안 포함)                    │    │
│  └─────────────────────────────────────────────────────┘    │
└─────────────────────────────┬───────────────────────────────┘
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                        Phase 2: 품질 검증                    │
│  ┌─────────────────────────────────────────────────────┐    │
│  │              Quality Reviewer                        │    │
│  │  - 평가 일관성 검증                                    │    │
│  │  - 점수-피드백 정합성 확인                              │    │
│  │  - 누락된 관점 보완                                    │    │
│  └─────────────────────────────────────────────────────┘    │
└─────────────────────────────┬───────────────────────────────┘
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                     Phase 3: 최종 조정 (선택적)               │
│  ┌─────────────────────────────────────────────────────┐    │
│  │              Final Adjuster (조건부)                  │    │
│  │  - Reviewer가 문제 발견 시에만 실행                     │    │
│  │  - 점수 보정 및 피드백 개선                             │    │
│  └─────────────────────────────────────────────────────┘    │
└─────────────────────────────────────────────────────────────┘
```

**장점**
- 균형 잡힌 비용/품질 트레이드오프
- 기존 시스템에서 점진적 확장 가능
- 품질 문제 시에만 추가 비용 발생

**단점**
- 토론 방식보다 다양성 낮음

**구현 복잡도**: ★★★☆☆

---

## 3. 옵션 비교표

| 기준 | A. 병렬 전문가 | B. 토론형 | C. 계층형 | D. 순차 파이프라인 | E. 리뷰어-리바이저 |
|------|---------------|----------|----------|------------------|-------------------|
| **API 호출 수** | 6-7회 | 9-15회 | 10-20회 | 3회 | 2-3회 |
| **평가 깊이** | ★★★★☆ | ★★★★★ | ★★★★★ | ★★★☆☆ | ★★★★☆ |
| **속도** | 빠름 (병렬) | 느림 | 매우 느림 | 보통 | 빠름~보통 |
| **비용** | 중간 | 높음 | 매우 높음 | 낮음 | 낮음~중간 |
| **구현 난이도** | 낮음 | 높음 | 매우 높음 | 낮음 | 중간 |
| **확장성** | 좋음 | 보통 | 좋음 | 보통 | 좋음 |

---

## 4. 추가 고려 방식

### 옵션 F: 앙상블 평가 (Ensemble Evaluation)

```
    동일 입력 → [LLM A] [LLM B] [LLM C] → 통계적 종합
                 (gpt-4o) (claude) (gemini)
```

- 다른 모델들의 평가 결과를 앙상블
- 모델별 편향 상쇄

### 옵션 G: Self-Consistency 기반

```
    동일 입력 → [LLM × 3회] → 다수결 / 평균
              (temperature 다르게)
```

- 같은 모델을 여러 번 호출하여 일관성 확보
- 비용 효율적 품질 향상

### 옵션 H: RAG + Agent 하이브리드

```
    ┌──────────────┐     ┌──────────────┐
    │  Knowledge   │ ←→  │   Evaluator  │
    │    Base      │     │    Agent     │
    │ (벡터 DB)    │     │              │
    └──────────────┘     └──────────────┘
```

- 평가 기준/예시를 RAG로 검색
- 더 근거 있는 평가 가능

---

## 5. 핵심 질문에 대한 답변

### Q1: "멀티에이전트를 쓰는 게 맞나?"

**결론: 상황에 따라 다름**

| 상황 | 추천 |
|------|------|
| MVP/빠른 개발 | 단일 에이전트 (현재 방식) |
| 평가 품질이 핵심 | 멀티에이전트 (옵션 E 추천) |
| 예산 여유 있음 | 토론형 (옵션 B) |
| 상세한 세부 평가 필요 | 병렬 전문가 (옵션 A) |

### Q2: "평가에 에이전트를 여러 개 둘 수 있는지?"

**가능합니다.** 평가 태스크에서 멀티에이전트가 효과적인 경우:

1. **복잡한 평가 기준**: 6개 지표처럼 다차원 평가
2. **전문성 필요**: 각 지표별 깊은 분석
3. **신뢰성 중요**: 단일 평가의 편향 방지
4. **교육적 목적**: 다양한 관점 제시

### Q3: "어떤 방식으로 진행하면 좋을지?"

**단계적 접근 추천:**

```
Phase 1 (현재 → 1주)
├── 옵션 E(리뷰어-리바이저) 구현
├── 2단계 평가: Evaluator → Reviewer
└── 기존 코드 최소 수정

Phase 2 (2-3주 후)
├── 옵션 A(병렬 전문가) 실험
├── 6개 지표별 전문 에이전트 구성
└── 결과 비교 분석

Phase 3 (품질 검증 후)
├── A/B 테스트로 최적 방식 선택
└── 토론형 도입 검토 (필요 시)
```

---

## 6. 추천 아키텍처: 옵션 E 상세 설계

### 왜 옵션 E를 추천하는가?

1. **점진적 확장**: 현재 단일 에이전트에서 자연스럽게 확장
2. **비용 효율**: 문제 발견 시에만 추가 호출
3. **구현 용이**: 기존 코드 구조 유지 가능
4. **품질 보장**: 리뷰 단계에서 일관성 검증

### 구현 구조

```javascript
// Phase 1: Primary Evaluator (기존 로직 활용)
async function primaryEvaluate(context) {
  const result = await evaluateWithMasterAgent(...);
  return {
    scores: result.questionEvaluations,
    overall: result.overallScore,
    needsReview: checkConsistency(result)
  };
}

// Phase 2: Quality Reviewer
async function reviewEvaluation(primaryResult, context) {
  const reviewPrompt = `
    당신은 평가 품질 검토 전문가입니다.

    [원본 평가 결과]
    ${JSON.stringify(primaryResult)}

    검토 항목:
    1. 점수와 피드백의 일관성
    2. 누락된 평가 관점
    3. 모범답안의 적절성

    문제점과 개선사항을 JSON으로 반환하세요.
  `;

  const review = await callOpenAI(reviewPrompt);
  return {
    hasIssues: review.issues.length > 0,
    suggestions: review.suggestions
  };
}

// Phase 3: Final Adjuster (조건부)
async function adjustEvaluation(primaryResult, reviewResult) {
  if (!reviewResult.hasIssues) return primaryResult;

  // 문제 있을 때만 보정
  const adjustPrompt = `
    [원본 평가]
    ${JSON.stringify(primaryResult)}

    [리뷰어 피드백]
    ${JSON.stringify(reviewResult.suggestions)}

    피드백을 반영하여 평가를 개선하세요.
  `;

  return await callOpenAI(adjustPrompt);
}
```

### 예상 효과

| 지표 | 현재 | 옵션 E 적용 후 |
|------|------|---------------|
| 평가 일관성 | 70% | 90%+ |
| API 호출 | 1회 | 2-3회 |
| 평균 응답시간 | 3초 | 5-7초 |
| 월 비용 증가 | - | +50~100% |

---

## 7. 다음 단계

### 즉시 실행 가능한 작업

1. **옵션 E 프로토타입 구현**
   - `architectureApiMasterAgent.js`에 리뷰어 단계 추가
   - 일관성 검사 로직 구현

2. **평가 품질 측정 기준 수립**
   - 전문가 평가와 LLM 평가 비교
   - 재현성(같은 입력에 같은 결과) 테스트

3. **비용/성능 모니터링**
   - API 호출 로깅
   - 응답 시간 측정

### 결정이 필요한 사항

- [ ] 타겟 평가 품질 수준 (현재 대비 몇 % 향상 목표?)
- [ ] 허용 가능한 추가 비용 범위
- [ ] 응답 시간 제약 (최대 몇 초?)
- [ ] 옵션 A(병렬) vs E(리뷰어) 우선 실험 대상

---

## 8. 참고 자료

### 멀티에이전트 프레임워크

| 프레임워크 | 특징 | 적합도 |
|-----------|------|--------|
| LangGraph | 그래프 기반 워크플로우 | ★★★★★ |
| AutoGen | MS, 대화형 에이전트 | ★★★★☆ |
| CrewAI | 역할 기반 팀 구성 | ★★★★☆ |
| Swarm | OpenAI, 경량 | ★★★☆☆ |

### 관련 논문/블로그

- "Multi-Agent Debate for Code Generation" (Google DeepMind)
- "Constitutional AI: Self-Improvement" (Anthropic)
- "LLM-as-a-Judge" 패턴

---

*문서 작성일: 2026-02-06*
*작성자: Claude Code*
