● 정확한 문제 진단입니다. 코드를 확인한 결과를 정리하겠습니다.                                   
  
  ---                                                                                              현재 구조의 문제점
                                                                                                 
  architectureApiFastTest.js:127-219 에서 단일 LLM이 한 번의 호출로:

  1. 부족한 영역 3가지 선택 (gaps_analysis)
  2. 각 영역에 대한 질문 생성 (questions)

  이 두 가지를 동시에 하고 있어서:

  같은 아키텍처 + 같은 설명 입력
      ↓
  LLM이 "오늘은 보안/성능/운영이 부족하다" 판단  ← 여기서 이미 갈림
      ↓
  다음번엔 "신뢰성/비용/보안이 부족하다" 판단     ← 매번 다른 질문

  게다가 temperature: 0.7이라 비결정성이 높습니다.

  ---
  제안하신 멀티 에이전트 질문 생성 방식 분석

  제안: 에이전트 N개 → 각자 기준 1개 → 질문 1개씩 → 합산

  이 방식은 좋은데, 한 가지 아직 해결이 안 되는 부분이 있습니다:

  **"어떤 3개 기둥을 선택할 것인가?"**를 누가 결정하나?

  여기에 3가지 전략이 있습니다:

  전략 1: Gap Analyzer 분리 → 전문 Agent 질문 생성 (2단계)

  [1단계] Gap Analyzer Agent (temp=0, 분석만)
      "이 아키텍처에서 부족한 기둥 3개를 골라라"
      → 결과: ["보안", "신뢰성", "비용"]

  [2단계] 3개의 Specialist Agent (병렬, Promise.all)
      보안 Agent  → "보안 원칙 기준으로 질문 1개 생성"
      신뢰성 Agent → "신뢰성 원칙 기준으로 질문 1개 생성"
      비용 Agent  → "비용 원칙 기준으로 질문 1개 생성"

  [합산] 3개 질문 조합 → 출제

  - 장점: 분석 역할과 질문 생성 역할 분리, 각 Agent가 자기 기둥 원칙만 집중
  - 단점: Gap Analyzer도 LLM이라 선택이 달라질 수 있음 (하지만 temp=0으로 완화 가능)
  - API 호출: 1 + 3 = 4회 (병렬이면 체감 2회)

  전략 2: 규칙 기반 선택 + Agent 질문 생성 (가장 재현 가능)

  [1단계] Rule-based Gap Detector (LLM 아님)
      - architecture.json의 rubric_functional/rubric_non_functional 기반
      - 사용자가 배치한 컴포넌트 vs 필수 컴포넌트 비교
      - 언급된 키워드 vs 6대 기둥 키워드 매칭
      → 결정론적으로 부족한 3개 기둥 선택

  [2단계] 3개 Specialist Agent (병렬)
      각자 자기 기둥 원칙으로 질문 1개 생성

  - 장점: 기둥 선택이 100% 결정론적 → 같은 입력 = 같은 3개 기둥 보장
  - 단점: 규칙 기반이라 유연성 떨어짐, 구현에 도메인 지식 필요
  - API 호출: 3회 (분석 단계 LLM 불필요)

  전략 3: 6개 전체 Agent → 상위 3개 선택

  [1단계] 6개 Agent 모두 병렬 실행
      각 Agent: "이 아키텍처에서 내 기둥 관점으로 부족한가? (0-100점)"
                + "부족하다면 질문 1개 생성"

  [2단계] 점수 하위 3개 (가장 부족한 3개) 선택 → 출제

  - 장점: 가장 공정하고 포괄적, 6개 관점 모두 평가
  - 단점: API 호출 6회 (비용 2배), 하지만 병렬이면 속도는 비슷
  - API 호출: 6회 (병렬이면 체감 1회)

  ---
  추천

  프로젝트의 핵심 목표인 "같은 질문 → 같은 답변 → 같은 점수" 재현성을 기준으로 보면:
  ┌──────────────────────┬──────────────────┬─────────────┬──────┬─────────────┐
  │         전략         │ 기둥 선택 재현성 │ 질문 재현성 │ 비용 │ 구현 난이도 │
  ├──────────────────────┼──────────────────┼─────────────┼──────┼─────────────┤
  │ 1. Gap Analyzer 분리 │ 중간 (temp=0)    │ 높음        │ 4회  │ 낮음        │
  ├──────────────────────┼──────────────────┼─────────────┼──────┼─────────────┤
  │ 2. 규칙 기반 + Agent │ 100%             │ 높음        │ 3회  │ 중간        │
  ├──────────────────────┼──────────────────┼─────────────┼──────┼─────────────┤
  │ 3. 6개 전체 Agent    │ 높음             │ 높음        │ 6회  │ 낮음        │
  └──────────────────────┴──────────────────┴─────────────┴──────┴─────────────┘
  전략 1 + 2 조합을 추천합니다:

  - 문제의 rubric_non_functional에 이미 카테고리가 명시되어 있음 (architecture.json)
  - 이걸 힌트로 활용 + 사용자 설명의 키워드 매칭으로 규칙 기반 1차 필터링
  - 애매한 경우에만 LLM Gap Analyzer 보조 → 하이브리드

  이 방향으로 구현을 진행할까요? 아니면 전략 3(6개 전부 돌리기)이 더 나을까요?