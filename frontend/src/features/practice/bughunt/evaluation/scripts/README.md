# Bug Hunt í‰ê°€ ì‹œìŠ¤í…œ ìŠ¤í¬ë¦½íŠ¸

Bug Hunt í‰ê°€ ì‹œìŠ¤í…œì˜ ê²€ì¦, ë¹„êµ, ë¶„ì„ì„ ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸ ëª¨ìŒì…ë‹ˆë‹¤.

## ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
data/scripts/
â”œâ”€â”€ validation/           # LLM í‰ê°€ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ model_comparison/     # ëª¨ë¸ ë¹„êµ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ prompt_comparison/    # í”„ë¡¬í”„íŠ¸ ë¹„êµ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ utils/               # ê³µí†µ ìœ í‹¸ë¦¬í‹°
```

## 1ï¸âƒ£ validation/ - LLM í‰ê°€ ê²€ì¦

**ëª©ì **: gpt-4o-mini í‰ê°€ì˜ ì‹ ë¢°ì„± ê²€ì¦

### ìŠ¤í¬ë¦½íŠ¸ ëª©ë¡

#### `generate_validation_samples.py`
60ê°œ ê²€ì¦ ìƒ˜í”Œ ìƒì„± (í’ˆì§ˆë³„ 12ê°œ)

```bash
python data/scripts/validation/generate_validation_samples.py
```

**ì¶œë ¥**: `data/validation/bughunt_validation/bug_hunt_validation_samples.json`

#### `run_evaluation.py`
LLM í‰ê°€ ì‹¤í–‰ (60ìƒ˜í”Œ Ã— 5íšŒ = 300íšŒ)

```bash
# Full ëª¨ë“œ (60ê°œ ìƒ˜í”Œ)
python data/scripts/validation/run_evaluation.py --trials 5

# Quick ëª¨ë“œ (5ê°œ ìƒ˜í”Œ, í…ŒìŠ¤íŠ¸ìš©)
python data/scripts/validation/run_evaluation.py --trials 5 --quick
```

**ì¶œë ¥**: `data/validation/evaluation_results.json`

#### `run_full_validation.py`
ì „ì²´ ê²€ì¦ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

```bash
python data/scripts/validation/run_full_validation.py
```

**ì‘ì—… ìˆœì„œ**:
1. ê²€ì¦ ìƒ˜í”Œ ìƒì„±
2. LLM í‰ê°€ ì‹¤í–‰
3. Rule-based í‰ê°€ ì‹¤í–‰
4. ê²°ê³¼ ë¶„ì„
5. ì‹œê°í™” ìƒì„±

#### `analyze_results.py`
í‰ê°€ ê²°ê³¼ ë¶„ì„ (ì¼ê´€ì„±, êµ¬ë¶„ë ¥, ìƒê´€ê³„ìˆ˜)

```bash
python data/scripts/validation/analyze_results.py
```

**ì¶œë ¥**:
- `data/validation/bughunt_validation/analysis_results.json`
- `data/validation/bughunt_validation/validation_summary.csv`

#### `visualize_results.py`
ë¶„ì„ ê²°ê³¼ ì‹œê°í™”

```bash
python data/scripts/validation/visualize_results.py
```

**ì¶œë ¥**: `data/validation/bughunt_validation/visualizations/`
- `consistency_boxplot.png` - ì¼ê´€ì„± ë°•ìŠ¤í”Œë¡¯
- `discrimination_barchart.png` - í’ˆì§ˆë³„ í‰ê·  ì ìˆ˜
- `correlation_scatter.png` - Rule-based ìƒê´€ê´€ê³„
- `summary_chart.png` - ì¢…í•© ìš”ì•½

## 2ï¸âƒ£ model_comparison/ - ëª¨ë¸ ë¹„êµ

**ëª©ì **: ìµœì  OpenAI ëª¨ë¸ ì„ ì •

### ìŠ¤í¬ë¦½íŠ¸ ëª©ë¡

#### `run_model_comparison.py`
ì—¬ëŸ¬ ëª¨ë¸ë¡œ ë™ì¼ ìƒ˜í”Œ í‰ê°€

```bash
# ëª¨ë“  ëª¨ë¸ ë¹„êµ
python data/scripts/model_comparison/run_model_comparison.py

# íŠ¹ì • ëª¨ë¸ë§Œ ì‹¤í–‰
python data/scripts/model_comparison/run_model_comparison.py --models gpt-4o-mini gpt-4o
```

**ë¹„êµ ëª¨ë¸**:
- gpt-4o-mini
- gpt-4o
- gpt-3.5-turbo

**ì¶œë ¥**: `data/validation/model_comparison/{model}_results.json`

#### `analyze_model_comparison.py`
ëª¨ë¸ ë¹„êµ ë¶„ì„ ë° ì¢…í•© ì ìˆ˜ ê³„ì‚°

```bash
python data/scripts/model_comparison/analyze_model_comparison.py
```

**ë¶„ì„ ì§€í‘œ**:
- ì¼ê´€ì„± (Consistency)
- êµ¬ë¶„ë ¥ (Discrimination)
- ìˆœìœ„ ì •í™•ë„ (Kendall's Tau)
- ì„±ëŠ¥ (ì†ë„, ë¹„ìš©)
- ì˜¤ë¥˜ìœ¨

**ì¶œë ¥**: `data/validation/model_comparison/model_comparison_analysis.json`

#### `visualize_model_comparison.py`
ëª¨ë¸ ë¹„êµ ì‹œê°í™”

```bash
python data/scripts/model_comparison/visualize_model_comparison.py
```

**ì¶œë ¥**: `data/validation/model_comparison/visualizations/`
- `consistency_comparison.png` - ì¼ê´€ì„± ë¹„êµ
- `discrimination_comparison.png` - êµ¬ë¶„ë ¥ ë¹„êµ
- `performance_comparison.png` - ì„±ëŠ¥ ë¹„êµ
- `radar_chart.png` - ë ˆì´ë” ì°¨íŠ¸
- `overall_ranking.png` - ì¢…í•© ìˆœìœ„

#### `model_evaluator.py`
ëª¨ë¸ í‰ê°€ ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤

**ìš©ë„**: ë‹¤ë¥¸ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ importí•˜ì—¬ ì‚¬ìš©

## 3ï¸âƒ£ prompt_comparison/ - í”„ë¡¬í”„íŠ¸ ë¹„êµ

**ëª©ì **: BEFORE vs AFTER í”„ë¡¬í”„íŠ¸ ê°œì„  íš¨ê³¼ ê²€ì¦

### ìŠ¤í¬ë¦½íŠ¸ ëª©ë¡

#### `compare_before_after.py`
BEFORE vs AFTER í”„ë¡¬í”„íŠ¸ ë¹„êµ ë¶„ì„

```bash
python data/scripts/prompt_comparison/compare_before_after.py
```

**ì…ë ¥**:
- BEFORE: `data/validation/bughunt_validation/evaluation_results.json`
- AFTER: `data/validation/prompt_comparison/after_results.json`

**ì¶œë ¥**: `data/validation/prompt_comparison/comparison_analysis.json`

**ë¹„êµ ì§€í‘œ**:
- ì¼ê´€ì„± ë³€í™”ìœ¨
- êµ¬ë¶„ë ¥ í–¥ìƒ
- ì ìˆ˜ ë¶„í¬ ë³€í™”
- í’ˆì§ˆë³„ í‰ê·  ì ìˆ˜ ë³€í™”

## 4ï¸âƒ£ utils/ - ê³µí†µ ìœ í‹¸ë¦¬í‹°

### ìŠ¤í¬ë¦½íŠ¸ ëª©ë¡

#### `rule_based_scorer.py`
Rule-based í‰ê°€ ì‹œìŠ¤í…œ

**í‰ê°€ ê¸°ì¤€**:
1. ì›ì¸ ë¶„ì„ ì •í™•ë„ (30ì )
2. ì½”ë“œ ìˆ˜ì • ì ì ˆì„± (25ì )
3. ì„¤ëª… ì™„ì„±ë„ (20ì )
4. ë…¼ë¦¬ì  ì¼ê´€ì„± (15ì )
5. ì „ë¬¸ ìš©ì–´ ì‚¬ìš© (10ì )

```bash
python data/scripts/utils/rule_based_scorer.py
```

#### `generate_pseudocode.py`
ê²€ì¦ ìƒ˜í”Œìš© ì˜ì‚¬ì½”ë“œ ìƒì„±

```bash
python data/scripts/utils/generate_pseudocode.py
```

#### `collect_data.py`
ë°ì´í„° ìˆ˜ì§‘ ìœ í‹¸ë¦¬í‹°

```bash
python data/scripts/utils/collect_data.py
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. LLM í‰ê°€ ê²€ì¦

```bash
# ì „ì²´ ê²€ì¦ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (1ì‹œê°„ ì†Œìš”)
python data/scripts/validation/run_full_validation.py

# ë˜ëŠ” ë‹¨ê³„ë³„ ì‹¤í–‰
python data/scripts/validation/generate_validation_samples.py
python data/scripts/validation/run_evaluation.py --trials 5
python data/scripts/validation/analyze_results.py
python data/scripts/validation/visualize_results.py
```

### 2. ëª¨ë¸ ë¹„êµ

```bash
# ëª¨ë¸ ë¹„êµ ì‹¤í–‰ (3-4ì‹œê°„ ì†Œìš”)
python data/scripts/model_comparison/run_model_comparison.py

# ë¶„ì„ ë° ì‹œê°í™”
python data/scripts/model_comparison/analyze_model_comparison.py
python data/scripts/model_comparison/visualize_model_comparison.py
```

### 3. í”„ë¡¬í”„íŠ¸ ë¹„êµ

```bash
# BEFORE vs AFTER ë¹„êµ
python data/scripts/prompt_comparison/compare_before_after.py
```

## ğŸ“Š ì£¼ìš” ê²°ê³¼ ìš”ì•½

### LLM í‰ê°€ ê²€ì¦ (gpt-4o-mini)
- âœ… ì¼ê´€ì„±: 0.92ì  í‘œì¤€í¸ì°¨
- âœ… Rule-based ìƒê´€ê³„ìˆ˜: 0.85
- âŒ êµ¬ë¶„ë ¥: Excellent = Good = 70ì 

### ëª¨ë¸ ë¹„êµ
- ğŸ† **ìµœì¢… ì„ ì •: gpt-4o-mini**
- ì´ìœ : ì¼ê´€ì„± 1ìœ„ (0.83ì ) + ë¹„ìš© íš¨ìœ¨ì„± 19ë°°

### í”„ë¡¬í”„íŠ¸ ê°œì„  (AFTER)
- âœ… êµ¬ë¶„ë ¥: +7.14ì 
- âœ… ì ìˆ˜ ë²”ìœ„: +22ì 
- âŒ ì¼ê´€ì„±: -169.6% (ê°œì„  í•„ìš”)

## ğŸ”§ ê°œë°œ ê°€ì´ë“œ

### ìƒˆ ìŠ¤í¬ë¦½íŠ¸ ì¶”ê°€ ì‹œ

1. ì ì ˆí•œ ë””ë ‰í† ë¦¬ì— ë°°ì¹˜
2. Docstringìœ¼ë¡œ ëª©ì ê³¼ ì‚¬ìš©ë²• ëª…ì‹œ
3. argparseë¡œ CLI ì¸í„°í˜ì´ìŠ¤ ì œê³µ
4. ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
5. README ì—…ë°ì´íŠ¸

### ì½”ë”© ì»¨ë²¤ì…˜

```python
"""
ìŠ¤í¬ë¦½íŠ¸ ì„¤ëª…

ì£¼ìš” ê¸°ëŠ¥ê³¼ ì‚¬ìš© ë°©ë²• ì„¤ëª…
"""
import json
from pathlib import Path

class AnalyzerName:
    """ë¶„ì„ê¸° í´ë˜ìŠ¤ ì„¤ëª…"""

    def __init__(self, input_file):
        """ì´ˆê¸°í™”"""
        pass

    def analyze(self):
        """ë¶„ì„ ì‹¤í–‰"""
        pass

if __name__ == "__main__":
    # CLI ì‹¤í–‰ ì½”ë“œ
    pass
```

## ğŸ“š ê´€ë ¨ ë¬¸ì„œ

- [Validation README](validation/README_VALIDATION.md)
- [Model Comparison README](model_comparison/README_MODEL_COMPARISON.md)
- [Prompt Comparison Guide](prompt_comparison/PROMPT_COMPARISON.md)
- [Main Validation Data](../../data/validation/README.md)

## ğŸ› ë¬¸ì œ í•´ê²°

### ì˜¤ë¥˜: "Module not found"
```bash
# Django í™˜ê²½ì—ì„œ ì‹¤í–‰ í•„ìš”
cd data
python scripts/validation/run_evaluation.py
```

### ì˜¤ë¥˜: "File not found"
```bash
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰
cd c:/SKN20-FINAL-5TEAM
python data/scripts/validation/run_evaluation.py
```

### Dockerì—ì„œ ì‹¤í–‰
```bash
docker exec skn20-final-5team-backend-1 python scripts/validation/run_evaluation.py
```
