"""
ëª¨ë¸ ë¹„êµ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

ì—¬ëŸ¬ ëª¨ë¸ì˜ í‰ê°€ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ë¹„êµ ì§€í‘œ ì‚°ì¶œ
"""
import json
import numpy as np
from pathlib import Path
from scipy import stats


class ModelComparisonAnalyzer:
    """ëª¨ë¸ ë¹„êµ ë¶„ì„ê¸°"""

    def __init__(self, results_file):
        self.results_file = results_file
        with open(results_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.metadata = data['metadata']
            self.model_results = data['model_results']

    def extract_scores(self, model_name):
        """ëª¨ë¸ë³„ ì ìˆ˜ ì¶”ì¶œ"""
        model_data = self.model_results[model_name]
        scores = []
        quality_scores = {'excellent': [], 'good': [], 'average': [], 'poor': [], 'very_poor': []}

        for sample_result in model_data['results']:
            quality = sample_result['quality_level']
            for trial in sample_result['trials']:
                if 'error' not in trial:
                    score = trial['thinking_score']
                    scores.append(score)
                    quality_scores[quality].append(score)

        return scores, quality_scores

    def calculate_consistency(self, model_name):
        """ì¼ê´€ì„± ë¶„ì„: ë™ì¼ ìƒ˜í”Œ ë°˜ë³µ í‰ê°€ì˜ í‘œì¤€í¸ì°¨"""
        model_data = self.model_results[model_name]
        std_devs = []

        for sample_result in model_data['results']:
            sample_scores = []
            for trial in sample_result['trials']:
                if 'error' not in trial:
                    sample_scores.append(trial['thinking_score'])

            if len(sample_scores) > 1:
                std_devs.append(np.std(sample_scores))

        return {
            'avg_std_dev': np.mean(std_devs) if std_devs else 0,
            'max_std_dev': max(std_devs) if std_devs else 0,
            'min_std_dev': min(std_devs) if std_devs else 0
        }

    def calculate_discrimination(self, model_name):
        """ê·¹ë‹¨ ì¼€ì´ìŠ¤ êµ¬ë¶„ë ¥: ìš°ìˆ˜ vs ë§¤ìš° ë¯¸í¡ ì ìˆ˜ ì°¨ì´"""
        _, quality_scores = self.extract_scores(model_name)

        excellent_avg = np.mean(quality_scores['excellent']) if quality_scores['excellent'] else 0
        very_poor_avg = np.mean(quality_scores['very_poor']) if quality_scores['very_poor'] else 0

        return {
            'excellent_avg': excellent_avg,
            'very_poor_avg': very_poor_avg,
            'score_diff': excellent_avg - very_poor_avg,
            'quality_means': {q: np.mean(scores) if scores else 0 for q, scores in quality_scores.items()}
        }

    def calculate_ranking_accuracy(self, model_name):
        """ìˆœìœ„ ì •í™•ë„: Kendall's Tau"""
        _, quality_scores = self.extract_scores(model_name)

        # í’ˆì§ˆ ë ˆë²¨ë³„ í‰ê·  ì ìˆ˜
        quality_order = ['excellent', 'good', 'average', 'poor', 'very_poor']
        actual_means = [np.mean(quality_scores[q]) if quality_scores[q] else 0 for q in quality_order]

        # ì˜ˆìƒ ìˆœìœ„ (ë†’ì€ ì ìˆ˜ë¶€í„° ë‚®ì€ ì ìˆ˜)
        expected_ranks = [5, 4, 3, 2, 1]  # excellent=5, very_poor=1
        actual_ranks = stats.rankdata(actual_means)

        # Kendall's Tau
        tau, p_value = stats.kendalltau(expected_ranks, actual_ranks)

        return {
            'kendall_tau': tau,
            'p_value': p_value,
            'actual_means': dict(zip(quality_order, actual_means))
        }

    def calculate_performance_metrics(self, model_name):
        """ì„±ëŠ¥ ì§€í‘œ: ë¹„ìš©, ì†ë„, í† í°"""
        model_data = self.model_results[model_name]
        stats_data = model_data.get('stats', {})

        times = []
        costs = []

        for sample_result in model_data['results']:
            for trial in sample_result['trials']:
                if 'error' not in trial:
                    times.append(trial.get('time', 0))
                    costs.append(trial.get('cost', 0))

        return {
            'avg_time': np.mean(times) if times else 0,
            'total_cost': sum(costs),
            'total_tokens': stats_data.get('total_tokens', 0),
            'total_evaluations': len(times)
        }

    def calculate_error_rate(self, model_name):
        """ì˜¤ë¥˜ìœ¨ ê³„ì‚°"""
        model_data = self.model_results[model_name]
        total = 0
        errors = 0

        for sample_result in model_data['results']:
            for trial in sample_result['trials']:
                total += 1
                if 'error' in trial:
                    errors += 1

        return {
            'error_count': errors,
            'total_count': total,
            'error_rate': (errors / total * 100) if total > 0 else 0
        }

    def analyze_all_models(self):
        """ëª¨ë“  ëª¨ë¸ ë¶„ì„"""
        analysis = {}

        for model_name in self.model_results.keys():
            print(f"ğŸ“Š ë¶„ì„ ì¤‘: {model_name}")

            analysis[model_name] = {
                'consistency': self.calculate_consistency(model_name),
                'discrimination': self.calculate_discrimination(model_name),
                'ranking': self.calculate_ranking_accuracy(model_name),
                'performance': self.calculate_performance_metrics(model_name),
                'error_rate': self.calculate_error_rate(model_name)
            }

        return analysis

    def create_comparison_summary(self, analysis):
        """ë¹„êµ ìš”ì•½ ìƒì„±"""
        summary = {
            'models': list(analysis.keys()),
            'comparison': {}
        }

        # ê° ì§€í‘œë³„ ìˆœìœ„
        metrics = {
            'consistency': {model: analysis[model]['consistency']['avg_std_dev'] for model in analysis},
            'discrimination': {model: analysis[model]['discrimination']['score_diff'] for model in analysis},
            'ranking_accuracy': {model: analysis[model]['ranking']['kendall_tau'] for model in analysis},
            'speed': {model: analysis[model]['performance']['avg_time'] for model in analysis},
            'cost': {model: analysis[model]['performance']['total_cost'] for model in analysis},
            'error_rate': {model: analysis[model]['error_rate']['error_rate'] for model in analysis}
        }

        # ìˆœìœ„ ê³„ì‚° (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ: consistency, speed, cost, error_rate / ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ: discrimination, ranking_accuracy)
        for metric, values in metrics.items():
            if metric in ['consistency', 'speed', 'cost', 'error_rate']:
                # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (ì˜¤ë¦„ì°¨ìˆœ)
                sorted_models = sorted(values.items(), key=lambda x: x[1])
            else:
                # ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ (ë‚´ë¦¼ì°¨ìˆœ)
                sorted_models = sorted(values.items(), key=lambda x: x[1], reverse=True)

            summary['comparison'][metric] = {
                'values': values,
                'ranking': [model for model, _ in sorted_models],
                'best': sorted_models[0][0],
                'worst': sorted_models[-1][0]
            }

        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ê° ì§€í‘œë¥¼ ì •ê·œí™”í•˜ì—¬ í•©ì‚°)
        model_scores = {model: 0 for model in analysis}

        for metric, data in summary['comparison'].items():
            values = list(data['values'].values())
            min_val = min(values)
            max_val = max(values)
            range_val = max_val - min_val if max_val > min_val else 1

            for model in analysis:
                value = data['values'][model]

                if metric in ['consistency', 'speed', 'cost', 'error_rate']:
                    # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (ì—­ì •ê·œí™”)
                    normalized = 1 - (value - min_val) / range_val
                else:
                    # ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
                    normalized = (value - min_val) / range_val

                # ê°€ì¤‘ì¹˜ ì ìš©
                weights = {
                    'consistency': 2.0,
                    'discrimination': 2.0,
                    'ranking_accuracy': 1.5,
                    'speed': 1.0,
                    'cost': 1.5,
                    'error_rate': 2.0
                }

                model_scores[model] += normalized * weights.get(metric, 1.0)

        # ì¢…í•© ìˆœìœ„
        sorted_scores = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)
        summary['overall_ranking'] = {
            'scores': model_scores,
            'ranking': [model for model, _ in sorted_scores],
            'best_model': sorted_scores[0][0],
            'best_score': sorted_scores[0][1]
        }

        return summary

    def generate_report(self, output_file):
        """ì „ì²´ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        print("\nğŸ” ëª¨ë¸ ë¹„êµ ë¶„ì„ ì‹œì‘...")

        analysis = self.analyze_all_models()
        summary = self.create_comparison_summary(analysis)

        report = {
            'metadata': self.metadata,
            'analysis': analysis,
            'summary': summary
        }

        # ì €ì¥
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“ ì €ì¥: {output_file}")

        # ìš”ì•½ ì¶œë ¥
        print(f"\n{'='*60}")
        print(f"ğŸ“Š ëª¨ë¸ ë¹„êµ ìš”ì•½")
        print(f"{'='*60}")

        for model in summary['models']:
            print(f"\nğŸ¤– {model}")
            print(f"  ì¼ê´€ì„± (í‘œì¤€í¸ì°¨): {analysis[model]['consistency']['avg_std_dev']:.2f}")
            print(f"  êµ¬ë¶„ë ¥ (ì ìˆ˜ ì°¨ì´): {analysis[model]['discrimination']['score_diff']:.2f}")
            print(f"  ìˆœìœ„ ì •í™•ë„ (Kendall Ï„): {analysis[model]['ranking']['kendall_tau']:.3f}")
            print(f"  í‰ê·  ì‘ë‹µ ì‹œê°„: {analysis[model]['performance']['avg_time']:.2f}s")
            print(f"  ì´ ë¹„ìš©: ${analysis[model]['performance']['total_cost']:.4f}")
            print(f"  ì˜¤ë¥˜ìœ¨: {analysis[model]['error_rate']['error_rate']:.1f}%")

        print(f"\n{'='*60}")
        print(f"ğŸ† ì¢…í•© ìˆœìœ„")
        print(f"{'='*60}")

        for idx, model in enumerate(summary['overall_ranking']['ranking'], 1):
            score = summary['overall_ranking']['scores'][model]
            print(f"{idx}. {model}: {score:.2f}ì ")

        print(f"\nâœ¨ ìµœìš°ìˆ˜ ëª¨ë¸: {summary['overall_ranking']['best_model']}")

        return report


if __name__ == "__main__":
    data_dir = Path(__file__).resolve().parent.parent.parent / 'data' / 'validation' / 'model_comparison'
    results_file = data_dir / 'model_comparison_results.json'
    output_file = data_dir / 'model_comparison_analysis.json'

    analyzer = ModelComparisonAnalyzer(results_file)
    analyzer.generate_report(output_file)
