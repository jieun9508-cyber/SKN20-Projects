"""
ë‹¤ì¤‘ ëª¨ë¸ ë¹„êµ í‰ê°€ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

ì—¬ëŸ¬ LLM ëª¨ë¸ì„ ë™ì¼í•œ ìƒ˜í”Œë¡œ í‰ê°€í•˜ê³  ê²°ê³¼ë¥¼ ë¹„êµ
"""
import os
import sys
import json
import time
import django
from pathlib import Path

# Django ì„¤ì • ë¡œë“œ
backend_dir = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(backend_dir))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'config.settings')
django.setup()

from model_evaluator import get_evaluator


class ModelComparisonRunner:
    """ë‹¤ì¤‘ ëª¨ë¸ ë¹„êµ í‰ê°€ ì‹¤í–‰ê¸°"""

    def __init__(self, samples_file, output_dir, models, num_trials=5):
        self.samples_file = samples_file
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.models = models
        self.num_trials = num_trials
        self.evaluators = {}

        # ê° ëª¨ë¸ë³„ evaluator ìƒì„±
        for model in models:
            try:
                self.evaluators[model] = get_evaluator(model)
                print(f"âœ… {model} evaluator ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ {model} evaluator ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    def create_evaluation_prompt(self, sample):
        """ìƒ˜í”Œ ë°ì´í„°ë¡œë¶€í„° í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        mission_title = sample['missionTitle']
        steps = sample['steps']
        explanations = sample['explanations']
        user_codes = sample['userCodes']
        performance = sample['performance']

        # ê° ë‹¨ê³„ë³„ ë°ì´í„° êµ¬ì„±
        step_context = []
        for idx, s in enumerate(steps):
            step_num = idx + 1
            original_code = s.get('buggy_code', '')
            modified_code = user_codes.get(str(step_num), '')
            explanation = explanations.get(str(step_num), 'ì„¤ëª… ì—†ìŒ')

            step_context.append(f"""### Step {step_num}: {s.get('title', s.get('bug_type', ''))}
- ë¬¸ì œ ì„¤ëª…: {s.get('instruction', '')}

- ì›ë³¸ ë²„ê·¸ ì½”ë“œ:
```python
{original_code}
```

- ì‚¬ìš©ì ìˆ˜ì • ì½”ë“œ:
```python
{modified_code}
```

- ì‚¬ìš©ì ì„¤ëª…: {explanation}""")

        step_context_str = '\n\n'.join(step_context)

        system_message = """ë„ˆëŠ” ë””ë²„ê¹… ì‚¬ê³ ë¥¼ í‰ê°€í•˜ëŠ” ì‹œìŠ¤í…œì´ë‹¤.
ì •ì˜¤ë‹µì´ ì•„ë‹ˆë¼ "ë””ë²„ê¹… ì‚¬ê³ ì˜ ì§ˆ"ì„ í‰ê°€í•œë‹¤.
ëƒ‰ì² í•˜ê³  ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•˜ë˜, êµìœ¡ì ì¸ ê´€ì ì„ ìœ ì§€í•œë‹¤."""

        prompt = f"""## í‰ê°€ ëŒ€ìƒ ë°ì´í„°

ë¯¸ì…˜: {mission_title}

[ì‚¬ìš©ì ì„±ê³¼ ì§€í‘œ]
- í€´ì¦ˆ ì˜¤ë‹µ íšŸìˆ˜: {performance.get('quizIncorrectCount', 0)}íšŒ
- ì½”ë“œ ì œì¶œ ì‹¤íŒ¨: {performance.get('codeSubmitFailCount', 0)}íšŒ
- íŒíŠ¸ ì‚¬ìš© íšŸìˆ˜: {performance.get('hintCount', 0)}íšŒ
- ì´ ì†Œìš” ì‹œê°„: {performance.get('totalDebugTime', 0)}ì´ˆ

{step_context_str}

## í‰ê°€ ë‹¨ê³„

1. ì‚¬ê³  ë°©í–¥ í‰ê°€ (ëª¨ë¸ A ê´€ì )
   ë‹¤ìŒ í•­ëª©ë“¤ì„ ê²€í† í•œë‹¤:
   - ì›ì¸ ì–¸ê¸‰ ì—¬ë¶€: ì‚¬ìš©ìê°€ ë²„ê·¸ì˜ ê·¼ë³¸ ì›ì¸ì„ ì–¸ê¸‰í–ˆëŠ”ê°€?
   - ì›ì¸-ìˆ˜ì • ì¼ì¹˜ ì—¬ë¶€: ì–¸ê¸‰í•œ ì›ì¸ê³¼ ì‹¤ì œ ì½”ë“œ ìˆ˜ì •ì´ ì¼ì¹˜í•˜ëŠ”ê°€?
   - ë¶€ì‘ìš© ê³ ë ¤ ì—¬ë¶€: ìˆ˜ì •ìœ¼ë¡œ ì¸í•œ ë¶€ì‘ìš©ì„ ê³ ë ¤í–ˆëŠ”ê°€?
   - ìˆ˜ì • ë²”ìœ„ ì ì ˆì„±: í•„ìš”í•œ ë¶€ë¶„ë§Œ ìˆ˜ì •í–ˆëŠ”ê°€, ê³¼ë„í•˜ê²Œ ìˆ˜ì •í–ˆëŠ”ê°€?
   - ì„¤ëª…-ì½”ë“œ ì¼ê´€ì„±: ì„¤ëª… ë‚´ìš©ê³¼ ì‹¤ì œ ì½”ë“œ ë³€ê²½ì´ ì¼ê´€ë˜ëŠ”ê°€?
   â†’ ì£¼ìš” í•­ëª©(ì›ì¸ ì–¸ê¸‰, ì›ì¸-ìˆ˜ì • ì¼ì¹˜, ì„¤ëª…-ì½”ë“œ ì¼ê´€ì„±) ì¶©ì¡± ì‹œ í†µê³¼

2. ì½”ë“œ ìœ„í—˜ í‰ê°€ (ëª¨ë¸ B ê´€ì )
   ë‹¤ìŒ ìš”ì†Œë¥¼ ë¶„ì„í•œë‹¤:
   - ë³€ê²½ ë¼ì¸ ìˆ˜: ì–¼ë§ˆë‚˜ ë§ì€ ì½”ë“œë¥¼ ë³€ê²½í–ˆëŠ”ê°€?
   - ì¡°ê±´ë¬¸/ì˜ˆì™¸ ì²˜ë¦¬ ë³€í™”: ë¡œì§ íë¦„ì— ì˜í–¥ì„ ì£¼ëŠ” ë³€ê²½ì´ ìˆëŠ”ê°€?
   - ê¸°ì¡´ ë¡œì§ í›¼ì† ì—¬ë¶€: ì›ë˜ ë™ì‘í•´ì•¼ í•  ë¶€ë¶„ì„ ë§ê°€ëœ¨ë ¸ëŠ”ê°€?
   â†’ ìœ„í—˜ ì ìˆ˜ 0~100 (0: ë§¤ìš° ì•ˆì „, 100: ë§¤ìš° ìœ„í—˜)

3. ì‚¬ê³  ì—°ì† ì ìˆ˜ í‰ê°€ (ëª¨ë¸ C ê´€ì )
   ì¢‹ì€ ë””ë²„ê¹… ë‹µë³€ì˜ íŠ¹ì„±ê³¼ ë¹„êµí•œë‹¤:
   - ë…¼ë¦¬ì  íë¦„: ë¬¸ì œ ì¸ì‹ â†’ ì›ì¸ ë¶„ì„ â†’ í•´ê²°ì±… ì œì‹œ ìˆœì„œ
   - ê·¼ê±° ì œì‹œ: ì™œ ê·¸ë ‡ê²Œ ìˆ˜ì •í–ˆëŠ”ì§€ ì´ìœ ë¥¼ ì„¤ëª…í–ˆëŠ”ê°€?
   - ëª…í™•ì„±: ì„¤ëª…ì´ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ê°€?
   - ê¸°ìˆ ì  ì •í™•ì„±: ì‚¬ìš©í•œ ìš©ì–´ì™€ ê°œë…ì´ ì •í™•í•œê°€?
   (ì°¸ê³ : ì˜¤ë‹µì´ë‚˜ íŒíŠ¸ ì‚¬ìš©ì´ ë§ë‹¤ë©´, ì‚¬ê³ ì˜ ìë¦½ì„±ì„ ë‚®ê²Œ í‰ê°€í•˜ë¼)
   â†’ ì‚¬ê³  ì ìˆ˜ 0~100

4. **ê° ë‹¨ê³„ë³„ ì„¤ëª… í”¼ë“œë°± ìƒì„± (í•„ìˆ˜)**
   ìœ„ì— ì œì‹œëœ ê° Stepì˜ "ì‚¬ìš©ì ì„¤ëª…"ì„ ê°œë³„ì ìœ¼ë¡œ í‰ê°€í•˜ì—¬ í”¼ë“œë°±ì„ ìƒì„±í•˜ë¼.
   ê° í”¼ë“œë°±ì€ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ì‘ì„±í•˜ë˜, ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨:
   - ì„¤ëª… í’ˆì§ˆ ì ìˆ˜ (0-100ì )
   - ì˜í•œ ì  (êµ¬ì²´ì ìœ¼ë¡œ)
   - ë¶€ì¡±í•œ ì  (êµ¬ì²´ì ìœ¼ë¡œ)
   - ê°œì„  ë°©í–¥ ì œì•ˆ

   ì˜ˆì‹œ:
   - ì„¤ëª…ì´ ë¶€ì‹¤í•œ ê²½ìš°: "ì„¤ëª… í’ˆì§ˆ: 20/100. ë²„ê·¸ ë°œê²¬ ì˜ë„ëŠ” ìˆìœ¼ë‚˜ êµ¬ì²´ì„±ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. 'ì–´ë–¤ ë³€ìˆ˜'ê°€ 'ì™œ' ë¬¸ì œì¸ì§€, 'ì–´ë–»ê²Œ' ìˆ˜ì •í–ˆëŠ”ì§€ ëª…í™•íˆ ì‘ì„±í•´ì£¼ì„¸ìš”."
   - ì„¤ëª…ì´ ì–‘í˜¸í•œ ê²½ìš°: "ì„¤ëª… í’ˆì§ˆ: 75/100. ì›ì¸ê³¼ í•´ê²°ì±…ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ì—°ê²°í–ˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ ìˆ˜ì •ìœ¼ë¡œ ì¸í•œ ë¶€ì‘ìš© ê³ ë ¤ê¹Œì§€ ì¶”ê°€í•˜ë©´ ë”ìš± ì™„ë²½í•©ë‹ˆë‹¤."

## ì¶œë ¥ í˜•ì‹
**ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ë§Œ ì¶œë ¥í•˜ë¼. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆë¼.**

{{
  "thinking_pass": true,
  "code_risk": 45,
  "thinking_score": 70,
  "ì´í‰": "ì „ì²´ í‰ê°€ë¥¼ ìš”ì•½í•˜ì—¬ ì‹œë‹ˆì–´ ì—”ì§€ë‹ˆì–´ ì…ì¥ì—ì„œ ì„¤ëª…í•˜ê³  ì¡´ëŒ“ë§ë¡œ ì…ë ¥",
  "step_feedbacks": [
    {{
      "step": 1,
      "feedback": "ì‹¤ì œ Step 1 ì‚¬ìš©ì ì„¤ëª…ì„ ë¶„ì„í•œ êµ¬ì²´ì ì¸ í”¼ë“œë°± (ì ìˆ˜ í¬í•¨, í•œ ë¬¸ë‹¨)"
    }},
    {{
      "step": 2,
      "feedback": "ì‹¤ì œ Step 2 ì‚¬ìš©ì ì„¤ëª…ì„ ë¶„ì„í•œ êµ¬ì²´ì ì¸ í”¼ë“œë°± (ì ìˆ˜ í¬í•¨, í•œ ë¬¸ë‹¨)"
    }},
    {{
      "step": 3,
      "feedback": "ì‹¤ì œ Step 3 ì‚¬ìš©ì ì„¤ëª…ì„ ë¶„ì„í•œ êµ¬ì²´ì ì¸ í”¼ë“œë°± (ì ìˆ˜ í¬í•¨, í•œ ë¬¸ë‹¨)"
    }}
  ]
}}

**ì¤‘ìš”**: step_feedbacks ë°°ì—´ì€ ë°˜ë“œì‹œ 3ê°œ í•­ëª©ì„ í¬í•¨í•´ì•¼ í•˜ë©°, ê° stepì— ëŒ€í•œ ì‹¤ì œ í‰ê°€ ë‚´ìš©ì„ ì‘ì„±í•´ì•¼ í•œë‹¤.
"""

        return system_message, prompt

    def run_single_evaluation(self, model, sample, trial_num):
        """ë‹¨ì¼ ëª¨ë¸ë¡œ ë‹¨ì¼ ìƒ˜í”Œ í‰ê°€"""
        evaluator = self.evaluators.get(model)
        if not evaluator:
            return {'error': True, 'message': f'No evaluator for {model}'}

        system_message, prompt = self.create_evaluation_prompt(sample)

        try:
            result = evaluator.evaluate(system_message, prompt)

            if result['success']:
                eval_result = result['result']
                return {
                    'trial': trial_num,
                    'thinking_pass': eval_result.get('thinking_pass', False),
                    'code_risk': eval_result.get('code_risk', 50),
                    'thinking_score': eval_result.get('thinking_score', 50),
                    'summary': eval_result.get('ì´í‰', ''),
                    'step_feedbacks': eval_result.get('step_feedbacks', []),
                    'tokens': result.get('tokens', {}),
                    'cost': result.get('cost', 0),
                    'time': result.get('time', 0)
                }
            else:
                return {
                    'trial': trial_num,
                    'error': True,
                    'message': result.get('error', 'Unknown error'),
                    'time': result.get('time', 0)
                }

        except Exception as e:
            return {
                'trial': trial_num,
                'error': True,
                'message': str(e)
            }

    def run_all_evaluations(self):
        """ëª¨ë“  ëª¨ë¸ë¡œ ëª¨ë“  ìƒ˜í”Œ í‰ê°€"""
        # ìƒ˜í”Œ ë¡œë“œ
        with open(self.samples_file, 'r', encoding='utf-8') as f:
            samples = json.load(f)

        total_evaluations = len(samples) * len(self.models) * self.num_trials

        print(f"\nğŸš€ ëª¨ë¸ ë¹„êµ í‰ê°€ ì‹œì‘")
        print(f"   - ìƒ˜í”Œ ìˆ˜: {len(samples)}")
        print(f"   - ëª¨ë¸ ìˆ˜: {len(self.models)}")
        print(f"   - ë°˜ë³µ íšŸìˆ˜: {self.num_trials}")
        print(f"   - ì´ í‰ê°€ íšŸìˆ˜: {total_evaluations}")
        print(f"   - í‰ê°€ ëª¨ë¸: {', '.join(self.models)}")

        start_time = time.time()
        completed = 0

        # ëª¨ë¸ë³„ ê²°ê³¼ ì €ì¥
        all_results = {}

        for model in self.models:
            print(f"\n{'='*60}")
            print(f"ğŸ¤– ëª¨ë¸: {model}")
            print(f"{'='*60}")

            model_results = []

            for sample_idx, sample in enumerate(samples):
                sample_id = sample['sample_id']
                print(f"\n[{sample_idx + 1}/{len(samples)}] {sample_id} í‰ê°€ ì¤‘...")

                sample_result = {
                    'sample_id': sample_id,
                    'case_id': sample['case_id'],
                    'quality_level': sample['quality_level'],
                    'expected_score_range': sample['expected_score_range'],
                    'trials': []
                }

                for trial in range(self.num_trials):
                    result = self.run_single_evaluation(model, sample, trial + 1)
                    sample_result['trials'].append(result)

                    if 'error' not in result:
                        print(f"  Trial {trial + 1}: ì‚¬ê³ ì ìˆ˜={result['thinking_score']} "
                              f"ìœ„í—˜ë„={result['code_risk']} "
                              f"í†µê³¼={'âœ…' if result['thinking_pass'] else 'âŒ'} "
                              f"ì‹œê°„={result['time']:.1f}s")
                        completed += 1
                    else:
                        print(f"  Trial {trial + 1}: âŒ ì˜¤ë¥˜ - {result.get('message', 'Unknown')}")

                    # Rate limiting
                    time.sleep(0.5)

                model_results.append(sample_result)

                # ì§„í–‰ë¥  í‘œì‹œ
                progress = (completed / total_evaluations) * 100
                elapsed = time.time() - start_time
                print(f"  ì§„í–‰ë¥ : {completed}/{total_evaluations} ({progress:.1f}%) | "
                      f"ê²½ê³¼: {elapsed / 60:.1f}ë¶„")

            # ëª¨ë¸ë³„ ê²°ê³¼ ì €ì¥
            all_results[model] = {
                'model_name': model,
                'results': model_results,
                'stats': self.evaluators[model].get_stats()
            }

            # ê°œë³„ ëª¨ë¸ ê²°ê³¼ íŒŒì¼ ì €ì¥
            model_file = self.output_dir / f'{model.replace("/", "_")}_results.json'
            with open(model_file, 'w', encoding='utf-8') as f:
                json.dump(all_results[model], f, ensure_ascii=False, indent=2)

            print(f"\nâœ… {model} í‰ê°€ ì™„ë£Œ")
            print(f"   ğŸ“ ì €ì¥: {model_file}")
            print(f"   ğŸ“Š í†µê³„: {self.evaluators[model].get_stats()}")

        # ì „ì²´ ê²°ê³¼ ì €ì¥
        summary_file = self.output_dir / 'model_comparison_results.json'
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump({
                'metadata': {
                    'total_samples': len(samples),
                    'models': self.models,
                    'trials_per_sample': self.num_trials,
                    'total_evaluations': total_evaluations,
                    'completed_evaluations': completed,
                    'elapsed_time_seconds': time.time() - start_time
                },
                'model_results': all_results
            }, f, ensure_ascii=False, indent=2)

        print(f"\n{'='*60}")
        print(f"âœ… ì „ì²´ ëª¨ë¸ ë¹„êµ í‰ê°€ ì™„ë£Œ!")
        print(f"{'='*60}")
        print(f"ğŸ“ ê²°ê³¼ ì €ì¥: {summary_file}")
        print(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {(time.time() - start_time) / 60:.1f}ë¶„")

        return all_results


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='ë‹¤ì¤‘ ëª¨ë¸ ë¹„êµ í‰ê°€ ì‹¤í–‰')
    parser.add_argument('--trials', type=int, default=5, help='ìƒ˜í”Œë‹¹ ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸: 5)')
    parser.add_argument('--quick', action='store_true', help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ê° í’ˆì§ˆë³„ 1ê°œ ìƒ˜í”Œë§Œ)')
    parser.add_argument('--models', nargs='+',
                       default=['gpt-4o', 'gpt-4o-mini', 'gpt-3.5-turbo', 'Llama-3.1-70B', 'Mixtral-8x7B'],
                       help='ë¹„êµí•  ëª¨ë¸ ëª©ë¡')
    args = parser.parse_args()

    # íŒŒì¼ ê²½ë¡œ
    data_dir = Path(__file__).resolve().parent.parent.parent / 'data' / 'validation'
    samples_file = data_dir / 'bug_hunt_validation_samples.json'
    output_dir = data_dir / 'model_comparison'

    # Quick ëª¨ë“œ: í…ŒìŠ¤íŠ¸ìš© (5ê°œ ìƒ˜í”Œë§Œ)
    if args.quick:
        print("âš¡ Quick ëª¨ë“œ: í’ˆì§ˆë³„ 1ê°œ ìƒ˜í”Œë§Œ í‰ê°€")
        with open(samples_file, 'r', encoding='utf-8') as f:
            all_samples = json.load(f)

        # ê° í’ˆì§ˆ ë ˆë²¨ì—ì„œ ì²« ë²ˆì§¸ ìƒ˜í”Œë§Œ ì„ íƒ
        test_samples = []
        for quality in ['excellent', 'good', 'average', 'poor', 'very_poor']:
            sample = next((s for s in all_samples if s['quality_level'] == quality), None)
            if sample:
                test_samples.append(sample)

        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        test_samples_file = data_dir / 'test_samples.json'
        with open(test_samples_file, 'w', encoding='utf-8') as f:
            json.dump(test_samples, f, ensure_ascii=False, indent=2)

        samples_file = test_samples_file

    # í‰ê°€ ì‹¤í–‰
    runner = ModelComparisonRunner(samples_file, output_dir, models=args.models, num_trials=args.trials)
    runner.run_all_evaluations()
