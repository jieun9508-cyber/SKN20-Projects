"""
BEFORE vs AFTER í”„ë¡¬í”„íŠ¸ ë¹„êµ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

BEFORE: ì›ë³¸ í”„ë¡¬í”„íŠ¸ (ì£¼ê´€ì  í‰ê°€)
AFTER: ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ (5ê°€ì§€ ê¸°ì¤€ + Few-shot)
"""
import json
import numpy as np
from pathlib import Path


class PromptComparisonAnalyzer:
    """BEFORE vs AFTER í”„ë¡¬í”„íŠ¸ ë¹„êµ ë¶„ì„"""

    def __init__(self, before_file, after_file):
        with open(before_file, 'r', encoding='utf-8') as f:
            before_data = json.load(f)
            self.before_results = before_data['results']

        with open(after_file, 'r', encoding='utf-8') as f:
            after_data = json.load(f)
            self.after_results = after_data['results']

    def extract_scores_by_quality(self, results):
        """í’ˆì§ˆ ë ˆë²¨ë³„ ì ìˆ˜ ì¶”ì¶œ"""
        quality_scores = {
            'excellent': [],
            'good': [],
            'average': [],
            'poor': [],
            'very_poor': []
        }

        for sample in results:
            quality = sample['quality_level']
            for trial in sample['trials']:
                if 'error' not in trial:
                    score = trial['thinking_score']
                    quality_scores[quality].append(score)

        return quality_scores

    def calculate_consistency(self, results):
        """ì¼ê´€ì„± ë¶„ì„: ë™ì¼ ìƒ˜í”Œì˜ í‘œì¤€í¸ì°¨"""
        std_devs = []

        for sample in results:
            sample_scores = []
            for trial in sample['trials']:
                if 'error' not in trial:
                    sample_scores.append(trial['thinking_score'])

            if len(sample_scores) > 1:
                std_devs.append(np.std(sample_scores))

        return {
            'avg_std_dev': np.mean(std_devs) if std_devs else 0,
            'max_std_dev': max(std_devs) if std_devs else 0,
            'min_std_dev': min(std_devs) if std_devs else 0,
            'all_std_devs': std_devs
        }

    def calculate_discrimination(self, quality_scores):
        """êµ¬ë¶„ë ¥: Excellent vs Very Poor ì ìˆ˜ ì°¨ì´"""
        excellent_avg = np.mean(quality_scores['excellent']) if quality_scores['excellent'] else 0
        very_poor_avg = np.mean(quality_scores['very_poor']) if quality_scores['very_poor'] else 0
        good_avg = np.mean(quality_scores['good']) if quality_scores['good'] else 0

        return {
            'excellent_avg': excellent_avg,
            'good_avg': good_avg,
            'very_poor_avg': very_poor_avg,
            'excellent_vs_very_poor_diff': excellent_avg - very_poor_avg,
            'excellent_vs_good_diff': excellent_avg - good_avg,
            'quality_means': {q: np.mean(scores) if scores else 0 for q, scores in quality_scores.items()}
        }

    def calculate_score_distribution(self, quality_scores):
        """ì ìˆ˜ ë¶„í¬ ë¶„ì„"""
        all_scores = []
        for scores in quality_scores.values():
            all_scores.extend(scores)

        return {
            'min': min(all_scores) if all_scores else 0,
            'max': max(all_scores) if all_scores else 0,
            'mean': np.mean(all_scores) if all_scores else 0,
            'std': np.std(all_scores) if all_scores else 0,
            'range': max(all_scores) - min(all_scores) if all_scores else 0
        }

    def compare(self):
        """ì „ì²´ ë¹„êµ ë¶„ì„"""
        print("=" * 80)
        print("ğŸ“Š BEFORE vs AFTER í”„ë¡¬í”„íŠ¸ ë¹„êµ ë¶„ì„")
        print("=" * 80)

        # BEFORE ë¶„ì„
        before_quality_scores = self.extract_scores_by_quality(self.before_results)
        before_consistency = self.calculate_consistency(self.before_results)
        before_discrimination = self.calculate_discrimination(before_quality_scores)
        before_distribution = self.calculate_score_distribution(before_quality_scores)

        # AFTER ë¶„ì„
        after_quality_scores = self.extract_scores_by_quality(self.after_results)
        after_consistency = self.calculate_consistency(self.after_results)
        after_discrimination = self.calculate_discrimination(after_quality_scores)
        after_distribution = self.calculate_score_distribution(after_quality_scores)

        # ê²°ê³¼ ì¶œë ¥
        print("\n1ï¸âƒ£ ì¼ê´€ì„± (Consistency) - ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ")
        print("-" * 80)
        print(f"  BEFORE í‰ê·  í‘œì¤€í¸ì°¨: {before_consistency['avg_std_dev']:.2f}ì ")
        print(f"  AFTER  í‰ê·  í‘œì¤€í¸ì°¨: {after_consistency['avg_std_dev']:.2f}ì ")
        improvement_consistency = ((before_consistency['avg_std_dev'] - after_consistency['avg_std_dev']) / before_consistency['avg_std_dev'] * 100) if before_consistency['avg_std_dev'] > 0 else 0
        print(f"  ê°œì„ ìœ¨: {improvement_consistency:+.1f}% {'âœ…' if improvement_consistency > 0 else 'âŒ'}")

        print("\n2ï¸âƒ£ êµ¬ë¶„ë ¥ (Discrimination)")
        print("-" * 80)
        print(f"  BEFORE Excellent vs Very Poor ì°¨ì´: {before_discrimination['excellent_vs_very_poor_diff']:.2f}ì ")
        print(f"  AFTER  Excellent vs Very Poor ì°¨ì´: {after_discrimination['excellent_vs_very_poor_diff']:.2f}ì ")
        improvement_disc = after_discrimination['excellent_vs_very_poor_diff'] - before_discrimination['excellent_vs_very_poor_diff']
        print(f"  ê°œì„ : {improvement_disc:+.2f}ì  {'âœ…' if improvement_disc > 0 else 'âŒ'}")

        print(f"\n  BEFORE Excellent vs Good ì°¨ì´: {before_discrimination['excellent_vs_good_diff']:.2f}ì ")
        print(f"  AFTER  Excellent vs Good ì°¨ì´: {after_discrimination['excellent_vs_good_diff']:.2f}ì ")
        improvement_ex_good = after_discrimination['excellent_vs_good_diff'] - before_discrimination['excellent_vs_good_diff']
        print(f"  ê°œì„ : {improvement_ex_good:+.2f}ì  {'âœ…' if improvement_ex_good > 0 else 'âŒ'}")

        print("\n3ï¸âƒ£ í’ˆì§ˆ ë ˆë²¨ë³„ í‰ê·  ì ìˆ˜")
        print("-" * 80)
        print(f"  {'í’ˆì§ˆ':<12} {'BEFORE':>10} {'AFTER':>10} {'ë³€í™”':>10}")
        print("-" * 80)
        for quality in ['excellent', 'good', 'average', 'poor', 'very_poor']:
            before_avg = before_discrimination['quality_means'][quality]
            after_avg = after_discrimination['quality_means'][quality]
            diff = after_avg - before_avg
            print(f"  {quality:<12} {before_avg:>10.2f} {after_avg:>10.2f} {diff:>+10.2f}")

        print("\n4ï¸âƒ£ ì ìˆ˜ ë¶„í¬ (Score Distribution)")
        print("-" * 80)
        print(f"  {'ì§€í‘œ':<15} {'BEFORE':>10} {'AFTER':>10} {'ê°œì„ ':>10}")
        print("-" * 80)
        print(f"  {'ìµœì†Œê°’':<15} {before_distribution['min']:>10.2f} {after_distribution['min']:>10.2f} {after_distribution['min']-before_distribution['min']:>+10.2f}")
        print(f"  {'ìµœëŒ€ê°’':<15} {before_distribution['max']:>10.2f} {after_distribution['max']:>10.2f} {after_distribution['max']-before_distribution['max']:>+10.2f}")
        print(f"  {'í‰ê· ':<15} {before_distribution['mean']:>10.2f} {after_distribution['mean']:>10.2f} {after_distribution['mean']-before_distribution['mean']:>+10.2f}")
        print(f"  {'í‘œì¤€í¸ì°¨':<15} {before_distribution['std']:>10.2f} {after_distribution['std']:>10.2f} {after_distribution['std']-before_distribution['std']:>+10.2f}")
        print(f"  {'ì ìˆ˜ ë²”ìœ„':<15} {before_distribution['range']:>10.2f} {after_distribution['range']:>10.2f} {after_distribution['range']-before_distribution['range']:>+10.2f}")

        print("\n" + "=" * 80)
        print("ğŸ“ˆ ì¢…í•© í‰ê°€")
        print("=" * 80)

        improvements = []
        if improvement_consistency > 0:
            improvements.append(f"âœ… ì¼ê´€ì„± {improvement_consistency:.1f}% ê°œì„ ")
        else:
            improvements.append(f"âŒ ì¼ê´€ì„± {abs(improvement_consistency):.1f}% ì•…í™”")

        if improvement_disc > 0:
            improvements.append(f"âœ… êµ¬ë¶„ë ¥ {improvement_disc:.2f}ì  í–¥ìƒ")
        else:
            improvements.append(f"âŒ êµ¬ë¶„ë ¥ {abs(improvement_disc):.2f}ì  ê°ì†Œ")

        if after_distribution['range'] > before_distribution['range']:
            improvements.append(f"âœ… ì ìˆ˜ ë²”ìœ„ {after_distribution['range'] - before_distribution['range']:.2f}ì  í™•ì¥")
        else:
            improvements.append(f"âŒ ì ìˆ˜ ë²”ìœ„ {before_distribution['range'] - after_distribution['range']:.2f}ì  ì¶•ì†Œ")

        for imp in improvements:
            print(f"  {imp}")

        # JSON ê²°ê³¼ ì €ì¥
        comparison_result = {
            'before': {
                'consistency': before_consistency,
                'discrimination': before_discrimination,
                'distribution': before_distribution
            },
            'after': {
                'consistency': after_consistency,
                'discrimination': after_discrimination,
                'distribution': after_distribution
            },
            'improvements': {
                'consistency_pct': improvement_consistency,
                'discrimination_diff': improvement_disc,
                'excellent_good_diff': improvement_ex_good,
                'range_diff': after_distribution['range'] - before_distribution['range']
            }
        }

        return comparison_result


if __name__ == "__main__":
    base_dir = Path(__file__).resolve().parent.parent.parent
    before_file = base_dir / 'data' / 'validation' / 'bughunt_validation' / 'evaluation_results.json'
    after_file = base_dir / 'data' / 'validation' / 'prompt_improved_evaluation_results.json'

    analyzer = PromptComparisonAnalyzer(before_file, after_file)
    result = analyzer.compare()

    # ê²°ê³¼ ì €ì¥
    output_file = base_dir / 'data' / 'validation' / 'before_after_comparison.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ ë¹„êµ ê²°ê³¼ ì €ì¥: {output_file}")
