"""
ê·œì¹™ ê¸°ë°˜ Bug Hunt í‰ê°€ ì ìˆ˜ ê³„ì‚°

LLM í‰ê°€ì™€ ë¹„êµí•˜ê¸° ìœ„í•œ ê°ê´€ì  ì§€í‘œ
"""
import re


class RuleBasedScorer:
    """ê·œì¹™ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°ê¸°"""

    # ë²„ê·¸ ìœ í˜•ë³„ í‚¤ì›Œë“œ
    BUG_KEYWORDS = {
        "data_leakage": ["data leakage", "ë°ì´í„° ëˆ„ìˆ˜", "train_test_split", "ìŠ¤ì¼€ì¼ë§", "fit_transform", "leakage"],
        "label_imbalance": ["imbalance", "ë¶ˆê· í˜•", "f1", "recall", "precision", "weighted", "accuracy"],
        "overfitting": ["overfitting", "ê³¼ì í•©", "validation", "ê²€ì¦", "early stopping"],
        "off_by_one": ["off-by-one", "ì¸ë±ìŠ¤", "index", "boundary", "ê²½ê³„", "ë²”ìœ„"],
        "null_pointer": ["null", "none", "ì²´í¬", "check", "ì˜ˆì™¸", "exception"],
        "type_mismatch": ["type", "íƒ€ì…", "í˜•ë³€í™˜", "str", "int", "casting"],
        "metric_selection": ["metric", "ì§€í‘œ", "í‰ê°€", "mse", "r2", "accuracy", "regression"],
        "feature_leakage": ["feature", "í”¼ì²˜", "ëˆ„ìˆ˜", "ë¯¸ë˜", "ì •ë³´", "leakage"],
        "hyperparameter": ["hyperparameter", "í•˜ì´í¼íŒŒë¼ë¯¸í„°", "learning rate", "í•™ìŠµë¥ "],
        "memory_leak": ["memory", "ë©”ëª¨ë¦¬", "ëˆ„ìˆ˜", "leak", "í•´ì œ", "generator"],
        "race_condition": ["race", "ê²½ìŸ", "ë™ê¸°í™”", "lock", "thread", "ë©€í‹°ìŠ¤ë ˆë“œ"],
        "api_timeout": ["timeout", "íƒ€ì„ì•„ì›ƒ", "exception", "ì˜ˆì™¸", "try", "except"]
    }

    def __init__(self):
        pass

    def score_sample(self, sample):
        """
        ìƒ˜í”Œì— ëŒ€í•œ ê·œì¹™ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°

        ì ìˆ˜ êµ¬ì„±:
        - ë²„ê·¸ ì›ì¸ í‚¤ì›Œë“œ ì–¸ê¸‰: 0-30ì 
        - ìˆ˜ì • ì½”ë“œ ì ì ˆì„±: 0-25ì 
        - ì„¤ëª… ì¶©ì‹¤ë„: 0-25ì 
        - ë¶€ì‘ìš© ê³ ë ¤ ì–¸ê¸‰: 0-20ì 
        """
        case_id = sample['case_id']
        explanations = sample['explanations']
        user_codes = sample['userCodes']

        # ëª¨ë“  ì„¤ëª… í…ìŠ¤íŠ¸ ê²°í•©
        all_text = ' '.join([
            explanations.get('1', ''),
            explanations.get('2', ''),
            explanations.get('3', '')
        ]).lower()

        score = 0

        # 1. ë²„ê·¸ ì›ì¸ í‚¤ì›Œë“œ ì–¸ê¸‰ (0-30ì )
        score += self._score_bug_keyword_mention(case_id, all_text)

        # 2. ìˆ˜ì • ì½”ë“œ ì ì ˆì„± (0-25ì )
        score += self._score_code_quality(sample)

        # 3. ì„¤ëª… ì¶©ì‹¤ë„ (0-25ì )
        score += self._score_explanation_completeness(explanations)

        # 4. ë¶€ì‘ìš© ê³ ë ¤ ì–¸ê¸‰ (0-20ì )
        score += self._score_side_effect_consideration(all_text)

        return min(100, score)  # ìµœëŒ€ 100ì 

    def _score_bug_keyword_mention(self, case_id, text):
        """ë²„ê·¸ ì›ì¸ í‚¤ì›Œë“œ ì–¸ê¸‰ ì—¬ë¶€ (0-30ì )"""
        keywords = self.BUG_KEYWORDS.get(case_id, [])

        matched = 0
        for keyword in keywords:
            if keyword in text:
                matched += 1

        # í‚¤ì›Œë“œ ë§¤ì¹­ ë¹„ìœ¨ì— ë”°ë¼ ì ìˆ˜
        if len(keywords) == 0:
            return 15  # ê¸°ë³¸ ì ìˆ˜

        match_ratio = matched / len(keywords)
        if match_ratio >= 0.5:
            return 30
        elif match_ratio >= 0.3:
            return 20
        elif match_ratio >= 0.1:
            return 10
        else:
            return 0

    def _score_code_quality(self, sample):
        """ìˆ˜ì • ì½”ë“œ ì ì ˆì„± (0-25ì )"""
        original_code = sample['steps'][1]['buggy_code']
        fixed_code = sample['userCodes'].get('2', '')

        score = 0

        # ì½”ë“œ ë³€ê²½ì´ ìˆëŠ”ê°€?
        if fixed_code != original_code:
            score += 10

        # ë³€ê²½ ë¼ì¸ ìˆ˜ê°€ ì ì ˆí•œê°€? (1-10ì¤„)
        original_lines = len(original_code.split('\n'))
        fixed_lines = len(fixed_code.split('\n'))
        line_diff = abs(fixed_lines - original_lines)

        if 1 <= line_diff <= 10:
            score += 10
        elif line_diff == 0:
            score += 0  # ë³€ê²½ ì—†ìŒ
        else:
            score += 5  # ë„ˆë¬´ ë§ì€ ë³€ê²½

        # ì£¼ì„ì´ ìˆëŠ”ê°€?
        if '#' in fixed_code or '"""' in fixed_code:
            score += 5

        return score

    def _score_explanation_completeness(self, explanations):
        """ì„¤ëª… ì¶©ì‹¤ë„ (0-25ì )"""
        score = 0

        # Step 1: ì§„ë‹¨ ì„¤ëª…
        step1 = explanations.get('1', '')
        if len(step1) >= 50:
            score += 10
        elif len(step1) >= 20:
            score += 5

        # Step 3: ìˆ˜ì • ì„¤ëª…
        step3 = explanations.get('3', '')
        if len(step3) >= 100:
            score += 15
        elif len(step3) >= 50:
            score += 10
        elif len(step3) >= 20:
            score += 5

        return score

    def _score_side_effect_consideration(self, text):
        """ë¶€ì‘ìš© ê³ ë ¤ ì–¸ê¸‰ (0-20ì )"""
        side_effect_keywords = [
            'ë¶€ì‘ìš©', 'side effect', 'ì˜í–¥', 'impact',
            'ì•ˆì „', 'safe', 'ë¬¸ì œì—†', 'ì¬ë°œ ë°©ì§€',
            'ê²½ê³„', 'boundary', 'ì˜ˆì™¸', 'exception',
            'ì„±ëŠ¥', 'performance', 'ë©”ëª¨ë¦¬', 'memory'
        ]

        matched = sum(1 for kw in side_effect_keywords if kw in text)

        if matched >= 3:
            return 20
        elif matched >= 2:
            return 15
        elif matched >= 1:
            return 10
        else:
            return 0

    def score_all_samples(self, samples):
        """ëª¨ë“  ìƒ˜í”Œì— ëŒ€í•œ ê·œì¹™ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°"""
        results = []

        for sample in samples:
            score = self.score_sample(sample)
            results.append({
                'sample_id': sample['sample_id'],
                'case_id': sample['case_id'],
                'quality_level': sample['quality_level'],
                'rule_based_score': score,
                'expected_score_range': sample['expected_score_range']
            })

        return results


if __name__ == "__main__":
    import json
    import os

    print("ğŸ¯ ê·œì¹™ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚° í…ŒìŠ¤íŠ¸...")

    # ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ
    data_dir = os.path.join(os.path.dirname(__file__), '..', '..', 'data', 'validation')
    samples_file = os.path.join(data_dir, 'bug_hunt_validation_samples.json')

    with open(samples_file, 'r', encoding='utf-8') as f:
        samples = json.load(f)

    scorer = RuleBasedScorer()
    results = scorer.score_all_samples(samples)

    # ê²°ê³¼ ì €ì¥
    output_file = os.path.join(data_dir, 'rule_based_scores.json')
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"âœ… ê·œì¹™ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚° ì™„ë£Œ!")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_file}")

    # í†µê³„ ì¶œë ¥
    print(f"\nğŸ“Š ì ìˆ˜ ë¶„í¬:")
    for quality in ['excellent', 'good', 'average', 'poor', 'very_poor']:
        quality_results = [r for r in results if r['quality_level'] == quality]
        if quality_results:
            avg_score = sum(r['rule_based_score'] for r in quality_results) / len(quality_results)
            print(f"   {quality:12s}: í‰ê·  {avg_score:.1f}ì ")
