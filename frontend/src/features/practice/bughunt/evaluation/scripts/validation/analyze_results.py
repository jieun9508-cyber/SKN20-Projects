"""
Bug Hunt í‰ê°€ ê²€ì¦ ê²°ê³¼ ë¶„ì„

4ê°€ì§€ ê²€ì¦:
1. í‰ê°€ ì¼ê´€ì„± (Consistency)
2. ê·¹ë‹¨ ì¼€ì´ìŠ¤ êµ¬ë¶„ë ¥ (Discrimination)
3. ìƒëŒ€ í‰ê°€ ì •í™•ë„ (Ranking)
4. ê·œì¹™ ê¸°ë°˜ êµì°¨ ê²€ì¦ (Convergent Validity)
"""
import json
import numpy as np
from scipy import stats
from pathlib import Path
from collections import defaultdict


class ValidationAnalyzer:
    """ê²€ì¦ ê²°ê³¼ ë¶„ì„ê¸°"""

    def __init__(self, evaluation_results_file, rule_based_scores_file):
        self.evaluation_results_file = evaluation_results_file
        self.rule_based_scores_file = rule_based_scores_file
        self.evaluation_data = None
        self.rule_scores = None

    def load_data(self):
        """ë°ì´í„° ë¡œë“œ"""
        with open(self.evaluation_results_file, 'r', encoding='utf-8') as f:
            self.evaluation_data = json.load(f)

        with open(self.rule_based_scores_file, 'r', encoding='utf-8') as f:
            self.rule_scores = {item['sample_id']: item['rule_based_score']
                               for item in json.load(f)}

        print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        print(f"   í‰ê°€ ê²°ê³¼: {len(self.evaluation_data['results'])}ê°œ ìƒ˜í”Œ")
        print(f"   ê·œì¹™ ì ìˆ˜: {len(self.rule_scores)}ê°œ ìƒ˜í”Œ")

    def analyze_consistency(self):
        """
        ê²€ì¦ 1: í‰ê°€ ì¼ê´€ì„±
        ë™ì¼ ìƒ˜í”Œì„ ë°˜ë³µ í‰ê°€í–ˆì„ ë•Œ ì ìˆ˜ì˜ í‘œì¤€í¸ì°¨ ì¸¡ì •
        """
        print("\n" + "="*60)
        print("ğŸ“Š ê²€ì¦ 1: í‰ê°€ ì¼ê´€ì„± (Consistency)")
        print("="*60)

        consistency_results = []

        for sample_result in self.evaluation_data['results']:
            sample_id = sample_result['sample_id']
            trials = sample_result['trials']

            # ì—ëŸ¬ ì œì™¸
            valid_trials = [t for t in trials if 'error' not in t]

            if len(valid_trials) < 2:
                continue

            # ì‚¬ê³  ì ìˆ˜ì˜ í‘œì¤€í¸ì°¨ ê³„ì‚°
            thinking_scores = [t['thinking_score'] for t in valid_trials]
            std_dev = np.std(thinking_scores, ddof=1)
            mean_score = np.mean(thinking_scores)

            consistency_results.append({
                'sample_id': sample_id,
                'quality': sample_result['quality_level'],
                'mean_score': mean_score,
                'std_dev': std_dev,
                'scores': thinking_scores
            })

        # í†µê³„
        all_std_devs = [r['std_dev'] for r in consistency_results]
        avg_std_dev = np.mean(all_std_devs)
        max_std_dev = np.max(all_std_devs)

        print(f"\nê²°ê³¼:")
        print(f"  í‰ê·  í‘œì¤€í¸ì°¨: {avg_std_dev:.2f}ì  (ëª©í‘œ â‰¤5ì )")
        print(f"  ìµœëŒ€ í‘œì¤€í¸ì°¨: {max_std_dev:.2f}ì  (ëª©í‘œ â‰¤10ì )")
        print(f"  íŒì •: {'âœ… í†µê³¼' if avg_std_dev <= 5 else 'âŒ ë¯¸ë‹¬'}")

        # í’ˆì§ˆë³„ ì¼ê´€ì„±
        print(f"\ní’ˆì§ˆë³„ ì¼ê´€ì„±:")
        for quality in ['excellent', 'good', 'average', 'poor', 'very_poor']:
            quality_results = [r for r in consistency_results if r['quality'] == quality]
            if quality_results:
                quality_std = np.mean([r['std_dev'] for r in quality_results])
                print(f"  {quality:12s}: í‰ê·  Ïƒ = {quality_std:.2f}ì ")

        return {
            'avg_std_dev': avg_std_dev,
            'max_std_dev': max_std_dev,
            'passed': avg_std_dev <= 5,
            'details': consistency_results
        }

    def analyze_discrimination(self):
        """
        ê²€ì¦ 2: ê·¹ë‹¨ ì¼€ì´ìŠ¤ êµ¬ë¶„ë ¥
        ìš°ìˆ˜ ê·¸ë£¹ê³¼ ë§¤ìš° ë¯¸í¡ ê·¸ë£¹ì˜ ì ìˆ˜ ì°¨ì´
        """
        print("\n" + "="*60)
        print("ğŸ“Š ê²€ì¦ 2: ê·¹ë‹¨ ì¼€ì´ìŠ¤ êµ¬ë¶„ë ¥ (Discrimination)")
        print("="*60)

        # ê° ìƒ˜í”Œì˜ í‰ê·  ì ìˆ˜ ê³„ì‚°
        sample_avg_scores = {}
        for sample_result in self.evaluation_data['results']:
            sample_id = sample_result['sample_id']
            quality = sample_result['quality_level']
            trials = sample_result['trials']

            valid_trials = [t for t in trials if 'error' not in t]
            if not valid_trials:
                continue

            avg_score = np.mean([t['thinking_score'] for t in valid_trials])
            sample_avg_scores[sample_id] = {
                'score': avg_score,
                'quality': quality
            }

        # ìš°ìˆ˜ vs ë§¤ìš° ë¯¸í¡ ë¹„êµ
        excellent_scores = [v['score'] for k, v in sample_avg_scores.items()
                           if v['quality'] == 'excellent']
        very_poor_scores = [v['score'] for k, v in sample_avg_scores.items()
                           if v['quality'] == 'very_poor']

        excellent_mean = np.mean(excellent_scores) if excellent_scores else 0
        very_poor_mean = np.mean(very_poor_scores) if very_poor_scores else 0
        score_diff = excellent_mean - very_poor_mean

        print(f"\nê²°ê³¼:")
        print(f"  ìš°ìˆ˜ ê·¸ë£¹ í‰ê· : {excellent_mean:.1f}ì  (n={len(excellent_scores)})")
        print(f"  ë§¤ìš° ë¯¸í¡ ê·¸ë£¹ í‰ê· : {very_poor_mean:.1f}ì  (n={len(very_poor_scores)})")
        print(f"  ì ìˆ˜ ì°¨ì´: {score_diff:.1f}ì  (ëª©í‘œ â‰¥30ì )")
        print(f"  íŒì •: {'âœ… í†µê³¼' if score_diff >= 30 else 'âŒ ë¯¸ë‹¬'}")

        # ëª¨ë“  í’ˆì§ˆ ë ˆë²¨ì˜ í‰ê· 
        print(f"\ní’ˆì§ˆë³„ í‰ê·  ì ìˆ˜:")
        for quality in ['excellent', 'good', 'average', 'poor', 'very_poor']:
            quality_scores = [v['score'] for k, v in sample_avg_scores.items()
                            if v['quality'] == quality]
            if quality_scores:
                quality_mean = np.mean(quality_scores)
                quality_std = np.std(quality_scores)
                print(f"  {quality:12s}: {quality_mean:.1f}Â±{quality_std:.1f}ì ")

        return {
            'excellent_mean': excellent_mean,
            'very_poor_mean': very_poor_mean,
            'score_diff': score_diff,
            'passed': score_diff >= 30,
            'all_scores': sample_avg_scores
        }

    def analyze_ranking(self):
        """
        ê²€ì¦ 3: ìƒëŒ€ í‰ê°€ ì •í™•ë„
        ê°™ì€ ì¼€ì´ìŠ¤ ë‚´ 5ê°œ í’ˆì§ˆ ë ˆë²¨ì˜ ìˆœìœ„ ì¼ì¹˜ë„ (Kendall's Tau)
        """
        print("\n" + "="*60)
        print("ğŸ“Š ê²€ì¦ 3: ìƒëŒ€ í‰ê°€ ì •í™•ë„ (Ranking Validation)")
        print("="*60)

        # ì¼€ì´ìŠ¤ë³„ë¡œ ê·¸ë£¹í™”
        cases = defaultdict(list)
        for sample_result in self.evaluation_data['results']:
            case_id = sample_result['case_id']
            quality = sample_result['quality_level']
            trials = sample_result['trials']

            valid_trials = [t for t in trials if 'error' not in t]
            if not valid_trials:
                continue

            avg_score = np.mean([t['thinking_score'] for t in valid_trials])
            cases[case_id].append({
                'quality': quality,
                'score': avg_score
            })

        # ê¸°ëŒ€ ìˆœìœ„: excellent > good > average > poor > very_poor
        quality_rank = {
            'excellent': 5,
            'good': 4,
            'average': 3,
            'poor': 2,
            'very_poor': 1
        }

        kendall_taus = []
        perfect_rankings = 0
        total_cases = 0

        print(f"\nì¼€ì´ìŠ¤ë³„ ìˆœìœ„ ë¶„ì„:")
        for case_id, samples in cases.items():
            if len(samples) < 5:  # 5ê°œ í’ˆì§ˆì´ ëª¨ë‘ ìˆì–´ì•¼ í•¨
                continue

            total_cases += 1

            # ì •ë ¬
            samples.sort(key=lambda x: x['score'], reverse=True)

            # ê¸°ëŒ€ ìˆœìœ„ì™€ ì‹¤ì œ ìˆœìœ„ ë¹„êµ
            expected_ranks = [quality_rank[s['quality']] for s in samples]
            actual_ranks = list(range(len(samples), 0, -1))

            tau, p_value = stats.kendalltau(expected_ranks, actual_ranks)
            kendall_taus.append(tau)

            # ì™„ë²½í•œ ìˆœìœ„ì¸ì§€ í™•ì¸
            expected_order = ['excellent', 'good', 'average', 'poor', 'very_poor']
            actual_order = [s['quality'] for s in samples]
            is_perfect = expected_order == actual_order

            if is_perfect:
                perfect_rankings += 1

            status = "âœ…" if is_perfect else "âš ï¸"
            print(f"  {case_id:20s} {status} Ï„={tau:.3f} | {[s['quality'][:4] for s in samples]}")

        avg_tau = np.mean(kendall_taus) if kendall_taus else 0
        perfect_ratio = perfect_rankings / total_cases if total_cases > 0 else 0

        print(f"\nê²°ê³¼:")
        print(f"  í‰ê·  Kendall's Tau: {avg_tau:.3f} (ëª©í‘œ â‰¥0.75)")
        print(f"  ì™„ë²½í•œ ìˆœìœ„: {perfect_rankings}/{total_cases} ({perfect_ratio*100:.1f}%)")
        print(f"  íŒì •: {'âœ… í†µê³¼' if avg_tau >= 0.75 else 'âŒ ë¯¸ë‹¬'}")

        return {
            'avg_kendall_tau': avg_tau,
            'perfect_rankings': perfect_rankings,
            'total_cases': total_cases,
            'passed': avg_tau >= 0.75
        }

    def analyze_convergent_validity(self):
        """
        ê²€ì¦ 4: ê·œì¹™ ê¸°ë°˜ êµì°¨ ê²€ì¦
        ê·œì¹™ ê¸°ë°˜ ì ìˆ˜ì™€ LLM ì ìˆ˜ì˜ ìƒê´€ê´€ê³„ (Pearson r)
        """
        print("\n" + "="*60)
        print("ğŸ“Š ê²€ì¦ 4: ê·œì¹™ ê¸°ë°˜ êµì°¨ ê²€ì¦ (Convergent Validity)")
        print("="*60)

        rule_scores_list = []
        llm_scores_list = []

        for sample_result in self.evaluation_data['results']:
            sample_id = sample_result['sample_id']
            trials = sample_result['trials']

            if sample_id not in self.rule_scores:
                continue

            valid_trials = [t for t in trials if 'error' not in t]
            if not valid_trials:
                continue

            rule_score = self.rule_scores[sample_id]
            llm_score = np.mean([t['thinking_score'] for t in valid_trials])

            rule_scores_list.append(rule_score)
            llm_scores_list.append(llm_score)

        # Pearson ìƒê´€ê³„ìˆ˜
        r, p_value = stats.pearsonr(rule_scores_list, llm_scores_list)

        print(f"\nê²°ê³¼:")
        print(f"  Pearson ìƒê´€ê³„ìˆ˜: r = {r:.3f} (ëª©í‘œ â‰¥0.65)")
        print(f"  p-value: {p_value:.4f}")
        print(f"  íŒì •: {'âœ… í†µê³¼' if r >= 0.65 else 'âŒ ë¯¸ë‹¬'}")

        # í’ˆì§ˆë³„ ìƒê´€ê´€ê³„
        print(f"\ní’ˆì§ˆë³„ ìƒê´€ê´€ê³„:")
        for quality in ['excellent', 'good', 'average', 'poor', 'very_poor']:
            quality_rule = []
            quality_llm = []

            for sample_result in self.evaluation_data['results']:
                if sample_result['quality_level'] != quality:
                    continue

                sample_id = sample_result['sample_id']
                if sample_id not in self.rule_scores:
                    continue

                trials = sample_result['trials']
                valid_trials = [t for t in trials if 'error' not in t]
                if not valid_trials:
                    continue

                quality_rule.append(self.rule_scores[sample_id])
                quality_llm.append(np.mean([t['thinking_score'] for t in valid_trials]))

            if len(quality_rule) >= 3:  # ìµœì†Œ 3ê°œ ì´ìƒ
                r_quality, _ = stats.pearsonr(quality_rule, quality_llm)
                print(f"  {quality:12s}: r = {r_quality:.3f}")

        return {
            'pearson_r': r,
            'p_value': p_value,
            'passed': r >= 0.65,
            'rule_scores': rule_scores_list,
            'llm_scores': llm_scores_list
        }

    def generate_summary_report(self, results):
        """ì¢…í•© ë³´ê³ ì„œ ìƒì„±"""
        print("\n" + "="*60)
        print("ğŸ“‹ ì¢…í•© ê²€ì¦ ë³´ê³ ì„œ")
        print("="*60)

        all_passed = all([
            results['consistency']['passed'],
            results['discrimination']['passed'],
            results['ranking']['passed'],
            results['convergent_validity']['passed']
        ])

        print(f"\nê²€ì¦ ê²°ê³¼:")
        print(f"  1. í‰ê°€ ì¼ê´€ì„±:        {'âœ… í†µê³¼' if results['consistency']['passed'] else 'âŒ ë¯¸ë‹¬'} "
              f"(Ïƒ={results['consistency']['avg_std_dev']:.2f}ì )")
        print(f"  2. ê·¹ë‹¨ ì¼€ì´ìŠ¤ êµ¬ë¶„:   {'âœ… í†µê³¼' if results['discrimination']['passed'] else 'âŒ ë¯¸ë‹¬'} "
              f"(ì°¨ì´={results['discrimination']['score_diff']:.1f}ì )")
        print(f"  3. ìƒëŒ€ í‰ê°€ ì •í™•ë„:   {'âœ… í†µê³¼' if results['ranking']['passed'] else 'âŒ ë¯¸ë‹¬'} "
              f"(Ï„={results['ranking']['avg_kendall_tau']:.3f})")
        print(f"  4. ê·œì¹™ ê¸°ë°˜ êµì°¨ê²€ì¦: {'âœ… í†µê³¼' if results['convergent_validity']['passed'] else 'âŒ ë¯¸ë‹¬'} "
              f"(r={results['convergent_validity']['pearson_r']:.3f})")

        print(f"\nìµœì¢… íŒì •: {'âœ… ì „ì²´ í†µê³¼' if all_passed else 'âš ï¸ ì¼ë¶€ ë¯¸ë‹¬'}")

        return {
            'all_passed': all_passed,
            'summary': {
                'consistency': {
                    'passed': results['consistency']['passed'],
                    'avg_std_dev': results['consistency']['avg_std_dev']
                },
                'discrimination': {
                    'passed': results['discrimination']['passed'],
                    'score_diff': results['discrimination']['score_diff']
                },
                'ranking': {
                    'passed': results['ranking']['passed'],
                    'avg_kendall_tau': results['ranking']['avg_kendall_tau']
                },
                'convergent_validity': {
                    'passed': results['convergent_validity']['passed'],
                    'pearson_r': results['convergent_validity']['pearson_r']
                }
            }
        }

    def run_all_analyses(self):
        """ëª¨ë“  ë¶„ì„ ì‹¤í–‰"""
        self.load_data()

        results = {
            'consistency': self.analyze_consistency(),
            'discrimination': self.analyze_discrimination(),
            'ranking': self.analyze_ranking(),
            'convergent_validity': self.analyze_convergent_validity()
        }

        summary = self.generate_summary_report(results)
        results['summary'] = summary

        # ê²°ê³¼ ì €ì¥
        output_file = Path(self.evaluation_results_file).parent / 'analysis_results.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)

        print(f"\nğŸ“ ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_file}")

        return results


if __name__ == "__main__":
    data_dir = Path(__file__).resolve().parent.parent.parent / 'data' / 'validation'
    evaluation_results = data_dir / 'evaluation_results.json'
    rule_scores = data_dir / 'rule_based_scores.json'

    analyzer = ValidationAnalyzer(evaluation_results, rule_scores)
    results = analyzer.run_all_analyses()
