"""
Bug Hunt í‰ê°€ ê²€ì¦ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸

60ê°œ ìƒ˜í”Œ ìƒì„±: 12ê°œ ë²„ê·¸ ì¼€ì´ìŠ¤ Ã— 5ê°œ í’ˆì§ˆ ë ˆë²¨
"""
import json
import os

# 12ê°œ ë²„ê·¸ ì¼€ì´ìŠ¤ ì •ì˜
BUG_CASES = [
    {
        "id": "data_leakage",
        "title": "Data Leakage",
        "bug_type": "ë°ì´í„° ëˆ„ìˆ˜",
        "description": "train_test_split ì „ì— ìŠ¤ì¼€ì¼ë§í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë³´ê°€ í•™ìŠµì— ìœ ì¶œ",
        "buggy_code": """from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)""",
        "correct_fix": """from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)"""
    },
    {
        "id": "label_imbalance",
        "title": "Label Imbalance",
        "bug_type": "ë ˆì´ë¸” ë¶ˆê· í˜•",
        "description": "ë¶ˆê· í˜• ë°ì´í„°ì…‹ì—ì„œ accuracyë§Œìœ¼ë¡œ í‰ê°€í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ ì˜¤íŒ",
        "buggy_code": """from sklearn.metrics import accuracy_score

y_pred = model.predict(X_test)
score = accuracy_score(y_test, y_pred)
print(f"Model accuracy: {score}")""",
        "correct_fix": """from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
print(f"Accuracy: {accuracy}, F1: {f1}, Recall: {recall}")"""
    },
    {
        "id": "overfitting",
        "title": "Overfitting",
        "bug_type": "ê³¼ì í•©",
        "description": "ê²€ì¦ ì„¸íŠ¸ ì—†ì´ í•™ìŠµí•˜ì—¬ ê³¼ì í•© ë°œìƒ",
        "buggy_code": """model.fit(X_train, y_train, epochs=100, batch_size=32)
test_score = model.evaluate(X_test, y_test)
print(f"Test score: {test_score}")""",
        "correct_fix": """from sklearn.model_selection import train_test_split

X_train_split, X_val, y_train_split, y_val = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42
)
model.fit(X_train_split, y_train_split,
          validation_data=(X_val, y_val),
          epochs=100, batch_size=32)"""
    },
    {
        "id": "off_by_one",
        "title": "Off-by-one Error",
        "bug_type": "ì¸ë±ìŠ¤ ì˜¤ë¥˜",
        "description": "ë¦¬ìŠ¤íŠ¸ ì¸ë±ì‹±ì—ì„œ ê²½ê³„ê°’ ì²˜ë¦¬ ì˜¤ë¥˜",
        "buggy_code": """def get_last_n_items(items, n):
    return items[-n:]

result = get_last_n_items([1, 2, 3], 5)  # ì˜¤ë¥˜!""",
        "correct_fix": """def get_last_n_items(items, n):
    if n <= 0:
        return []
    if n >= len(items):
        return items
    return items[-n:]"""
    },
    {
        "id": "null_pointer",
        "title": "Null Pointer",
        "bug_type": "Null ì°¸ì¡° ì˜¤ë¥˜",
        "description": "None ê°’ ì²´í¬ ì—†ì´ ë©”ì„œë“œ í˜¸ì¶œ",
        "buggy_code": """def process_user(user_data):
    username = user_data['name'].lower()
    return username""",
        "correct_fix": """def process_user(user_data):
    if user_data is None or 'name' not in user_data:
        return None
    if user_data['name'] is None:
        return None
    return user_data['name'].lower()"""
    },
    {
        "id": "type_mismatch",
        "title": "Type Mismatch",
        "bug_type": "íƒ€ì… ë¶ˆì¼ì¹˜",
        "description": "ë¬¸ìì—´ê³¼ ìˆ«ìë¥¼ ì—°ì‚°í•˜ì—¬ íƒ€ì… ì—ëŸ¬ ë°œìƒ",
        "buggy_code": """def calculate_total(price, quantity):
    total = price * quantity
    return "Total: " + total""",
        "correct_fix": """def calculate_total(price, quantity):
    total = price * quantity
    return f"Total: {total}"  # ë˜ëŠ” "Total: " + str(total)"""
    },
    {
        "id": "metric_selection",
        "title": "Metric Selection Error",
        "bug_type": "í‰ê°€ ì§€í‘œ ì„ íƒ ì˜¤ë¥˜",
        "description": "íšŒê·€ ë¬¸ì œì— classification metric ì‚¬ìš©",
        "buggy_code": """from sklearn.metrics import accuracy_score

y_pred = model.predict(X_test)
score = accuracy_score(y_test, y_pred)""",
        "correct_fix": """from sklearn.metrics import mean_squared_error, r2_score

y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)"""
    },
    {
        "id": "feature_leakage",
        "title": "Feature Leakage",
        "bug_type": "í”¼ì²˜ ëˆ„ìˆ˜",
        "description": "targetê³¼ ê°•í•œ ìƒê´€ê´€ê³„ê°€ ìˆëŠ” ë¯¸ë˜ ì •ë³´ í¬í•¨",
        "buggy_code": """# ëŒ€ì¶œ ìŠ¹ì¸ ì˜ˆì¸¡ ëª¨ë¸
features = ['income', 'age', 'credit_score', 'loan_approved_date']
X = df[features]
y = df['loan_approved']""",
        "correct_fix": """# loan_approved_dateëŠ” ë¯¸ë˜ ì •ë³´ì´ë¯€ë¡œ ì œì™¸
features = ['income', 'age', 'credit_score']
X = df[features]
y = df['loan_approved']"""
    },
    {
        "id": "hyperparameter",
        "title": "Hyperparameter Error",
        "bug_type": "í•˜ì´í¼íŒŒë¼ë¯¸í„° ì˜¤ë¥˜",
        "description": "learning_rateê°€ ë„ˆë¬´ ì»¤ì„œ ë°œì‚°",
        "buggy_code": """model = keras.Sequential([...])
model.compile(optimizer=keras.optimizers.Adam(learning_rate=1.0),
              loss='mse')""",
        "correct_fix": """model = keras.Sequential([...])
model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),
              loss='mse')"""
    },
    {
        "id": "memory_leak",
        "title": "Memory Leak",
        "bug_type": "ë©”ëª¨ë¦¬ ëˆ„ìˆ˜",
        "description": "ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ê³„ì† ì¶•ì ",
        "buggy_code": """results = []
for file in large_file_list:
    data = load_large_file(file)
    results.append(data)  # ë©”ëª¨ë¦¬ ëˆ„ì !""",
        "correct_fix": """def process_file(file):
    data = load_large_file(file)
    result = process(data)
    return result

# ë˜ëŠ” generator ì‚¬ìš©
for file in large_file_list:
    data = load_large_file(file)
    process(data)  # ì²˜ë¦¬ í›„ ë©”ëª¨ë¦¬ í•´ì œ"""
    },
    {
        "id": "race_condition",
        "title": "Race Condition",
        "bug_type": "ê²½ìŸ ìƒíƒœ",
        "description": "ë©€í‹°ìŠ¤ë ˆë“œì—ì„œ ê³µìœ  ë³€ìˆ˜ ì ‘ê·¼ ì‹œ ë™ê¸°í™” ëˆ„ë½",
        "buggy_code": """counter = 0

def increment():
    global counter
    counter += 1

threads = [Thread(target=increment) for _ in range(100)]
for t in threads: t.start()""",
        "correct_fix": """from threading import Lock

counter = 0
lock = Lock()

def increment():
    global counter
    with lock:
        counter += 1"""
    },
    {
        "id": "api_timeout",
        "title": "API Timeout",
        "bug_type": "API íƒ€ì„ì•„ì›ƒ",
        "description": "ì™¸ë¶€ API í˜¸ì¶œ ì‹œ íƒ€ì„ì•„ì›ƒ ì„¤ì • ëˆ„ë½",
        "buggy_code": """import requests

response = requests.get('https://api.example.com/data')
data = response.json()""",
        "correct_fix": """import requests

try:
    response = requests.get('https://api.example.com/data', timeout=5)
    response.raise_for_status()
    data = response.json()
except requests.exceptions.Timeout:
    print("Request timed out")
except requests.exceptions.RequestException as e:
    print(f"Request failed: {e}")"""
    }
]

# 5ê°€ì§€ í’ˆì§ˆ ë ˆë²¨ì˜ ë‹µë³€ í…œí”Œë¦¿
def generate_answer(case, quality_level):
    """í’ˆì§ˆ ë ˆë²¨ì— ë”°ë¥¸ ë‹µë³€ ìƒì„±"""

    if quality_level == "excellent":
        # ìš°ìˆ˜: ì •í™•í•œ ì›ì¸ + ë…¼ë¦¬ì  ì„¤ëª… + ì˜¬ë°”ë¥¸ ìˆ˜ì •
        return {
            "quality": "excellent",
            "expected_score_range": [85, 100],
            "step1_diagnosis": f"{case['bug_type']} ë¬¸ì œì…ë‹ˆë‹¤. {case['description']}ë¡œ ì¸í•´ ë°œìƒí–ˆìŠµë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ {case['buggy_code'][:50]}... ë¶€ë¶„ì—ì„œ ë¬¸ì œê°€ ë°œìƒí•˜ë©°, ì´ëŠ” ë°ì´í„° ë¬´ê²°ì„±/ë¡œì§ ì•ˆì •ì„±ì„ í•´ì¹©ë‹ˆë‹¤.",
            "step2_fix": case['correct_fix'],
            "step3_explanation": f"ì›ì¸ì€ {case['bug_type']}ì´ì—ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ {case['correct_fix'][:50]}...ì™€ ê°™ì´ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤. ì´ ìˆ˜ì •ìœ¼ë¡œ ë¬¸ì œì˜ ê·¼ë³¸ ì›ì¸ì´ ì œê±°ë˜ë©°, ë¶€ì‘ìš© ì—†ì´ ì•ˆì „í•˜ê²Œ ë™ì‘í•©ë‹ˆë‹¤. ì¶”ê°€ë¡œ ìœ ì‚¬í•œ ë¬¸ì œ ì¬ë°œì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ê²½ê³„ ì¡°ê±´ ì²´í¬ë„ ê°•í™”í–ˆìŠµë‹ˆë‹¤."
        }

    elif quality_level == "good":
        # ì–‘í˜¸: ì›ì¸ì€ ë§ì§€ë§Œ ì„¤ëª… ê°„ëµ
        return {
            "quality": "good",
            "expected_score_range": [70, 84],
            "step1_diagnosis": f"{case['bug_type']} ë¬¸ì œë¡œ ë³´ì…ë‹ˆë‹¤. {case['description'][:40]}... ë•Œë¬¸ì…ë‹ˆë‹¤.",
            "step2_fix": case['correct_fix'],
            "step3_explanation": f"{case['bug_type']} ë¬¸ì œì˜€ìŠµë‹ˆë‹¤. ì½”ë“œë¥¼ ìˆ˜ì •í•˜ì—¬ í•´ê²°í–ˆìŠµë‹ˆë‹¤."
        }

    elif quality_level == "average":
        # ë³´í†µ: ë°©í–¥ì€ ë§ì§€ë§Œ ë””í…Œì¼ ë¶€ì¡±
        return {
            "quality": "average",
            "expected_score_range": [55, 69],
            "step1_diagnosis": f"ì½”ë“œì— ë¬¸ì œê°€ ìˆëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.",
            "step2_fix": case['correct_fix'][:len(case['correct_fix'])//2] + "\n# ... ì¼ë¶€ ìˆ˜ì •",
            "step3_explanation": f"ë²„ê·¸ë¥¼ ì°¾ì•„ì„œ ê³ ì³¤ìŠµë‹ˆë‹¤."
        }

    elif quality_level == "poor":
        # ë¯¸í¡: ì›ì¸ ì¼ë¶€ë§Œ ë§ìŒ
        return {
            "quality": "poor",
            "expected_score_range": [35, 54],
            "step1_diagnosis": f"ë­”ê°€ ì˜ëª»ëœ ê²ƒ ê°™ìŠµë‹ˆë‹¤.",
            "step2_fix": case['buggy_code'] + "\n# ë³€ìˆ˜ëª…ë§Œ ë³€ê²½",
            "step3_explanation": f"ìˆ˜ì •í•´ë´¤ëŠ”ë° ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤."
        }

    else:  # very_poor
        # ë§¤ìš° ë¯¸í¡: ì›ì¸ í‹€ë¦¼ ë˜ëŠ” ì„¤ëª… ì—†ìŒ
        return {
            "quality": "very_poor",
            "expected_score_range": [0, 34],
            "step1_diagnosis": "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.",
            "step2_fix": case['buggy_code'],  # ìˆ˜ì • ì•ˆí•¨
            "step3_explanation": "ì˜ ëª¨ë¥´ê² ì–´ìš”."
        }

# 60ê°œ ìƒ˜í”Œ ìƒì„±
def generate_all_samples():
    samples = []

    quality_levels = ["excellent", "good", "average", "poor", "very_poor"]

    for case_idx, case in enumerate(BUG_CASES):
        for quality_idx, quality in enumerate(quality_levels):
            answer = generate_answer(case, quality)

            sample = {
                "sample_id": f"{case['id']}_{quality}",
                "case_id": case['id'],
                "case_title": case['title'],
                "bug_type": case['bug_type'],
                "quality_level": quality,
                "expected_score_range": answer['expected_score_range'],

                # Bug Hunt í‰ê°€ APIì— ì „ë‹¬í•  í˜•ì‹
                "missionTitle": case['title'],
                "steps": [
                    {
                        "step": 1,
                        "title": "ë²„ê·¸ ì§„ë‹¨",
                        "bug_type": case['bug_type'],
                        "instruction": case['description'],
                        "buggy_code": case['buggy_code']
                    },
                    {
                        "step": 2,
                        "title": "ì½”ë“œ ìˆ˜ì •",
                        "bug_type": case['bug_type'],
                        "instruction": "ë²„ê·¸ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”",
                        "buggy_code": case['buggy_code']
                    },
                    {
                        "step": 3,
                        "title": "ì„¤ëª… ì‘ì„±",
                        "bug_type": case['bug_type'],
                        "instruction": "ìˆ˜ì • ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”",
                        "buggy_code": case['buggy_code']
                    }
                ],
                "explanations": {
                    "1": answer['step1_diagnosis'],
                    "2": answer['step2_fix'],
                    "3": answer['step3_explanation']
                },
                "userCodes": {
                    "1": case['buggy_code'],
                    "2": answer['step2_fix'],
                    "3": answer['step2_fix']
                },
                "performance": {
                    "quizIncorrectCount": 0,
                    "codeSubmitFailCount": 0,
                    "hintCount": 0 if quality in ["excellent", "good"] else (1 if quality == "average" else 2),
                    "totalDebugTime": 180
                }
            }

            samples.append(sample)

    return samples

if __name__ == "__main__":
    print("ğŸ¯ Bug Hunt í‰ê°€ ê²€ì¦ìš© ìƒ˜í”Œ ìƒì„± ì‹œì‘...")

    samples = generate_all_samples()

    # ì €ì¥
    output_dir = os.path.join(os.path.dirname(__file__), '..', '..', 'data', 'validation')
    os.makedirs(output_dir, exist_ok=True)

    output_file = os.path.join(output_dir, 'bug_hunt_validation_samples.json')

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(samples, f, ensure_ascii=False, indent=2)

    print(f"âœ… ì´ {len(samples)}ê°œ ìƒ˜í”Œ ìƒì„± ì™„ë£Œ!")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_file}")
    print(f"\nğŸ“Š êµ¬ì„±:")
    print(f"   - ë²„ê·¸ ì¼€ì´ìŠ¤: {len(BUG_CASES)}ê°œ")
    print(f"   - í’ˆì§ˆ ë ˆë²¨: 5ê°œ (excellent, good, average, poor, very_poor)")
    print(f"   - ì´ ìƒ˜í”Œ: {len(BUG_CASES)} Ã— 5 = {len(samples)}ê°œ")
