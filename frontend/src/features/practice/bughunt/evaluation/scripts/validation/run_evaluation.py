"""
Bug Hunt í‰ê°€ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

60ê°œ ìƒ˜í”Œ Ã— 5íšŒ ë°˜ë³µ = 300íšŒ í‰ê°€ ìˆ˜í–‰
"""
import os
import sys
import json
import time
import django
from pathlib import Path

# Django ì„¤ì • ë¡œë“œ
backend_dir = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(backend_dir))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'config.settings')
django.setup()

from django.test import RequestFactory
from core.views.ai_view import BugHuntEvaluationView


class EvaluationRunner:
    """í‰ê°€ ì‹¤í–‰ê¸°"""

    def __init__(self, samples_file, output_file, num_trials=5):
        self.samples_file = samples_file
        self.output_file = output_file
        self.num_trials = num_trials
        self.factory = RequestFactory()
        self.view = BugHuntEvaluationView.as_view()

    def run_single_evaluation(self, sample):
        """ë‹¨ì¼ ìƒ˜í”Œì— ëŒ€í•œ í‰ê°€ ì‹¤í–‰"""
        # Django request ê°ì²´ ìƒì„±
        request = self.factory.post(
            '/api/ai/bug-hunt-evaluation/',
            data=json.dumps({
                'missionTitle': sample['missionTitle'],
                'steps': sample['steps'],
                'explanations': sample['explanations'],
                'userCodes': sample['userCodes'],
                'performance': sample['performance']
            }),
            content_type='application/json'
        )

        # í‰ê°€ ì‹¤í–‰
        response = self.view(request)

        if response.status_code == 200:
            return response.data
        else:
            return {
                'error': True,
                'status_code': response.status_code,
                'message': str(response.data) if hasattr(response, 'data') else 'Unknown error'
            }

    def run_all_evaluations(self):
        """ëª¨ë“  ìƒ˜í”Œì— ëŒ€í•´ ë°˜ë³µ í‰ê°€ ì‹¤í–‰"""
        # ìƒ˜í”Œ ë¡œë“œ
        with open(self.samples_file, 'r', encoding='utf-8') as f:
            samples = json.load(f)

        results = []
        total_evaluations = len(samples) * self.num_trials

        print(f"ğŸš€ í‰ê°€ ì‹œì‘: {len(samples)}ê°œ ìƒ˜í”Œ Ã— {self.num_trials}íšŒ = {total_evaluations}íšŒ")
        print(f"â±ï¸  ì˜ˆìƒ ì†Œìš” ì‹œê°„: {total_evaluations * 3 / 60:.1f}ë¶„")

        start_time = time.time()
        completed = 0

        for sample_idx, sample in enumerate(samples):
            sample_id = sample['sample_id']
            print(f"\n[{sample_idx + 1}/{len(samples)}] {sample_id} í‰ê°€ ì¤‘...")

            sample_results = {
                'sample_id': sample_id,
                'case_id': sample['case_id'],
                'quality_level': sample['quality_level'],
                'expected_score_range': sample['expected_score_range'],
                'trials': []
            }

            for trial in range(self.num_trials):
                try:
                    result = self.run_single_evaluation(sample)

                    if 'error' not in result:
                        trial_result = {
                            'trial': trial + 1,
                            'thinking_pass': result.get('thinking_pass', False),
                            'code_risk': result.get('code_risk', 50),
                            'thinking_score': result.get('thinking_score', 50),
                            'summary': result.get('ì´í‰', ''),
                            'step_feedbacks': result.get('step_feedbacks', [])
                        }

                        sample_results['trials'].append(trial_result)
                        completed += 1

                        print(f"  Trial {trial + 1}: ì‚¬ê³ ì ìˆ˜={trial_result['thinking_score']} "
                              f"ìœ„í—˜ë„={trial_result['code_risk']} "
                              f"í†µê³¼={'âœ…' if trial_result['thinking_pass'] else 'âŒ'}")
                    else:
                        print(f"  Trial {trial + 1}: âŒ ì˜¤ë¥˜ - {result.get('message', 'Unknown')}")
                        sample_results['trials'].append({
                            'trial': trial + 1,
                            'error': True,
                            'message': result.get('message', 'Unknown error')
                        })

                    # API Rate Limiting ë°©ì§€
                    time.sleep(0.5)

                except Exception as e:
                    print(f"  Trial {trial + 1}: âŒ ì˜ˆì™¸ ë°œìƒ - {str(e)}")
                    sample_results['trials'].append({
                        'trial': trial + 1,
                        'error': True,
                        'message': str(e)
                    })

            results.append(sample_results)

            # ì§„í–‰ë¥  í‘œì‹œ
            progress = (completed / total_evaluations) * 100
            elapsed = time.time() - start_time
            print(f"  ì§„í–‰ë¥ : {completed}/{total_evaluations} ({progress:.1f}%) | "
                  f"ê²½ê³¼ ì‹œê°„: {elapsed / 60:.1f}ë¶„")

        # ê²°ê³¼ ì €ì¥
        os.makedirs(os.path.dirname(self.output_file), exist_ok=True)
        with open(self.output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'metadata': {
                    'total_samples': len(samples),
                    'trials_per_sample': self.num_trials,
                    'total_evaluations': total_evaluations,
                    'completed_evaluations': completed,
                    'elapsed_time_seconds': time.time() - start_time
                },
                'results': results
            }, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… í‰ê°€ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ ì €ì¥: {self.output_file}")
        print(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {(time.time() - start_time) / 60:.1f}ë¶„")

        return results


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='Bug Hunt í‰ê°€ ì‹¤í–‰')
    parser.add_argument('--trials', type=int, default=5, help='ìƒ˜í”Œë‹¹ ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸: 5)')
    parser.add_argument('--quick', action='store_true', help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ê° í’ˆì§ˆë³„ 1ê°œ ìƒ˜í”Œë§Œ)')
    args = parser.parse_args()

    # íŒŒì¼ ê²½ë¡œ
    data_dir = Path(__file__).resolve().parent.parent.parent / 'data' / 'validation'
    samples_file = data_dir / 'bug_hunt_validation_samples.json'
    output_file = data_dir / 'evaluation_results.json'

    # Quick ëª¨ë“œ: í…ŒìŠ¤íŠ¸ìš© (5ê°œ ìƒ˜í”Œë§Œ)
    if args.quick:
        print("âš¡ Quick ëª¨ë“œ: í’ˆì§ˆë³„ 1ê°œ ìƒ˜í”Œë§Œ í‰ê°€")
        with open(samples_file, 'r', encoding='utf-8') as f:
            all_samples = json.load(f)

        # ê° í’ˆì§ˆ ë ˆë²¨ì—ì„œ ì²« ë²ˆì§¸ ìƒ˜í”Œë§Œ ì„ íƒ
        test_samples = []
        for quality in ['excellent', 'good', 'average', 'poor', 'very_poor']:
            sample = next((s for s in all_samples if s['quality_level'] == quality), None)
            if sample:
                test_samples.append(sample)

        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        test_samples_file = data_dir / 'test_samples.json'
        with open(test_samples_file, 'w', encoding='utf-8') as f:
            json.dump(test_samples, f, ensure_ascii=False, indent=2)

        samples_file = test_samples_file
        output_file = data_dir / 'test_evaluation_results.json'

    # í‰ê°€ ì‹¤í–‰
    runner = EvaluationRunner(samples_file, output_file, num_trials=args.trials)
    runner.run_all_evaluations()
