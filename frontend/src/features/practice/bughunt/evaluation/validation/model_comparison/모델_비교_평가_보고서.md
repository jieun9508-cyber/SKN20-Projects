# Bug Hunt LLM 모델 비교 평가 보고서

## 📋 개요

**평가 목적**: Bug Hunt 평가 시스템에 최적화된 LLM 모델 선정
**평가 기간**: 2026년 2월
**평가 모델**: gpt-4o, gpt-4o-mini, gpt-3.5-turbo (3개 모델 모두 Full 평가)
**검증 샘플**: 60개 (12개 버그 유형 × 5개 품질 레벨)
**평가 횟수**: 900회 (3개 모델 × 60개 샘플 × 5회 반복)
**실제 완료**: 882/900회 (gpt-4o-mini 일부 오류 발생)

---

## 🎯 실험 설계

### 모델 선정 과정

#### 1단계: 후보 모델 선정

**OpenAI 모델**:
- `gpt-4o`: 최신 GPT-4 Optimized 모델 (고성능)
- `gpt-4o-mini`: 경량화된 GPT-4 Optimized 모델 (현재 사용 중)
- `gpt-3.5-turbo`: GPT-3.5 Turbo 모델 (비교군)

**Hugging Face 모델** (시도했으나 실패):
- `Llama-3.1-70B`: Meta의 Llama 3.1 70B Instruct
- `Mixtral-8x7B`: Mistral AI의 Mixtral 8x7B Instruct
- **실패 원인**: Hugging Face Inference API HTTP 410 에러 (무료 API deprecated)

#### 2단계: Quick 테스트 (5개 샘플 × 3회 반복)

**목적**: 빠른 사전 검증으로 모델 특성 파악

**Quick 테스트 결과**:

| 모델 | 구분력 | 일관성 | 특징 |
|------|--------|--------|------|
| gpt-4o | 58.3점 | 표준편차 2.83 | 구분력 우수 |
| gpt-4o-mini | 40.0점 | 표준편차 1.89 | 균형적 |
| gpt-3.5-turbo | 0.0점 | 표준편차 0.0 | **모든 샘플 70점** |

**Quick 테스트 판단**:
- gpt-3.5-turbo는 Quick 테스트에서 품질 구분 실패
- 하지만 **정확한 검증을 위해 3개 모델 모두 Full 평가 실행**
- 최종 판단은 Full 평가 결과(60 샘플 × 5회)로 결정

#### 3단계: Full 평가 (60개 샘플 × 5회 반복)

**3개 모델 모두 Full 평가 완료**:
- gpt-4o: 300/300 평가 완료 ✅
- gpt-4o-mini: 282/300 평가 완료 (18회 오류)
- gpt-3.5-turbo: 300/300 평가 완료 ✅

---

## 📊 평가 방법론

### 평가 지표

#### 1. 일관성 (Consistency) - ⬇️ 낮을수록 좋음

- **측정**: 동일 샘플 5회 반복 평가의 표준편차
- **목표**: ≤ 5점
- **의미**: 낮을수록 평가가 안정적

**왜 낮을수록 좋은가?**

```
gpt-4o-mini (표준편차 0.83점):
똑같은 답변을 5번 평가:
시도 1: 70점
시도 2: 70점
시도 3: 70점
시도 4: 70점
시도 5: 70점
→ 평균 70점, 표준편차 0점 ✅
→ 학생이 재제출해도 동일한 점수

gpt-4o (표준편차 2.46점):
똑같은 답변을 5번 평가:
시도 1: 65점
시도 2: 72점
시도 3: 68점
시도 4: 75점
시도 5: 70점
→ 평균 70점, 표준편차 3.74점 ❌
→ 학생이 재제출하면 65~75점으로 10점 차이!
→ 교육 공정성 문제
```

**교육적 중요성**:
- 동일한 답변은 동일한 점수를 받아야 함
- 재평가 시 점수 변동이 크면 학생들의 신뢰 상실
- 일관성이 낮으면 "운에 따라 점수가 바뀐다"는 인식

#### 2. 구분력 (Discrimination) - ⬆️ 높을수록 좋음

- **측정**: 우수 케이스 평균 - 매우 미흡 케이스 평균
- **목표**: ≥ 30점
- **의미**: 높을수록 품질 차이를 잘 구분

#### 3. 순위 정확도 (Ranking Accuracy) - ⬆️ 높을수록 좋음

- **측정**: Kendall's Tau (품질 레벨 순서 일치도)
- **목표**: ≥ 0.8
- **의미**: 높을수록 품질 순서를 정확히 평가

#### 4. 속도 (Speed) - ⬇️ 낮을수록 좋음

- **측정**: 평균 응답 시간 (초)
- **의미**: 낮을수록 빠름

#### 5. 비용 (Cost) - ⬇️ 낮을수록 좋음

- **측정**: 총 API 비용 (USD)
- **의미**: 낮을수록 경제적

#### 6. 신뢰성 (Reliability) - ⬇️ 낮을수록 좋음

- **측정**: 오류율 (%)
- **목표**: < 5%
- **의미**: 낮을수록 안정적

### 평가 데이터

**60개 검증 샘플 구성**:
- 12개 버그 유형: Data Leakage, Off-by-One, Null Pointer, API Timeout 등
- 5개 품질 레벨: Excellent(85-100), Good(70-84), Average(55-69), Poor(35-54), Very Poor(0-34)

**평가 방법**:
- 각 샘플당 5회 반복 평가 (일관성 측정)
- 총 900회 평가 (3개 모델 × 60개 샘플 × 5회)

---

## 🔍 Full 평가 결과

### 종합 비교표

| 지표 | gpt-4o | gpt-4o-mini | gpt-3.5-turbo | 우위 |
|------|---------|-------------|---------------|------|
| **일관성** (표준편차 ⬇️) | 2.46점 | **0.83점** | 1.55점 | 🏆 mini |
| **구분력** (점수 차이 ⬆️) | **53.17점** | 40.00점 | 4.42점 | 🏆 4o |
| **순위 정확도** (Kendall τ ⬆️) | **1.000** | 0.949 | 0.400 | 🏆 4o |
| **속도** (초 ⬇️) | 8.60초 | 7.07초 | **4.55초** | 🏆 turbo |
| **비용** (USD ⬇️) | $2.27 | **$0.12** | $0.52 | 🏆 mini |
| **오류율** (% ⬇️) | **0.0%** | 6.0% | **0.0%** | 🏆 4o/turbo |
| **종합 점수** | 5.50점 | **6.71점** | 5.33점 | 🏆 mini |

**우위 항목 개수**:
- gpt-4o: 3개 (구분력, 순위 정확도, 오류율)
- gpt-4o-mini: 3개 (일관성, 속도, 비용)
- gpt-3.5-turbo: 1개 (속도)

### 상세 분석

#### 1. 일관성 분석

**gpt-4o-mini 압도적 우위** (표준편차 0.83)

```
gpt-4o-mini:
- 평균 표준편차: 0.83점 ✅ (목표 5점 대비 크게 우수)
- 최대 표준편차: 8.00점
- 최소 표준편차: 0.00점
→ gpt-4o보다 3배 더 일관적!

gpt-4o:
- 평균 표준편차: 2.46점
- 최대 표준편차: 9.17점
- 최소 표준편차: 0.00점

gpt-3.5-turbo:
- 평균 표준편차: 1.55점
- 최대 표준편차: 9.80점
- 최소 표준편차: 0.00점
```

**해석**:
- gpt-4o-mini가 동일 샘플에 대해 가장 일관된 점수 부여
- 교육 공정성 측면에서 가장 중요한 지표
- 학생들이 재제출 시 점수 변동 최소화

#### 2. 구분력 분석

**gpt-4o 우위** (점수 차이 53.17점)

**gpt-4o 품질별 평균 점수**:
```
Excellent:   76.5점
Good:        73.8점 (↓ 2.7점)
Average:     48.4점 (↓ 25.4점)
Poor:        29.8점 (↓ 18.6점)
Very Poor:   23.3점 (↓ 6.5점)

구분력: 76.5 - 23.3 = 53.2점 ✅
```

**gpt-4o-mini 품질별 평균 점수**:
```
Excellent:   70.0점
Good:        70.0점 (↓ 0.0점) ⚠️ 구분 실패
Average:     56.5점 (↓ 13.5점)
Poor:        38.4점 (↓ 18.1점)
Very Poor:   30.0점 (↓ 8.4점)

구분력: 70.0 - 30.0 = 40.0점 ✅ (목표 30점 달성)
```

**gpt-3.5-turbo 품질별 평균 점수**:
```
Excellent:   74.1점
Good:        72.1점 (↓ 2.0점)
Average:     64.0점 (↓ 8.1점)
Poor:        69.3점 (↑ 5.3점) ❌ 역전!
Very Poor:   69.7점 (↑ 0.4점) ❌ 역전!

구분력: 74.1 - 69.7 = 4.4점 ❌ (목표 30점 크게 미달)
```

**심각한 문제**: gpt-3.5-turbo는 Poor(69.3점)와 Very Poor(69.7점)를 Average(64.0점)보다 **높게** 평가 (순위 역전)

#### 3. 순위 정확도 분석

**gpt-4o 완벽, gpt-3.5-turbo 실패**

```
gpt-4o: τ = 1.000, p = 0.017 ✅ (완벽한 순위 유지)
gpt-4o-mini: τ = 0.949, p = 0.023 ✅ (목표 0.8 달성)
gpt-3.5-turbo: τ = 0.400, p = 0.483 ❌ (목표 0.8 미달, 통계적 유의미성도 없음)
```

**해석**:
- gpt-4o: 품질 레벨 순서 100% 정확
- gpt-4o-mini: 미세한 오류 (Excellent=Good)
- gpt-3.5-turbo: 품질 순서 심각하게 혼란 (Poor > Average)

#### 4. 속도 분석

**gpt-3.5-turbo 가장 빠름**

```
gpt-3.5-turbo: 평균 4.55초/건
gpt-4o-mini: 평균 7.07초/건 (55% 느림)
gpt-4o: 평균 8.60초/건 (89% 느림)
```

**해석**: gpt-3.5-turbo가 가장 빠르지만 정확도가 매우 낮아 의미 없음

#### 5. 비용 분석

**gpt-4o-mini 압도적 우위**

```
300회 평가 기준:
- gpt-4o-mini: $0.12 (100%)
- gpt-3.5-turbo: $0.52 (433%)
- gpt-4o: $2.27 (1,892%)

비용 절감: gpt-4o 대비 94.7% ✅

연간 예상 비용 (10,000회 평가 기준):
- gpt-4o-mini: $402
- gpt-3.5-turbo: $1,729
- gpt-4o: $7,578
```

**해석**: gpt-4o-mini가 가장 경제적 (gpt-4o의 5%, gpt-3.5-turbo의 23%)

#### 6. 오류율 분석

**gpt-4o-mini만 오류 발생**

```
gpt-4o: 0/300 오류 (0.0%) ✅
gpt-3.5-turbo: 0/300 오류 (0.0%) ✅
gpt-4o-mini: 18/300 오류 (6.0%) ⚠️ (목표 5% 초과)
```

### 왜 gpt-4o-mini만 오류가 발생했는가?

#### 원인 분석

**1. API Rate Limit (가장 가능성 높음)**

```
gpt-4o-mini는 저렴한 모델:
- 300회 비용: $0.12
- 분당 요청 제한 더 엄격
- 무료/저렴한 티어 사용자에게 낮은 우선순위

300회 연속 평가 중:
→ "Rate limit exceeded" 에러
→ Retry 로직 없이 실패
```

**2. Premium 모델 우선순위**

```
gpt-4o (Premium):
- 300회 비용: $2.27
- OpenAI가 고가 모델에 높은 우선순위 부여
- 더 많은 서버 리소스 할당
- 더 관대한 rate limit
→ 오류 0%

gpt-4o-mini (Economy):
- 300회 비용: $0.12 (1/19)
- 낮은 우선순위
- 제한된 리소스
→ 오류 6%
```

**3. 간헐적 서버 문제**

```
일시적 문제:
- 모델 인스턴스 재시작
- 로드 밸런싱 중 일부 요청 실패
- 네트워크 타임아웃
```

#### 해결 방법

**즉시 적용 가능 (코드 수정)**:

```python
import time
from openai import OpenAI

def evaluate_with_retry(sample, max_retries=3, retry_delay=2.0):
    """Retry 로직이 있는 평가 함수"""

    for attempt in range(max_retries):
        try:
            # API 호출
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[...],
                timeout=60  # timeout 증가 (30초 → 60초)
            )

            return response

        except Exception as e:
            if attempt < max_retries - 1:
                print(f"오류 발생 (시도 {attempt+1}/{max_retries}): {e}")
                print(f"{retry_delay}초 후 재시도...")
                time.sleep(retry_delay)
            else:
                print(f"최종 실패: {e}")
                raise

    # 요청 간 대기 (rate limit 완화)
    time.sleep(1.0)  # 매 요청마다 1초 대기
```

**예상 효과**:

```
Before (현재):
- 오류율: 6.0% (18/300)
- 연속 요청으로 rate limit 초과

After (개선 후):
- 오류율: < 1% 예상
- Retry 3회 → 일시적 오류 해결
- 1초 대기 → rate limit 여유
```

#### 비용-안정성 트레이드오프

```
gpt-4o:
- 비용: $2.27 (19배 비쌈)
- 안정성: 100% (오류 0%)

gpt-4o-mini (현재):
- 비용: $0.12
- 안정성: 94% (오류 6%)

gpt-4o-mini (개선 후 예상):
- 비용: $0.12 (동일)
- 안정성: 99%+ (오류 <1%)
```

---

## 🏆 종합 순위 및 선정 이유

### 최종 선정: gpt-4o-mini

**종합 점수**: 6.71점 (1위)

### 왜 gpt-4o가 우위 항목 3개인데 mini가 1등인가?

#### 우위 항목 개수

```
gpt-4o 우위: 3개
- 구분력 (53.17 vs 40.00)
- 순위 정확도 (1.000 vs 0.949)
- 오류율 (0% vs 6%)

gpt-4o-mini 우위: 3개
- 일관성 (0.83 vs 2.46)
- 속도 (7.07초 vs 8.60초)
- 비용 ($0.12 vs $2.27)
```

**동점인데 왜 mini가 1등?** → **차이의 크기가 다르기 때문!**

#### 종합 점수 계산 방식

**1단계: 정규화 (0-1 범위)**

각 지표를 0(최악)-1(최고) 범위로 변환:

```
예시) 일관성 (낮을수록 좋음):
값: 4o=2.46, mini=0.83, turbo=1.55
min=0.83, max=2.46, range=1.63

정규화 (역방향 - 낮을수록 좋음):
4o: 1 - (2.46-0.83)/1.63 = 0.00 (최악)
mini: 1 - (0.83-0.83)/1.63 = 1.00 (최고)
turbo: 1 - (1.55-0.83)/1.63 = 0.56
```

**2단계: 가중치 적용**

```python
가중치:
- 일관성: 2.0      (교육 공정성 중요!)
- 구분력: 2.0      (평가 능력 중요!)
- 순위 정확도: 1.5 (품질 인식)
- 속도: 1.0        (사용자 경험)
- 비용: 1.5        (경제성)
- 오류율: 2.0      (안정성 중요!)
```

**3단계: 실제 점수 계산**

**gpt-4o-mini가 큰 차이로 이긴 항목**:

```
일관성 (가중치 2.0):
- mini: 1.00 × 2.0 = 2.00점 ✅
- 4o: 0.00 × 2.0 = 0.00점
→ mini가 +2.00점 리드 (차이가 3배!)

비용 (가중치 1.5):
- mini: 1.00 × 1.5 = 1.50점 ✅
- 4o: 0.00 × 1.5 = 0.00점
→ mini가 +1.50점 리드 (차이가 19배!)

속도 (가중치 1.0):
- mini: 0.36 × 1.0 = 0.36점
- 4o: 0.00 × 1.0 = 0.00점
→ mini가 +0.36점 리드
```

**gpt-4o가 작은 차이로 이긴 항목**:

```
구분력 (가중치 2.0):
- 4o: 1.00 × 2.0 = 2.00점 ✅
- mini: 0.73 × 2.0 = 1.46점
→ 4o가 +0.54점 만회 (둘 다 목표 달성, 차이 작음)

순위 정확도 (가중치 1.5):
- 4o: 1.00 × 1.5 = 1.50점 ✅
- mini: 0.92 × 1.5 = 1.38점
→ 4o가 +0.12점 만회 (둘 다 목표 달성, 차이 매우 작음)

오류율 (가중치 2.0):
- 4o: 1.00 × 2.0 = 2.00점 ✅
- mini: 0.00 × 2.0 = 0.00점
→ 4o가 +2.00점 만회
```

**최종 계산**:

```
gpt-4o-mini 누적:
+ 일관성: +2.00점
+ 비용: +1.50점
+ 속도: +0.36점
= +3.86점

gpt-4o 만회:
+ 구분력: +0.54점
+ 순위 정확도: +0.12점
+ 오류율: +2.00점
= +2.66점

차이: mini가 1.20점 앞섬
(실제 최종: mini 6.71 vs 4o 5.50 = 1.21점 차이)
```

### 핵심 인사이트

**우위 항목 개수가 아닌 차이의 크기가 중요**:

```
mini가 이긴 이유:
1. 일관성 차이: 3배 (0.83 vs 2.46)
2. 비용 차이: 19배 ($0.12 vs $2.27)
→ 압도적 차이로 높은 점수

4o가 이긴 이유:
1. 구분력 차이: 1.3배 (53.17 vs 40.00)
   - 하지만 둘 다 목표 30점 달성
2. 순위 정확도 차이: 5% (1.000 vs 0.949)
   - 하지만 둘 다 목표 0.8 달성
3. 오류율 차이: 6% (0% vs 6%)
   - 코드 개선으로 해결 가능
→ 상대적으로 작은 차이
```

---

## 💡 결론 및 모델 선정

### 최종 선정: gpt-4o-mini

**선정 근거**:

#### ✅ 결정적 장점

1. **압도적 일관성 (가장 중요)**
   - 표준편차 0.83점 (gpt-4o의 1/3 수준)
   - **교육 공정성 측면에서 가장 중요**: 동일한 답변에 안정적인 점수 부여
   - 학생들이 재제출 시 점수 변동 최소화
   - 평가 시스템에 대한 신뢰도 향상

2. **최고 비용 효율성**
   - gpt-4o 대비 94.7% 비용 절감 ($2.27 → $0.12)
   - gpt-3.5-turbo 대비도 77% 절감 ($0.52 → $0.12)
   - 연간 10,000회 기준 $7,176 절감
   - 장기 운영 시 큰 경제적 이득

3. **충분한 구분력**
   - 40점 차이로 목표 기준(30점) 달성 ✅
   - gpt-3.5-turbo(4.4점)와 비교 시 9배 더 우수
   - 실제 사용에 문제 없는 수준

4. **우수한 순위 정확도**
   - Kendall τ 0.949 (목표 0.8 크게 초과) ✅
   - 대부분의 품질 레벨 순서 정확히 유지

#### ⚠️ 개선 필요 사항

1. **오류율 6.0%**
   - 목표 5% 초과 (18/300 평가 실패)
   - 원인: API rate limit, 낮은 우선순위
   - **해결**: Retry 로직 강화 → 1% 이하로 개선 가능
   - 근본적 문제 아님 (코드 개선으로 해결)

2. **Excellent vs Good 구분 부족**
   - 현재: 둘 다 70점
   - 원인: 평가 프롬프트가 미세한 차이를 구분하기 어려움
   - **해결**: 프롬프트 개선 (체크리스트, Few-shot 예시)

---

## ❌ gpt-4o를 선택하지 않은 이유

**장점**:
- 구분력 53.2점 (최우수) ✅
- 순위 정확도 100% (완벽) ✅
- 오류율 0% (완벽) ✅

**결정적 단점**:
- **비용 19배** ($2.27 vs $0.12)
- **일관성 낮음** (표준편차 2.46 vs 0.83, 3배 차이)

**판단**:
- gpt-4o의 성능 향상이 **19배 비용 증가를 정당화하기 어려움**
- 구분력 차이(+13점): 둘 다 목표 달성, 실용적 차이 작음
- 순위 정확도 차이(+5%): 둘 다 목표 달성, 미세한 차이
- 일관성 낮음: **교육 공정성 측면에서 불리**
- gpt-4o-mini의 약점은 **프롬프트/코드 개선으로 해결 가능**

**비용-성능 분석**:

```
연간 10,000회 평가 기준:

gpt-4o:
- 비용: $7,578
- 구분력: 53.2점
- 일관성: 2.46 (낮음)

gpt-4o-mini:
- 비용: $402 (5%)
- 구분력: 40.0점 (충분)
- 일관성: 0.83 (우수)
- 절감액: $7,176
```

---

## ❌ gpt-3.5-turbo를 선택하지 않은 이유

**심각한 문제점**:

1. **구분력 4.4점** ❌
   - 목표 30점에 85% 미달
   - Excellent(74점) vs Very Poor(70점) 차이 거의 없음
   - 평가 시스템으로 **완전히 부적합**

2. **순위 정확도 0.400** ❌
   - 목표 0.8에 50% 미달
   - **Poor(69.3점)가 Average(64.0점)보다 높음** (순위 역전!)
   - 통계적 유의미성 없음 (p=0.48)

3. **비용도 gpt-4o-mini보다 4배 비쌈**
   - $0.52 vs $0.12
   - 속도는 빠르지만 정확도가 너무 낮아 무의미

**Quick 테스트 vs Full 평가 비교**:

```
Quick (5 샘플 × 3회):
- 모든 샘플 70점 (구분력 0)

Full (60 샘플 × 5회):
- 64~74점 범위 (구분력 4.4)
- 순위 역전 발생

결론: 샘플 수를 늘려도 근본적인 품질 구분 능력 부족
```

---

## 📈 개선 계획

### 1. 오류율 개선 (즉시 적용)

**목표**: 6.0% → <1%

**방법**:

```python
# 1. Retry 로직 강화
max_retries = 3
retry_delay = 2.0  # 초

# 2. Timeout 증가
timeout = 60  # 30초 → 60초

# 3. Rate limit 완화
request_interval = 1.0  # 요청 간 1초 대기

# 4. Exponential backoff
retry_delay = 2.0 ** attempt  # 2초, 4초, 8초
```

**예상 효과**:
- Retry 3회로 일시적 오류 대부분 해결
- 요청 간 대기로 rate limit 여유
- 오류율 6% → <1%

### 2. 프롬프트 개선 (단기)

**현재 문제**: Excellent vs Good 구분 부족 (둘 다 70점)

**개선 방법**:

#### A. 평가 기준 체계화 (체크리스트)

```
총 100점:

□ 버그 원인 정확히 언급 (20점)
  - 우수(18-20점): 근본 원인까지 설명
  - 양호(15-17점): 직접 원인만 언급
  - 보통(10-14점): 원인 언급했으나 불명확

□ 수정-원인 논리적 연결 (20점)
  - 우수(18-20점): 인과관계 명확
  - 양호(15-17점): 연결은 있으나 모호
  - 보통(10-14점): 연결 약함

□ 구체적 해결 방법 (20점)
  - 우수(18-20점): 구체적이고 실행 가능
  - 양호(15-17점): 방향성은 맞으나 추상적
  - 보통(10-14점): 모호하거나 불완전

□ 부작용 고려 (20점)
  - 우수(18-20점): 부작용 명시하고 대응 방안 제시
  - 양호(15-17점): 부작용 언급
  - 보통(10-14점): 부작용 미고려

□ 설명 명확성 (20점)
  - 우수(18-20점): 기술적으로 정확하고 이해하기 쉬움
  - 양호(15-17점): 대체로 명확
  - 보통(10-14점): 모호하거나 부정확
```

#### B. Few-shot 예시 추가

```
예시 1 (Excellent, 85점):
[문제] Data Leakage 버그
[사용자 답변] "train_test_split 전에 스케일링을 하면 테스트 데이터 정보가 학습에 유출됩니다.
             스케일링은 train 데이터로만 fit하고 test는 transform만 해야 합니다."
[수정 코드] scaler.fit(X_train); X_train_scaled = scaler.transform(X_train)

[채점]
- 원인 (20점): 근본 원인(정보 유출) 정확히 파악
- 논리 (19점): 원인-수정 완벽하게 연결
- 해결 (20점): 구체적이고 정확한 해결책
- 부작용 (18점): transform만 사용해야 한다고 명시
- 명확성 (20점): 기술적으로 정확하고 명확
총점: 97점 (Excellent)

---

예시 2 (Good, 75점):
[문제] Data Leakage 버그
[사용자 답변] "전체 데이터로 스케일링하면 안됩니다. train만 사용해야 합니다."
[수정 코드] scaler.fit(X_train)

[채점]
- 원인 (15점): 직접 원인만 언급, 왜 안되는지 설명 부족
- 논리 (16점): 원인-수정 연결되나 자세하지 않음
- 해결 (17점): 방향은 맞으나 transform 언급 없음
- 부작용 (14점): 부작용 고려 부족
- 명확성 (16점): 간결하나 자세한 설명 부족
총점: 78점 (Good)

Excellent와 Good의 차이:
- 근본 원인 vs 직접 원인
- 상세한 설명 vs 간결한 설명
- 부작용 명시 vs 부작용 미언급
```

#### C. 구조화된 출력 형식

```json
{
  "scores": {
    "cause_identification": 18,
    "logic_connection": 16,
    "solution_quality": 17,
    "side_effects": 15,
    "explanation_clarity": 19
  },
  "total_score": 85,
  "quality_level": "Excellent",
  "key_strength": "근본 원인까지 정확히 파악하고 부작용 고려",
  "improvement_needed": "해결 방법을 더 구체적으로 설명하면 완벽"
}
```

#### 예상 효과

**Before** (현재):
```
Excellent: 70점
Good: 70점  ← 구분 실패!
Average: 56.5점
```

**After** (개선 후 예상):
```
Excellent: 85점 (+15점)
Good: 75점 (+5점) ← 구분 가능!
Average: 50점 (-6.5점, 더 엄격)

구분력: 40점 → 55점 (38% 향상!)
```

---

## 📁 부록

### A. 파일 구조

```
data/validation/model_comparison/
├── 모델_비교_평가_보고서.md          # 이 파일
├── model_comparison_analysis.json    # 분석 결과 (JSON)
├── model_comparison_results.json     # 통합 평가 결과 (91KB)
├── gpt-4o_results.json              # gpt-4o Full 평가 (601KB, 300회)
├── gpt-4o-mini_results.json         # gpt-4o-mini Full 평가 (510KB, 282회)
├── gpt-3.5-turbo_results.json       # gpt-3.5-turbo Full 평가 (9,071줄, 300회)
├── Llama-3.1-70B_results.json       # Llama 실패 로그 (3.4KB)
└── visualizations/                   # 시각화 그래프
    ├── consistency_comparison.png    # 일관성 비교
    ├── discrimination_comparison.png # 구분력 비교
    ├── performance_comparison.png    # 성능(속도/비용) 비교
    ├── quality_distribution.png      # 품질별 점수 분포
    ├── overall_ranking.png           # 종합 순위
    └── radar_chart.png               # 레이더 차트
```

### B. 평가 환경

- **실행 환경**: Docker 컨테이너 (skn20-final-5team-backend-1)
- **프로그래밍 언어**: Python 3
- **주요 라이브러리**: openai, requests, numpy, matplotlib, scipy
- **평가 기간**: 2026-02-05
- **총 소요 시간**: 약 1시간 48분 (6,498초)
- **총 평가 횟수**: 882/900회 (완료율 98.0%)

### C. 모델별 상세 지표

#### gpt-4o

```json
{
  "consistency": {
    "avg_std_dev": 2.46,
    "max_std_dev": 9.17,
    "min_std_dev": 0.0
  },
  "discrimination": {
    "excellent_avg": 76.5,
    "very_poor_avg": 23.3,
    "score_diff": 53.2
  },
  "ranking": {
    "kendall_tau": 1.000,
    "p_value": 0.017
  },
  "performance": {
    "avg_time": 8.60,
    "total_cost": 2.27,
    "total_tokens": 554431,
    "total_evaluations": 300
  },
  "error_rate": {
    "error_count": 0,
    "total_count": 300,
    "error_rate": 0.0
  }
}
```

#### gpt-4o-mini

```json
{
  "consistency": {
    "avg_std_dev": 0.83,
    "max_std_dev": 8.0,
    "min_std_dev": 0.0
  },
  "discrimination": {
    "excellent_avg": 70.0,
    "very_poor_avg": 30.0,
    "score_diff": 40.0
  },
  "ranking": {
    "kendall_tau": 0.949,
    "p_value": 0.023
  },
  "performance": {
    "avg_time": 7.07,
    "total_cost": 0.12,
    "total_tokens": 540797,
    "total_evaluations": 282
  },
  "error_rate": {
    "error_count": 18,
    "total_count": 300,
    "error_rate": 6.0
  }
}
```

#### gpt-3.5-turbo

```json
{
  "consistency": {
    "avg_std_dev": 1.55,
    "max_std_dev": 9.80,
    "min_std_dev": 0.0
  },
  "discrimination": {
    "excellent_avg": 74.1,
    "very_poor_avg": 69.7,
    "score_diff": 4.4
  },
  "ranking": {
    "kendall_tau": 0.400,
    "p_value": 0.483
  },
  "performance": {
    "avg_time": 4.55,
    "total_cost": 0.52,
    "total_tokens": 739762,
    "total_evaluations": 300
  },
  "error_rate": {
    "error_count": 0,
    "total_count": 300,
    "error_rate": 0.0
  }
}
```

### D. 종합 순위 계산 상세

**가중치**:
- 일관성: 2.0 (교육 공정성)
- 구분력: 2.0 (평가 능력)
- 순위 정확도: 1.5 (품질 인식)
- 속도: 1.0 (사용자 경험)
- 비용: 1.5 (경제성)
- 오류율: 2.0 (안정성)

**정규화 방법**:
- 낮을수록 좋음 (일관성, 속도, 비용, 오류율): `1 - (value - min) / range`
- 높을수록 좋음 (구분력, 순위 정확도): `(value - min) / range`

**최종 순위**:
1. **gpt-4o-mini**: 6.71점 ⭐
2. gpt-4o: 5.50점
3. gpt-3.5-turbo: 5.33점

---

## 📞 문의 및 추가 자료

**검증 담당**: SKN20 Final 5 Team
**검증 일시**: 2026-02-05
**관련 문서**:
- [LLM 검증 최종 보고서](../bughunt_validation/LLM_검증_최종_보고서.md)
- [모델 비교 시스템 README](../../../backend/scripts/README_MODEL_COMPARISON.md)

---

**최종 업데이트**: 2026-02-05
**문서 버전**: 3.0 (상세 설명 추가 - 일관성, 오류율, 종합 점수 계산)
