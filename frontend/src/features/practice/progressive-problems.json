{
  "progressiveProblems": [
    {
      "id": "P1",
      "project_title": "AI ì—”ì§€ë‹ˆì–´ë§ í•µì‹¬ ë””ë²„ê¹…",
      "scenario": "AI ëª¨ë¸ ê°œë°œ íŒŒì´í”„ë¼ì¸ì—ì„œ ë°œìƒí•˜ëŠ” ì‹¤ì œ ì—ëŸ¬ë“¤ì„ í•´ê²°í•˜ë©° ì—”ì§€ë‹ˆì–´ë§ ì—­ëŸ‰ì„ í‚¤ì›ë‹ˆë‹¤.",
      "difficulty": 2,
      "totalSteps": 3,
      "steps": [
        {
          "step": 1,
          "title": "í•™ìŠµì€ ë˜ëŠ”ë° ì„±ëŠ¥ì´ ì•ˆ ì˜¤ë¥´ì§€? (Numerical Stability)",
          "file_name": "model_train.py",
          "description": "ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ì´ ì—ëŸ¬ ì—†ì´ í•™ìŠµë˜ì§€ë§Œ accuracyê°€ 0.5 ê·¼ì²˜ì—ì„œ ê³ ì •ë˜ëŠ” í˜„ìƒì…ë‹ˆë‹¤.\n\n**ìƒí™©:**\n- ë°ì´í„°: 10ì°¨ì› ì…ë ¥, 10,000ê°œ ìƒ˜í”Œ, í´ë˜ìŠ¤ ë¹„ìœ¨ 50:50\n- Loss: 0.693(ln2) ê·¼ì²˜ì—ì„œ ì‹œì‘ â†’ 0.690 â†’ 0.688 (ë¯¸ì„¸ ê°ì†Œ í›„ ì •ì²´)\n- Accuracy: 0.48 ~ 0.52 ì‚¬ì´ ì§„ë™, 100 epoch ì´í›„ì—ë„ ë™ì¼\n- ì—ëŸ¬ ë©”ì‹œì§€ ì—†ìŒ\n\n**í•µì‹¬ ì§ˆë¬¸:** ì™œ Lossê°€ ln(2) â‰ˆ 0.693ì—ì„œ ì‹œì‘í• ê¹Œìš”? ì´ ìˆ«ìê°€ ì˜ë¯¸í•˜ëŠ” ë°”ëŠ”?",
          "objectives": [
            "Sigmoid + BCELoss ì¡°í•©ì˜ ìˆ˜ì¹˜ì  ë¶ˆì•ˆì •ì„± ì´í•´",
            "Gradient Saturation í˜„ìƒê³¼ ì›ì¸ íŒŒì•…",
            "Logits ê¸°ë°˜ ì†ì‹¤ í•¨ìˆ˜(BCEWithLogitsLoss)ì˜ ìˆ˜ì¹˜ ì•ˆì •ì„± ì›ë¦¬ ì´í•´"
          ],
          "buggy_code": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Linear(10, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        return self.sigmoid(self.fc(x))\n\nmodel = Model()\ncriterion = nn.BCELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# Training loop\nfor epoch in range(100):\n    pred = model(X_train)\n    loss = criterion(pred, y_train)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    print(f\"Epoch {epoch}: Loss {loss.item():.4f}\")",
          "correct_code": "import torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Linear(10, 1)\n        # Sigmoid ì œê±°: Logitsì„ ì§ì ‘ ì¶œë ¥\n\n    def forward(self, x):\n        return self.fc(x)\n\nmodel = Model()\n# Sigmoid + BCELossë¥¼ ë‚´ë¶€ì ìœ¼ë¡œ ê²°í•©í•˜ì—¬ ìˆ˜ì¹˜ ì•ˆì •ì„± í™•ë³´\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nfor epoch in range(100):\n    logits = model(X_train)\n    loss = criterion(logits, y_train)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    print(f\"Epoch {epoch}: Loss {loss.item():.4f}\")",
          "hints": [
            "Loss 0.693ì€ -log(0.5)ì…ë‹ˆë‹¤. ëª¨ë¸ì´ ëª¨ë“  ìƒ˜í”Œì— ëŒ€í•´ 'ë°˜ë°˜ ì°ê¸°'ë¥¼ í•˜ê³  ìˆë‹¤ëŠ” ëœ»ì´ì£ .",
            "Sigmoid ì¶œë ¥ì´ 0ì´ë‚˜ 1ì— ê°€ê¹Œì›Œì§€ë©´ BCELoss ë‚´ë¶€ì˜ log(p) ë˜ëŠ” log(1-p)ê°€ -âˆë¡œ ë°œì‚°í•©ë‹ˆë‹¤.",
            "BCEWithLogitsLossëŠ” ë‚´ë¶€ì ìœ¼ë¡œ log(1 + exp(-z)) í˜•íƒœ(LogSumExp trick)ë¥¼ ì‚¬ìš©í•´ ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤."
          ],
          "error_log": "Epoch 1: Loss 0.6931, Acc 0.50\nEpoch 10: Loss 0.6928, Acc 0.51\nEpoch 50: Loss 0.6925, Acc 0.49\nEpoch 100: Loss 0.6924, Acc 0.50\n[ê²°ê³¼] 100 epoch í›„ì—ë„ random guess ìˆ˜ì¤€",
          "success_log": "Epoch 1: Loss 0.6801, Acc 0.62\nEpoch 10: Loss 0.4502, Acc 0.85\nEpoch 50: Loss 0.1803, Acc 0.94\nEpoch 100: Loss 0.0892, Acc 0.97\n[ê²°ê³¼] ì •ìƒì ì¸ í•™ìŠµ ê³¡ì„ ",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "BCEWithLogitsLoss"
            ],
            "forbidden": [
              "BCELoss",
              "self.sigmoid"
            ]
          },
          "error_info": {
            "type": "Model Design Error (Gradient Saturation)",
            "description": "Sigmoid ì¶œë ¥ì´ 0 ë˜ëŠ” 1ì— ê°€ê¹Œì›Œì§€ë©´ ë‘ ê°€ì§€ ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤:\n1. BCELoss ë‚´ë¶€ log(p), log(1-p) ê³„ì‚°ì—ì„œ ìˆ˜ì¹˜ì  ë¶ˆì•ˆì • (log(0) â†’ -âˆ)\n2. Sigmoidì˜ ë¯¸ë¶„ê°’ì´ 0ì— ê°€ê¹Œì›Œì ¸ gradientê°€ ì†Œì‹¤ (Saturation)\n\nì´ë¡œ ì¸í•´ ëª¨ë¸ì´ ì´ˆê¸° random guess ìƒíƒœì—ì„œ ë²—ì–´ë‚˜ì§€ ëª»í•©ë‹ˆë‹¤.",
            "suggestion": "Logitsì„ ì§ì ‘ ì…ë ¥ë°›ëŠ” `BCEWithLogitsLoss`ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì´ í•¨ìˆ˜ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ìˆ˜ì¹˜ì ìœ¼ë¡œ ì•ˆì •ì ì¸ ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
          }
        },
        {
          "step": 2,
          "title": "ì‹¤í—˜ì€ ì„±ê³µ, ë°°í¬ëŠ” ì‹¤íŒ¨ (Pipeline Consistency)",
          "file_name": "pipeline.py",
          "description": "Validationì—ì„œëŠ” ì„±ëŠ¥ì´ ë§¤ìš° ì¢‹ì•˜ìœ¼ë‚˜, ì‹¤ì œ ì„œë¹„ìŠ¤ ë°°í¬ í›„ ì˜ˆì¸¡ê°’ì´ í•œìª½ í´ë˜ìŠ¤ë¡œ ì ë¦½ë‹ˆë‹¤.\n\n**ì¸¡ì •ëœ ìˆ˜ì¹˜:**\n- Train Accuracy: 0.94\n- Validation Accuracy: 0.91 (ì¢‹ì•„ ë³´ì„)\n- Production Accuracy: 0.58 (ë§í•¨)\n- Production ì˜ˆì¸¡ ë¶„í¬: Class 1ë¡œ 82% ì ë¦¼\n\n**ë°ì´í„° íŠ¹ì„±:**\n- Feature 20ê°œ, ê° featureì˜ scaleì´ 0.001 ~ 10,000ìœ¼ë¡œ ë‹¤ì–‘\n- Train 50,000ê±´, Validation 10,000ê±´\n\n**í•µì‹¬ ì§ˆë¬¸:** Validationë„ ì˜ ë‚˜ì™”ëŠ”ë° ì™œ Productionì—ì„œë§Œ ë§í• ê¹Œìš”?",
          "objectives": [
            "Train/Val/Serving íŒŒì´í”„ë¼ì¸ ì¼ê´€ì„±ì˜ ì¤‘ìš”ì„± ì´í•´",
            "Scaler ê°ì²´ì˜ ìƒíƒœ(state) ê´€ë¦¬ ì›ë¦¬ íŒŒì•…",
            "sklearn Pipelineì„ í†µí•œ êµ¬ì¡°ì  í•´ê²° ë°©ë²• ìŠµë“"
          ],
          "buggy_code": "from sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nimport joblib\n\nscaler = StandardScaler()\n\n# === í•™ìŠµ ë‹¨ê³„ ===\nX_train_scaled = scaler.fit_transform(X_train)  # Î¼_train, Ïƒ_train ì €ì¥\nmodel = LogisticRegression()\nmodel.fit(X_train_scaled, y_train)\n\n# === ê²€ì¦ ë‹¨ê³„ ===\nX_val_scaled = scaler.fit_transform(X_val)  # âš ï¸ Î¼_val, Ïƒ_valë¡œ ë®ì–´ì“°ê¸°!\nval_score = model.score(X_val_scaled, y_val)\nprint(f\"Validation Score: {val_score}\")\n\n# ì €ì¥\njoblib.dump(scaler, 'scaler.pkl')  # Î¼_val, Ïƒ_valì´ ì €ì¥ë¨\njoblib.dump(model, 'model.pkl')\n\n# === ì„œë¹™ ë‹¨ê³„ ===\nscaler = joblib.load('scaler.pkl')\nmodel = joblib.load('model.pkl')\nX_input_scaled = scaler.transform(X_input)  # Î¼_val ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜\npred = model.predict(X_input_scaled)",
          "correct_code": "from sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nimport joblib\n\n# === ë°©ë²• 1: transformë§Œ ì‚¬ìš© ===\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nmodel = LogisticRegression()\nmodel.fit(X_train_scaled, y_train)\n\n# Validation: transformë§Œ! (fit í•˜ì§€ ì•ŠìŒ)\nX_val_scaled = scaler.transform(X_val)\nval_score = model.score(X_val_scaled, y_val)\nprint(f\"Validation Score: {val_score}\")\n\n# === ë°©ë²• 2 (ê¶Œì¥): Pipeline ì‚¬ìš© ===\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', LogisticRegression())\n])\npipeline.fit(X_train, y_train)\nval_score = pipeline.score(X_val, y_val)\n\njoblib.dump(pipeline, 'pipeline.pkl')\n\n# ì„œë¹™: ë‹¨ì¼ ê°ì²´ë¡œ ì¼ê´€ì„± ë³´ì¥\npipeline = joblib.load('pipeline.pkl')\npred = pipeline.predict(X_input)",
          "hints": [
            "fit_transform()ì€ ë‘ ê°€ì§€ ì¼ì„ í•©ë‹ˆë‹¤: (1) ë°ì´í„°ë¡œë¶€í„° mean, std ê³„ì‚°(fit), (2) ë³€í™˜(transform). Validationì—ì„œ fitì„ ë‹¤ì‹œ í•˜ë©´?",
            "ë¶„í¬ ë¶ˆì¼ì¹˜ ì‹œê°í™”:\n  Train  â†’ fit_transform â†’ Î¼_A, Ïƒ_A ì €ì¥ â†’ Model í•™ìŠµ\n  Val    â†’ fit_transform â†’ Î¼_B, Ïƒ_Bë¡œ ë®ì–´ì“°ê¸°!\n  Live   â†’ transform     â†’ Î¼_B ê¸°ì¤€ ë³€í™˜ â†’ Model(Î¼_A ê¸°ì¤€)ê³¼ ë¶ˆì¼ì¹˜",
            "sklearnì˜ Pipelineì„ ì‚¬ìš©í•˜ë©´ preprocessingê³¼ modelì´ í•˜ë‚˜ë¡œ ë¬¶ì—¬ ì´ëŸ° ì‹¤ìˆ˜ë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
          ],
          "error_log": "=== í•™ìŠµ/ê²€ì¦ ë‹¨ê³„ ===\nTrain Accuracy: 0.94\nValidation Accuracy: 0.91 âœ“ (ì¢‹ì•„ ë³´ì„)\n\n=== ë°°í¬ í›„ ===\nProduction Accuracy: 0.58 âœ—\nPrediction Distribution: {0: 0.18, 1: 0.82}\n[ê²½ê³ ] ì˜ˆì¸¡ì´ Class 1ë¡œ ì‹¬í•˜ê²Œ í¸í–¥ë¨",
          "success_log": "=== í•™ìŠµ/ê²€ì¦ ë‹¨ê³„ ===\nTrain Accuracy: 0.94\nValidation Accuracy: 0.89 âœ“\n\n=== ë°°í¬ í›„ ===\nProduction Accuracy: 0.87 âœ“\nPrediction Distribution: {0: 0.73, 1: 0.27}\n[ì •ìƒ] Train/Val/Prod ì¼ê´€ëœ ì„±ëŠ¥",
          "solution_check": {
            "type": "multi_condition",
            "required_any": [
              "scaler.transform(X_val)",
              "Pipeline"
            ],
            "forbidden": [
              "fit_transform(X_val)"
            ]
          },
          "error_info": {
            "type": "Pipeline Consistency Error (Train-Serving Skew)",
            "description": "ê²€ì¦ ë°ì´í„°ì— fit_transform()ì„ ì‚¬ìš©í•˜ë©´ scalerì˜ ë‚´ë¶€ ìƒíƒœ(mean_, std_)ê°€ validation ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ë®ì–´ì“°ì—¬ì§‘ë‹ˆë‹¤.\n\nê²°ê³¼ì ìœ¼ë¡œ:\n- Model: X_trainì˜ ë¶„í¬(Î¼_A, Ïƒ_A)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•™ìŠµë¨\n- Serving: X_valì˜ ë¶„í¬(Î¼_B, Ïƒ_B)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì…ë ¥ ë³€í™˜\n- ì…ë ¥ ë¶„í¬ ë¶ˆì¼ì¹˜ë¡œ decision boundaryê°€ ì–´ê¸‹ë‚¨\n\nValidationì´ ì˜ ë‚˜ì˜¨ ì´ìœ : X_valë„ ìì²´ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”ë˜ì–´ meanâ‰ˆ0, stdâ‰ˆ1 í˜•íƒœê°€ ë˜ë¯€ë¡œ 'ìš°ì—°íˆ' ë™ì‘í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì„.",
            "suggestion": "Validation ë° Serving ë°ì´í„°ëŠ” ë°˜ë“œì‹œ Training ë°ì´í„°ë¡œ fitëœ scalerë¥¼ ì‚¬ìš©í•´ transform()ë§Œ í•´ì•¼ í•©ë‹ˆë‹¤. sklearn.pipeline.Pipeline ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
          }
        },
        {
          "step": 3,
          "title": "ì§€í‘œëŠ” ì™„ë²½, ì‚¬ì—…ì€ í­ë§ (Problem Definition)",
          "file_name": "feature_engineering.py",
          "description": "AUC 0.91, Accuracy 0.88ë¡œ ëª¨ë¸ ì„±ëŠ¥ì€ ì™„ë²½í•´ ë³´ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ ëª¨ë¸ë¡œ ë§ˆì¼€íŒ… ìº í˜ì¸ì„ ì§‘í–‰í–ˆë”ë‹ˆ ì „í™˜ìœ¨ì´ ì˜¤íˆë ¤ ë–¨ì–´ì§€ê³ , ì¶©ì„± ê³ ê° ë¶ˆë§Œë§Œ ëŠ˜ì—ˆìŠµë‹ˆë‹¤.\n\n**ì˜¤í”„ë¼ì¸ ì§€í‘œ:**\n- AUC: 0.91, Accuracy: 0.88, Precision: 0.85\n\n**ë¹„ì¦ˆë‹ˆìŠ¤ ê²°ê³¼:**\n- ìº í˜ì¸ ì „ ì „í™˜ìœ¨: 12%\n- ìº í˜ì¸ í›„ ì „í™˜ìœ¨: 7% (â†“)\n- ê³ ê° ë¶ˆë§Œ ì ‘ìˆ˜: ì¦ê°€\n\n**ëª¨ë¸ ëª©ì :** ì´íƒˆ ê°€ëŠ¥ì„± ë†’ì€ ê³ ê° ì˜ˆì¸¡ â†’ í• ì¸ ì¿ í° ë°œì†¡\n\n**í•µì‹¬ ì§ˆë¬¸:** ëª¨ë¸ì´ í•™ìŠµí•œ ê²ƒì€ 'ì´íƒˆí•  ì‚¬ëŒ'ì¼ê¹Œìš”, 'ì¿ í°ì„ ë°›ì€ ì‚¬ëŒ'ì¼ê¹Œìš”?",
          "objectives": [
            "Action Contamination(ê°œì… ì˜¤ì—¼) ê°œë… ì´í•´",
            "ì˜ˆì¸¡(Prediction)ê³¼ ì¸ê³¼(Causation)ì˜ ì°¨ì´ êµ¬ë¶„",
            "Leaky Feature íƒì§€ ë° ì œê±°"
          ],
          "buggy_code": "# ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸ìš© í”¼ì²˜ ì •ì˜\nfeatures = [\n    'monthly_payment',        # ì›” ìš”ê¸ˆ\n    'usage_time',             # ì‚¬ìš© ì‹œê°„\n    'customer_service_call',  # ê³ ê° ì„¼í„° ìƒë‹´ ì´ë ¥\n    'retention_coupon_used',  # í•´ì§€ ë°©ì–´ ì¿ í° ì‚¬ìš© ì—¬ë¶€\n    'cancel_request_flag'     # í•´ì§€ ìƒë‹´ ì‹ ì²­ ì—¬ë¶€\n]\n\n# í•™ìŠµ ë°ì´í„°\n# - ê³¼ê±° 30ì¼ í–‰ë™ ë¡œê·¸\n# - ë¼ë²¨: í–¥í›„ 30ì¼ ë‚´ ì´íƒˆ ì—¬ë¶€\n\nmodel.fit(X[features], y)\nprint(f\"AUC: {roc_auc_score(y_val, model.predict_proba(X_val)[:, 1])}\")",
          "correct_code": "# ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸ìš© í”¼ì²˜ ì •ì˜\n# ì›ì¹™: ì˜ˆì¸¡ ì‹œì ì— ì•Œ ìˆ˜ ìˆê³ , ìš°ë¦¬ ê°œì…ê³¼ ë¬´ê´€í•œ ì •ë³´ë§Œ ì‚¬ìš©\nfeatures = [\n    'monthly_payment',        # ì›” ìš”ê¸ˆ\n    'usage_time',             # ì‚¬ìš© ì‹œê°„\n    'customer_service_call',  # ê³ ê° ì„¼í„° ìƒë‹´ ì´ë ¥\n    # 'retention_coupon_used', # âœ— ì œê±°: ìš°ë¦¬ê°€ ì˜ˆì¸¡ í›„ ìˆ˜í–‰í•  ì•¡ì…˜ (Action Contamination)\n    # 'cancel_request_flag'    # âœ— ì œê±°: ì´ë¯¸ ì´íƒˆ ì˜ì‚¬ í™•ì •ëœ ì§€í‘œ (Leaky Feature)\n]\n\n# ì£¼ì˜: AUCê°€ ë‚®ì•„ì§€ë”ë¼ë„ ì´ê²ƒì´ ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ë¥¼ ë‚¼ ìˆ˜ ìˆëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.\nmodel.fit(X[features], y)\nprint(f\"AUC: {roc_auc_score(y_val, model.predict_proba(X_val)[:, 1])}\")\n\n# ë” ë‚˜ì€ ì ‘ê·¼: Uplift Modeling\n# ëª©í‘œë¥¼ 'ì´íƒˆ ì˜ˆì¸¡'ì´ ì•„ë‹Œ 'ê°œì… íš¨ê³¼ ì˜ˆì¸¡'ìœ¼ë¡œ ì¬ì •ì˜\n# Ï„(x) = P(ì´íƒˆ|ì¿ í° ì—†ìŒ) - P(ì´íƒˆ|ì¿ í° ìˆìŒ)",
          "hints": [
            "retention_coupon_usedê°€ í”¼ì²˜ì— ìˆë‹¤ë©´, ëª¨ë¸ì€ 'ì¿ í°ì„ ë°›ìœ¼ë©´ ì•ˆ ë– ë‚œë‹¤'ëŠ” ìƒê´€ê´€ê³„ë§Œ í•™ìŠµí•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ìš°ë¦¬ê°€ ì•Œê³  ì‹¶ì€ ê±´ 'ëˆ„êµ¬ì—ê²Œ ì¿ í°ì„ ì¤˜ì•¼ íš¨ê³¼ê°€ ìˆëŠ”ê°€'ì…ë‹ˆë‹¤.",
            "cancel_request_flagê°€ 1ì¸ ê³ ê°ì€ ì´ë¯¸ ì´íƒˆ ì˜ì‚¬ë¥¼ ë°íŒ ê²ƒì…ë‹ˆë‹¤. ì´ê±¸ ì˜ˆì¸¡í•  í•„ìš”ê°€ ìˆì„ê¹Œìš”?",
            "ì´ ë¬¸ì œì˜ ë³¸ì§ˆ:\n  - ëª¨ë¸ì´ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ: P(ì´íƒˆ | features)\n  - ë¹„ì¦ˆë‹ˆìŠ¤ê°€ ì›í•˜ëŠ” ê²ƒ: P(ì´íƒˆ | ì¿ í° ì—†ìŒ) - P(ì´íƒˆ | ì¿ í° ìˆìŒ)\n  ì´ ë‘˜ì€ ì™„ì „íˆ ë‹¤ë¥¸ ì§ˆë¬¸ì…ë‹ˆë‹¤."
          ],
          "error_log": "=== ì˜¤í”„ë¼ì¸ í‰ê°€ ===\nAUC: 0.91 âœ“\nAccuracy: 0.88 âœ“\n\n=== ìº í˜ì¸ ê²°ê³¼ ===\nì¿ í° ë°œì†¡ ëŒ€ìƒ: ìƒìœ„ 20% ì´íƒˆ ì˜ˆì¸¡ ê³ ê°\nì „í™˜ìœ¨: 7% (ê¸°ì¡´ 12% ëŒ€ë¹„ í•˜ë½)\nê³ ê° í”¼ë“œë°±: \"ì™œ ê°‘ìê¸° ì¿ í°ì´ ì˜¤ì£ ? í•´ì§€í•  ìƒê° ì—†ì—ˆëŠ”ë°...\"\n\n[ë¶„ì„] ëª¨ë¸ì´ 'ì´ë¯¸ ì¿ í° ë°›ì€ ì‚¬ëŒ'ì„ ì˜ˆì¸¡í•¨",
          "success_log": "=== ì˜¤í”„ë¼ì¸ í‰ê°€ ===\nAUC: 0.75 (ë‚®ì•„ì§, í•˜ì§€ë§Œ ì •ìƒ)\nAccuracy: 0.72\n\n=== ìº í˜ì¸ ê²°ê³¼ ===\nì¿ í° ë°œì†¡ ëŒ€ìƒ: Uplift ìƒìœ„ 20% ê³ ê°\nì „í™˜ìœ¨: 18% (ê¸°ì¡´ 12% ëŒ€ë¹„ ìƒìŠ¹)\në¹„ìš© íš¨ìœ¨: 40% ê°œì„ \n\n[ì„±ê³µ] ê°œì… íš¨ê³¼ê°€ ë†’ì€ ê³ ê°ì—ê²Œë§Œ íƒ€ê²ŸíŒ…",
          "solution_check": {
            "type": "multi_condition",
            "forbidden": [
              "retention_coupon_used",
              "cancel_request_flag"
            ]
          },
          "error_info": {
            "type": "Problem Definition Error (Action Contamination & Data Leakage)",
            "description": "ë‘ ê°€ì§€ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤:\n\n1. **Action Contamination (ê°œì… ì˜¤ì—¼)**\n   retention_coupon_usedëŠ” 'ìš°ë¦¬ì˜ ê°œì…(Action)'ì…ë‹ˆë‹¤. ì´ê±¸ í”¼ì²˜ë¡œ ì“°ë©´ ëª¨ë¸ì€ ì¸ê³¼ê´€ê³„ê°€ ì•„ë‹Œ ìƒê´€ê´€ê³„ë§Œ í•™ìŠµí•©ë‹ˆë‹¤.\n   - í•™ìŠµ: \"ì¿ í° ì“´ ì‚¬ëŒ = ì•ˆ ë– ë‚¨\" (ìƒê´€ê´€ê³„)\n   - ì‹¤ì œ: \"ì¿ í° ë•Œë¬¸ì— ì•ˆ ë– ë‚¨\" vs \"ì›ë˜ ì•ˆ ë– ë‚  ì‚¬ëŒ\" êµ¬ë¶„ ë¶ˆê°€\n\n2. **Leaky Feature (ë¯¸ë˜ ì •ë³´ ëˆ„ìˆ˜)**\n   cancel_request_flagëŠ” ì´ë¯¸ ì´íƒˆ ì˜ì‚¬ê°€ í™•ì •ëœ ì‹œì ì˜ ì •ë³´ì…ë‹ˆë‹¤. ì˜ˆì¸¡ ì‹œì ì—ëŠ” ì•Œ ìˆ˜ ì—†ëŠ” ë¯¸ë˜ ì •ë³´ì…ë‹ˆë‹¤.\n\n**ê·¼ë³¸ ì›ì¸:** ì˜ˆì¸¡(Prediction) ë¬¸ì œì™€ ì¸ê³¼(Causal) ë¬¸ì œë¥¼ í˜¼ë™í•¨.\n- ëª¨ë¸ì´ ë‹µí•˜ëŠ” ì§ˆë¬¸: \"ì´ ê³ ê°ì´ ì´íƒˆí• ê¹Œ?\"\n- ë¹„ì¦ˆë‹ˆìŠ¤ê°€ ì›í•˜ëŠ” ì§ˆë¬¸: \"ì´ ê³ ê°ì—ê²Œ ì¿ í°ì„ ì£¼ë©´ ì´íƒˆì„ ë§‰ì„ ìˆ˜ ìˆì„ê¹Œ?\"",
            "suggestion": "1. ì˜ˆì¸¡ ì‹œì ì— ì•Œ ìˆ˜ ì—†ê±°ë‚˜, ì˜ˆì¸¡ ê²°ê³¼ë¡œ ë°œìƒí•˜ëŠ” ì•¡ì…˜ ê´€ë ¨ í”¼ì²˜ëŠ” ì œê±°í•˜ì„¸ìš”.\n2. ë” ë‚˜ì€ ì ‘ê·¼: ë¬¸ì œë¥¼ 'Uplift Modeling'ìœ¼ë¡œ ì¬ì •ì˜í•˜ì„¸ìš”. ê°œì… íš¨ê³¼(Treatment Effect)ë¥¼ ì§ì ‘ ëª¨ë¸ë§í•˜ë©´ 'ì¿ í°ì´ íš¨ê³¼ ìˆëŠ” ê³ ê°'ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
          }
        }
      ]
    },
    {
      "id": "P2",
      "project_title": "ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ê¸°ë³¸ ë°©ì–´",
      "scenario": "ì‚¬ìš©ì í–‰ë™ ë¡œê·¸ë¡œ ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì´ ì¢…ì¢… ì˜ˆì™¸ë¡œ í„°ì§€ê±°ë‚˜, í•™ìŠµì´ 'ë˜ëŠ” ê²ƒì²˜ëŸ¼' ë³´ì´ì§€ë§Œ ê²°ê³¼ê°€ ë¶ˆì•ˆì •í•©ë‹ˆë‹¤.",
      "difficulty": 2,
      "totalSteps": 3,
      "steps": [
        {
          "step": 1,
          "title": "ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ëˆ„ë½ (NaN Handling)",
          "bug_type": "A",
          "bug_type_name": "Null Handling",
          "questions": {
            "text": "ëª¨ë¸ í•™ìŠµ ì§ì „ì— 'Input contains NaN' ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ì›ì¸ì€?",
            "options": [
              "í•™ìŠµ ë°ì´í„°ê°€ ë„ˆë¬´ ì»¤ì„œ",
              "ì „ì²˜ë¦¬ ê³¼ì •ì—ì„œ ê²°ì¸¡ì¹˜(NaN)ê°€ ë‚¨ì•„ ìˆì–´ì„œ",
              "ëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœí•´ì„œ",
              "ì •ê·œí™”ë¥¼ í•´ì„œ"
            ],
            "answer": 1
          },
          "buggy_code": "import pandas as pd\n\ndef preprocess(df):\n    df = df.copy()\n    df['age'] = df['age'].astype(int)\n    return df",
          "correct_code": "import pandas as pd\n\ndef preprocess(df):\n    df = df.copy()\n    df['age'] = df['age'].fillna(0).astype(int)\n    return df",
          "hint": "NaNì´ ìˆìœ¼ë©´ astype()ì´ ì‹¤íŒ¨í•©ë‹ˆë‹¤. ê²°ì¸¡ì¹˜ë¥¼ ì²˜ë¦¬í•˜ì„¸ìš”.",
          "solution_check": {
            "type": "regex",
            "value": "(fillna\\(\\s*(0)?\\s*\\)|dropna\\(\\s*\\))",
            "flags": ""
          },
          "coaching": "ğŸ¯ í˜„ì—…: ì‹¤ë¬´ ë°ì´í„°ëŠ” í•­ìƒ ê²°ì¸¡ì¹˜ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. fillna(0), fillna(í‰ê· ), fillna(ì¤‘ì•™ê°’) ì¤‘ ë„ë©”ì¸ì— ë§ëŠ” ì „ëµì„ ì„ íƒí•˜ì„¸ìš”."
        },
        {
          "step": 2,
          "title": "0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì–´ (Zero Division Guard)",
          "bug_type": "B",
          "bug_type_name": "Null Guard",
          "questions": {
            "text": "ì‚¬ìš©ìë³„ í‰ê·  êµ¬ë§¤ì•¡ ê³„ì‚° ì‹œ 'division by zero' ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì–´ë–¤ ìƒí™©ì¼ê¹Œìš”?",
            "options": [
              "êµ¬ë§¤ì•¡ì´ ë„ˆë¬´ ì»¤ì„œ",
              "íŠ¹ì • ì‚¬ìš©ìì˜ êµ¬ë§¤ íšŸìˆ˜(count)ê°€ 0ì´ì–´ì„œ",
              "CSV ì¸ì½”ë”©ì´ ê¹¨ì ¸ì„œ",
              "ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ì„œ"
            ],
            "answer": 1
          },
          "buggy_code": "def avg_spend(total_spend, count):\n    return total_spend / count",
          "correct_code": "def avg_spend(total_spend, count):\n    if count == 0:\n        return 0\n    return total_spend / count",
          "hint": "countê°€ 0ì¼ ë•Œë¥¼ ë°©ì–´í•´ì•¼ í•©ë‹ˆë‹¤.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "if count == 0"
            ],
            "forbidden": []
          },
          "coaching": "ğŸ¯ í˜„ì—…: ì‹ ê·œ ê°€ì…ìë‚˜ ë¹„í™œì„± ì‚¬ìš©ìëŠ” countê°€ 0ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°©ì–´ ì½”ë“œëŠ” ì„ íƒì´ ì•„ë‹ˆë¼ í•„ìˆ˜ì…ë‹ˆë‹¤."
        },
        {
          "step": 3,
          "title": "ì›ë³¸ ë°ì´í„° ë³´ì¡´ (State Leak)",
          "bug_type": "C",
          "bug_type_name": "State Leak",
          "questions": {
            "text": "ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•œ ë’¤ ì›ë³¸ ë°ì´í„°í”„ë ˆì„ê¹Œì§€ í•¨ê»˜ ë°”ë€Œì—ˆìŠµë‹ˆë‹¤. ì™œì¼ê¹Œìš”?",
            "options": [
              "ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•´ì„œ",
              "ë°ì´í„°í”„ë ˆì„ì´ ì°¸ì¡°ë¡œ ì „ë‹¬ë˜ì–´ ì›ë³¸ì´ ìˆ˜ì •ë˜ì—ˆê¸° ë•Œë¬¸ì—",
              "Pandas ë²„ê·¸ë¼ì„œ",
              "ì „ì—­ ë³€ìˆ˜ê°€ ìë™ ìƒì„±ë˜ì–´ì„œ"
            ],
            "answer": 1
          },
          "buggy_code": "def add_feature(df):\n    df['is_vip'] = df['spend'] > 100\n    return df",
          "correct_code": "def add_feature(df):\n    df = df.copy()\n    df['is_vip'] = df['spend'] > 100\n    return df",
          "hint": "DataFrameì€ ì°¸ì¡°ë¡œ ì „ë‹¬ë˜ë¯€ë¡œ ì›ë³¸ì´ ìˆ˜ì •ë©ë‹ˆë‹¤. ë³µì‚¬ë³¸ì„ ë§Œë“¤ì–´ ì‘ì—…í•˜ì„¸ìš”.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              ".copy()"
            ],
            "forbidden": [
              "inplace=True"
            ]
          },
          "coaching": "ğŸ¯ í˜„ì—…: State LeakëŠ” ì¬í˜„ ë¶ˆê°€ëŠ¥í•œ ë²„ê·¸ì˜ ì£¼ìš” ì›ì¸ì…ë‹ˆë‹¤. í•¨ìˆ˜ëŠ” ì…ë ¥ì„ ë³€ê²½í•˜ì§€ ì•Šê³  ìƒˆë¡œìš´ ê°ì²´ë¥¼ ë°˜í™˜í•˜ì„¸ìš”."
        }
      ]
    },
    {
      "id": "P3",
      "project_title": "í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹ ë¢°ì„± í™•ë³´",
      "scenario": "ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸ì„ ë°°í¬í•˜ê¸° ì „ ì„±ëŠ¥ ë¦¬í¬íŠ¸ë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ°ë° ë¦¬í¬íŠ¸ ìˆ˜ì¹˜ê°€ ê³¼ë„í•˜ê²Œ ì¢‹ê±°ë‚˜, ì‹¤í–‰í•  ë•Œë§ˆë‹¤ ê²°ê³¼ê°€ ë‹¬ë¼ì§‘ë‹ˆë‹¤.",
      "difficulty": 3,
      "totalSteps": 3,
      "steps": [
        {
          "step": 1,
          "title": "ë¶ˆê· í˜• ë°ì´í„° ë¶„í•  ì˜¤ë¥˜ (Stratify ëˆ„ë½)",
          "bug_type": "A",
          "bug_type_name": "Sampling Bug",
          "questions": {
            "text": "ì´íƒˆë¥  5%ì¸ ë°ì´í„°ì—ì„œ ë§¤ë²ˆ í‰ê°€ ê²°ê³¼ê°€ í¬ê²Œ í”ë“¤ë¦½ë‹ˆë‹¤. ê°€ì¥ ë¨¼ì € ì˜ì‹¬í•´ì•¼ í•  ê²ƒì€?",
            "options": [
              "ëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœí•´ì„œ",
              "train/test ë¶„í•  ì‹œ í´ë˜ìŠ¤ ë¹„ìœ¨ì´ ê¹¨ì¡Œì„ ê°€ëŠ¥ì„±",
              "ì „ì²˜ë¦¬ ì‹œê°„ì´ ë„ˆë¬´ ê¸¸ì–´ì„œ",
              "íŠ¹ì„±ì´ ë„ˆë¬´ ë§ì•„ì„œ"
            ],
            "answer": 1
          },
          "buggy_code": "from sklearn.model_selection import train_test_split\n\ndef split(df):\n    X = df.drop('churn', axis=1)\n    y = df['churn']\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    return X_train, X_test, y_train, y_test",
          "correct_code": "from sklearn.model_selection import train_test_split\n\ndef split(df):\n    X = df.drop('churn', axis=1)\n    y = df['churn']\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y\n    )\n    return X_train, X_test, y_train, y_test",
          "hint": "ë°ì´í„°ê°€ ë¶ˆê· í˜•í•©ë‹ˆë‹¤. train_test_splitì˜ íŠ¹ì • íŒŒë¼ë¯¸í„°ë¥¼ í™œìš©í•´ y í´ë˜ìŠ¤ ë¹„ìœ¨ì„ ìœ ì§€í•˜ì„¸ìš”.",
          "solution_check": {
            "type": "regex",
            "value": "stratify\\s*=\\s*y\\s*(?=,\\s*\\w|\\))",
            "flags": ""
          },
          "coaching": "ğŸ¯ í˜„ì—…: ë¶ˆê· í˜• ë°ì´í„°(ì‚¬ê¸° íƒì§€, ì´íƒˆ ì˜ˆì¸¡)ì—ì„œ stratify ì—†ì´ splití•˜ë©´ testì— positive ìƒ˜í”Œì´ ê±°ì˜ ì—†ì–´ í‰ê°€ê°€ ë¬´ì˜ë¯¸í•´ì§‘ë‹ˆë‹¤."
        },
        {
          "step": 2,
          "title": "ì¬í˜„ì„±(Seed) ëˆ„ë½",
          "bug_type": "B",
          "bug_type_name": "Non-determinism",
          "questions": {
            "text": "ê°™ì€ ì½”ë“œë¡œ í•™ìŠµí–ˆëŠ”ë° ì‹¤í–‰í•  ë•Œë§ˆë‹¤ ì ìˆ˜ê°€ ë‹¤ë¦…ë‹ˆë‹¤. ê°€ì¥ ê·¸ëŸ´ë“¯í•œ ì›ì¸ì€?",
            "options": [
              "CPU ì„±ëŠ¥ì´ ë‹¬ë¼ì„œ",
              "ë‚œìˆ˜ ì‹œë“œê°€ ê³ ì •ë˜ì§€ ì•Šì•„ì„œ",
              "ë°ì´í„°ê°€ ë„ˆë¬´ ë§ì•„ì„œ",
              "ëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœí•´ì„œ"
            ],
            "answer": 1
          },
          "buggy_code": "import numpy as np\nimport random\n\ndef train(model, X_train, y_train):\n    random.seed(42)\n    model.fit(X_train, y_train)\n    return model",
          "correct_code": "import numpy as np\nimport random\n\ndef train(model, X_train, y_train):\n    random.seed(42)\n    np.random.seed(42)\n    model.fit(X_train, y_train)\n    return model",
          "hint": "ê²°ê³¼ë¥¼ ì¬í˜„í•˜ë ¤ë©´ Numpyì˜ ë‚œìˆ˜ ì‹œë“œë¥¼ í•™ìŠµ ì „ì— ê³ ì •í•˜ì„¸ìš”.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "np.random.seed(",
              "random.seed("
            ],
            "forbidden": []
          },
          "coaching": "ğŸ¯ í˜„ì—…: ì¬í˜„ ë¶ˆê°€ëŠ¥í•˜ë©´ A/B í…ŒìŠ¤íŠ¸, ë””ë²„ê¹…, í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ ëª¨ë‘ ë¬´ì˜ë¯¸í•´ì§‘ë‹ˆë‹¤. ì‹œë“œ ê³ ì •ì€ í•„ìˆ˜ì…ë‹ˆë‹¤."
        },
        {
          "step": 3,
          "title": "ë¶ˆê· í˜• ë°ì´í„° ì§€í‘œ ì„ íƒ ì˜¤ë¥˜ (Accuracy í•¨ì •)",
          "bug_type": "C",
          "bug_type_name": "Metric Choice",
          "questions": {
            "text": "ì´íƒˆë¥  5%ì¸ ë°ì´í„°ì—ì„œ accuracy 95%ê°€ ë‚˜ì™”ì§€ë§Œ ì´íƒˆ ê³ ê°ì„ í•˜ë‚˜ë„ ëª» ì¡ìŠµë‹ˆë‹¤. ì´ìœ ëŠ”?",
            "options": [
              "accuracyê°€ ë¶ˆê· í˜• ë°ì´í„°ì—ì„œ ë‹¤ìˆ˜ í´ë˜ìŠ¤ì— í¸í–¥ë˜ì–´ ì˜¤í•´ë¥¼ ë§Œë“¤ ìˆ˜ ìˆì–´ì„œ",
              "ëª¨ë¸ì´ ë„ˆë¬´ ë³µì¡í•´ì„œ",
              "ë°ì´í„°ê°€ ë„ˆë¬´ ì»¤ì„œ",
              "ì „ì²˜ë¦¬ë¥¼ í•´ì„œ"
            ],
            "answer": 0
          },
          "buggy_code": "from sklearn.metrics import accuracy_score\n\ndef evaluate(model, X_test, y_test):\n    y_pred = model.predict(X_test)\n    return accuracy_score(y_test, y_pred)",
          "correct_code": "from sklearn.metrics import f1_score\n\ndef evaluate(model, X_test, y_test):\n    y_pred = model.predict(X_test)\n    return f1_score(y_test, y_pred)",
          "hint": "ë¶ˆê· í˜• ë°ì´í„°ì—ì„œëŠ” accuracy ëŒ€ì‹  ì¬í˜„ìœ¨ê³¼ ì •ë°€ë„ë¥¼ ëª¨ë‘ ê³ ë ¤í•˜ëŠ” ì§€í‘œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [],
            "required_any": [
              "f1_score"
            ],
            "forbidden": [
              "accuracy_score"
            ]
          },
          "coaching": "ğŸ¯ í˜„ì—…: ëª¨ë“  ê²ƒì„ 'ìœ ì§€'ë¡œ ì˜ˆì¸¡í•´ë„ 95% accuracyê°€ ë‚˜ì˜µë‹ˆë‹¤. F1-scoreëŠ” recallê³¼ precisionê³¼ ì¡°í™”í‰ê· ìœ¼ë¡œ ë¶ˆê· í˜• ë°ì´í„°ì— ì í•©í•©ë‹ˆë‹¤."
        }
      ]
    },
    {
      "id": "P4",
      "project_title": "ë°°í¬ ì „ ë§ˆì§€ë§‰ í•¨ì • ì œê±°",
      "scenario": "ëª¨ë¸ì€ ì¶©ë¶„íˆ í•™ìŠµë˜ì—ˆê³  í‰ê°€ë„ í†µê³¼í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë°°í¬ ì§ì „ ì ê²€ì—ì„œ ìš´ì˜ í™˜ê²½ì—ì„œë§Œ í„°ì§€ëŠ” ë¬¸ì œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.",
      "difficulty": 4,
      "totalSteps": 3,
      "steps": [
        {
          "step": 1,
          "title": "í•™ìŠµ/ì„œë¹™ í”¼ì²˜ ë¶ˆì¼ì¹˜ (Feature Mismatch)",
          "bug_type": "A",
          "bug_type_name": "Schema Drift",
          "questions": {
            "text": "ì˜¤í”„ë¼ì¸ì—ì„œëŠ” í†µê³¼í–ˆëŠ”ë° ìš´ì˜ì—ì„œ 'Feature names mismatch' ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì›ì¸ì€?",
            "options": [
              "ëª¨ë¸ì´ ë„ˆë¬´ ì»¤ì„œ",
              "í•™ìŠµ ì‹œ ì‚¬ìš©í•œ í”¼ì²˜ ì»¬ëŸ¼ê³¼ ì„œë¹™ ì…ë ¥ ì»¬ëŸ¼ì˜ ìˆœì„œë‚˜ ê°œìˆ˜ê°€ ë‹¬ë¼ì„œ",
              "ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ì„œ",
              "ë‚œìˆ˜ ì‹œë“œë¥¼ ê³ ì •í•´ì„œ"
            ],
            "answer": 1
          },
          "buggy_code": "def serve(model, input_df):\n    return model.predict(input_df)",
          "correct_code": "def serve(model, input_df, feature_cols):\n    X = input_df[feature_cols]\n    return model.predict(X)",
          "hint": "í•™ìŠµ ì‹œ ì‚¬ìš©í•œ feature ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ì €ì¥í•´ë‘ê³ , ì„œë¹™ ì‹œ ê·¸ ì»¬ëŸ¼ë§Œ ì„ íƒí•´ì„œ ì‚¬ìš©í•˜ì„¸ìš”.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "input_df[feature_cols]"
            ],
            "forbidden": []
          },
          "coaching": "ğŸ¯ í˜„ì—…: Train-Serve SkewëŠ” ML ì‹œìŠ¤í…œ ì¥ì• ì˜ 70% ì´ìƒì„ ì°¨ì§€í•©ë‹ˆë‹¤. Feature ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ ë©”íƒ€ë°ì´í„°ë¡œ ì €ì¥í•˜ì„¸ìš”."
        },
        {
          "step": 2,
          "title": "ìƒˆë¡œìš´ ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ëˆ„ë½ (Unseen Category)",
          "bug_type": "B",
          "bug_type_name": "Unseen Category",
          "questions": {
            "text": "ìš´ì˜ ë°ì´í„°ì— í•™ìŠµ ë•Œ ì—†ë˜ ì‹ ê·œ êµ­ê°€ ì½”ë“œê°€ ë“¤ì–´ì˜¤ë©´ì„œ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì–´ë–¤ ë¬¸ì œì¼ê¹Œìš”?",
            "options": [
              "ì†ë„ê°€ ë¹¨ë¼ì§„ë‹¤",
              "ì¸ì½”ë”ê°€ í•™ìŠµ ì‹œ ë³´ì§€ ëª»í•œ ì¹´í…Œê³ ë¦¬ë¥¼ ì²˜ë¦¬í•˜ì§€ ëª»í•´ ì—ëŸ¬ë¥¼ ë‚¸ë‹¤",
              "ë©”ëª¨ë¦¬ê°€ ì¤„ì–´ë“ ë‹¤",
              "ì •í™•ë„ê°€ ìë™ìœ¼ë¡œ ì˜¬ë¼ê°„ë‹¤"
            ],
            "answer": 1
          },
          "buggy_code": "from sklearn.preprocessing import OneHotEncoder\n\ndef fit_encoder(train_cat):\n    enc = OneHotEncoder()\n    enc.fit(train_cat)\n    return enc",
          "correct_code": "from sklearn.preprocessing import OneHotEncoder\n\ndef fit_encoder(train_cat):\n    enc = OneHotEncoder(handle_unknown='ignore')\n    enc.fit(train_cat)\n    return enc",
          "hint": "OneHotEncoderì— handle_unknown='_ _ _ _ _ _' ì˜µì…˜ì„ ì„¤ì •í•˜ì„¸ìš”.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "handle_unknown='ignore'"
            ],
            "forbidden": []
          },
          "coaching": "ğŸ¯ í˜„ì—…: ìš´ì˜ ë°ì´í„°ëŠ” í•­ìƒ í•™ìŠµ ë°ì´í„°ë³´ë‹¤ ë‹¤ì–‘í•©ë‹ˆë‹¤. ìƒˆ ìƒí’ˆ, ì‹ ê·œ êµ­ê°€ ë“±ì´ ê³„ì† ì¶”ê°€ë˜ë¯€ë¡œ ë°©ì–´ ì½”ë“œê°€ í•„ìˆ˜ì…ë‹ˆë‹¤."
        },
        {
          "step": 3,
          "title": "ì „ì²˜ë¦¬ê¸°/ëª¨ë¸ ë²ˆë“¤ ì €ì¥ ëˆ„ë½",
          "bug_type": "C",
          "bug_type_name": "Artifact Drift",
          "questions": {
            "text": "ëª¨ë¸ì„ ì¬ë°°í¬í–ˆëŠ”ë° ì„±ëŠ¥ì´ ê°‘ìê¸° ë‚˜ë¹ ì¡ŒìŠµë‹ˆë‹¤. ëª¨ë¸ ì½”ë“œëŠ” ì•ˆ ë°”ê¿¨ëŠ”ë° ì™œì¼ê¹Œìš”?",
            "options": [
              "ëª¨ë¸ì´ ìŠ¤ìŠ¤ë¡œ ë³€í•´ì„œ",
              "ì „ì²˜ë¦¬ê¸°ê°€ ì¬í•™ìŠµë˜ì–´ ì´ì „ê³¼ ë‹¤ë¥¸ ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜í•˜ê¸° ë•Œë¬¸ì—",
              "CPUê°€ ëŠë ¤ì ¸ì„œ",
              "ë°ì´í„°ê°€ ì¤„ì–´ì„œ"
            ],
            "answer": 1
          },
          "buggy_code": "import joblib\n\ndef save_model(model, scaler, encoder, path):\n    joblib.dump(model, path)\n    return path",
          "correct_code": "import joblib\n\ndef save_model(model, scaler, encoder, path):\n    bundle = {\n        'model': model,\n        'scaler': scaler,\n        'encoder': encoder\n    }\n    joblib.dump(bundle, path)\n    return path",
          "hint": "ëª¨ë¸ë¿ ì•„ë‹ˆë¼ scaler, encoderë„ í•¨ê»˜ ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤. ë”•ì…”ë„ˆë¦¬ë¡œ ë¬¶ì–´ì„œ bundle í˜•íƒœë¡œ ë§Œë“  í›„ dumpí•˜ì„¸ìš”.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "'model':",
              "'scaler':",
              "'encoder':"
            ],
            "forbidden": []
          },
          "coaching": "ğŸ¯ í˜„ì—…: ëª¨ë¸ë§Œ ì €ì¥í•˜ë©´ ì „ì²˜ë¦¬ ê¸°ì¤€ì´ ë‹¬ë¼ì ¸ì„œ ì„±ëŠ¥ì´ ë¬´ë„ˆì§‘ë‹ˆë‹¤. ëª¨ë¸+ì „ì²˜ë¦¬ê¸°ë¥¼ í•˜ë‚˜ì˜ ë²ˆë“¤ë¡œ ê´€ë¦¬í•˜ì„¸ìš”."
        }
      ]
    },
    {
      "id": "P5",
      "project_title": "ì •í™”ê°€ ì‹œê¸‰í•œ ì˜¤ì—¼ëœ ë°ì´í„° ì„¹í„°",
      "scenario": "ë§ˆë” ì„œë²„ì˜ íŠ¹ì • ì„¹í„°ì—ì„œ ì •í™•ë„ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ë†’ê²Œ ë³´ê³ ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì „í˜•ì ì¸ 'ë°ì´í„° ì˜¤ì—¼(Hallucination)' ì¦ìƒì…ë‹ˆë‹¤. {username}ë‹˜, íŒŒíŠ¸ë„ˆ Coduckê³¼ í•¨ê»˜ ì˜¤ì—¼ëœ íŒŒì´í”„ë¼ì¸ì˜ ë²„ê·¸ë¥¼ ìˆœì„œëŒ€ë¡œ ì°¾ì•„ ì •í™”í•´ì•¼ í•©ë‹ˆë‹¤.",
      "difficulty": 1,
      "totalSteps": 3,
      "steps": [
        {
          "step": 1,
          "title": "ë¹„ì •ìƒì ìœ¼ë¡œ ë†’ì€ ê²€ì¦ ì„±ëŠ¥ (Data Leakage)",
          "bug_type": "A",
          "bug_type_name": "Data Leakage",
          "questions": {
            "text": "ì „ì²˜ë¦¬ ì½”ë“œì—ì„œ ì´ìƒí•œ ë¶€ë¶„ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤:\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, ...)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X)  \nX_test = scaler.transform(X_test)\n\nTrain 0.98 vs Test 0.65ë¡œ ì°¨ì´ê°€ í½ë‹ˆë‹¤. ë¬´ì—‡ì´ ë¬¸ì œì¼ê¹Œìš”?",
            "options": [
              "scalerë¥¼ ì „ì²´ ë°ì´í„° Xë¡œ fití•´ì„œ",
              "train_test_splitì˜ test_sizeê°€ ë„ˆë¬´ ì‘ì•„ì„œ",
              "StandardScaler ëŒ€ì‹  MinMaxScalerë¥¼ ì¨ì•¼ í•´ì„œ",
              "random_stateë¥¼ ê³ ì •í•˜ì§€ ì•Šì•„ì„œ"
            ],
            "answer": 0
          },
          "buggy_code": "from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\ndef prepare_data(df):\n    X = df.drop('churn', axis=1)\n    y = df['churn']\n    \n    X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n    )\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X)\n    X_test = scaler.transform(X_test)\n    \n    return X_train, X_test, y_train, y_test",
          "correct_code": "from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\ndef prepare_data(df):\n    X = df.drop('churn', axis=1)\n    y = df['churn']\n    \n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n    \n    return X_train, X_test, y_train, y_test",
          "hint": "scalerê°€ ì „ì²´ ë°ì´í„° Xë¡œ fití•˜ë©´, X_testì˜ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ì •ë³´ë„ í•¨ê»˜ í•™ìŠµë©ë‹ˆë‹¤. ì´ê²Œ ì™œ ë¬¸ì œì¼ê¹Œìš”?",
          "solution_check": {
            "type": "multi_condition",
            "required_any": [
              "fit_transform(X_train)",
              "fit(X_train)"
            ],
            "forbidden": [
              "fit_transform(X)",
              "fit(X)"
            ]
          },
          "coaching": "ğŸ¯ í˜„ì—…: Data LeakageëŠ” ëª¨ë¸ì´ 'ë„ˆë¬´ ì˜ ë‚˜ì˜¬ ë•Œ' ì˜ì‹¬í•´ì•¼ í•˜ëŠ” 1ìˆœìœ„ ë²„ê·¸ì…ë‹ˆë‹¤. ì „ì²˜ë¦¬ëŠ” ë°˜ë“œì‹œ train/test ë¶„í•  í›„ì— trainìœ¼ë¡œë§Œ fití•˜ê³ , testëŠ” transformë§Œ í•´ì•¼ í•©ë‹ˆë‹¤."
        },
        {
          "step": 2,
          "title": "ë¶ˆì•ˆì •í•œ í‰ê°€ ê²°ê³¼ (Imbalanced Sampling)",
          "bug_type": "B",
          "bug_type_name": "Sampling Bug",
          "questions": {
            "text": "Step 1ì„ ê³ ì³¤ë”ë‹ˆ test ì •í™•ë„ê°€ 0.85ë¡œ ë‚´ë ¤ê°”ìŠµë‹ˆë‹¤.\n\nê·¸ëŸ°ë° í˜„ì¬ splitì˜ test setì„ í™•ì¸í•´ë³´ë‹ˆ ì´íƒˆë¥ ì´ 3% ì •ë„ì…ë‹ˆë‹¤. ì›ë˜ ì „ì²´ ë°ì´í„°ì˜ ì´íƒˆë¥ ì€ 5%ì¸ë°, test setì˜ ë¹„ìœ¨ì´ ë‹¬ë¼ì¡ŒìŠµë‹ˆë‹¤.\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42)\në¬´ì—‡ì„ ì˜ì‹¬í•´ì•¼ í• ê¹Œìš”?",
            "options": [
              "random_state ê°’ì´ ì˜ëª»ë¨",
              "test_sizeë¥¼ 0.3 ì´ìƒìœ¼ë¡œ",
              "train/testì˜ í´ë˜ìŠ¤ ë¹„ìœ¨ì´ ë‹¬ë¼ì„œ",
              "shuffle=Falseë¡œ ë³€ê²½"
            ],
            "answer": 2
          },
          "buggy_code": "from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\ndef prepare_data(df):\n    X = df.drop('churn', axis=1)\n    y = df['churn']\n    \n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n    \n    return X_train, X_test, y_train, y_test",
          "correct_code": "from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\ndef prepare_data(df):\n    X = df.drop('churn', axis=1)\n    y = df['churn']\n    \n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y\n    )\n    \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n    \n    return X_train, X_test, y_train, y_test",
          "hint": "ë¶ˆê· í˜• ë°ì´í„°ì—ì„  split í›„ train/testì˜ í´ë˜ìŠ¤ ë¹„ìœ¨ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ê°•ì œë¡œ ìœ ì§€í•˜ëŠ” ì˜µì…˜ì´ ìˆìŠµë‹ˆë‹¤.",
          "solution_check": {
            "type": "regex",
            "value": "stratify\\s*=\\s*y\\s*(?=,\\s*\\w|\\))",
            "flags": ""
          },
          "coaching": "ğŸ¯ í˜„ì—…: ë¶ˆê· í˜• ë°ì´í„°(ì´íƒˆ ì˜ˆì¸¡, ì‚¬ê¸° íƒì§€ ë“±)ì—ì„œ stratify ì—†ì´ splití•˜ë©´ train/testì˜ í´ë˜ìŠ¤ ë¹„ìœ¨ì´ ë‹¬ë¼ì ¸ í‰ê°€ê°€ ë¶ˆì•ˆì •í•´ì§‘ë‹ˆë‹¤. íŠ¹íˆ testì— positive ìƒ˜í”Œì´ ì ìœ¼ë©´ í‰ê°€ ìì²´ê°€ ë¬´ì˜ë¯¸í•´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        },
        {
          "step": 3,
          "title": "ì˜ëª»ëœ í‰ê°€ ë°ì´í„° ì‚¬ìš© (Train Data Evaluation)",
          "bug_type": "C",
          "bug_type_name": "Evaluation Error",
          "questions": {
            "text": "Step 1, 2ë¥¼ ê³ ì³¤ìŠµë‹ˆë‹¤. ì´ì œ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  í‰ê°€í–ˆìŠµë‹ˆë‹¤:\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_train) \naccuracy = accuracy_score(y_train, y_pred)\nprint(f'ì •í™•ë„: {accuracy}')  # 0.95\n\nê·¸ëŸ°ë° testë¡œ í™•ì¸í•˜ë‹ˆ 0.60ì´ ë‚˜ì˜µë‹ˆë‹¤. ë¬´ì—‡ì´ ë¬¸ì œì¼ê¹Œìš”?",
            "options": [
              "accuracy_score ëŒ€ì‹  f1_scoreë¥¼ ì¨ì•¼ í•¨",
              "train ë°ì´í„°ë¡œ í‰ê°€í•´ì„œ ì„±ëŠ¥ì´ ê³¼ëŒ€í‰ê°€ë¨",
              "predict ëŒ€ì‹  predict_probaë¥¼ ì¨ì•¼ í•¨",
              "y_trainê³¼ y_pred ìˆœì„œê°€ ë°”ë€œ"
            ],
            "answer": 1
          },
          "buggy_code": "from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\ndef train_and_evaluate(df):\n    X = df.drop('churn', axis=1)\n    y = df['churn']\n    \n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y\n    )\n    \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n    \n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    \n    y_pred = model.predict(X_train)\n    accuracy = accuracy_score(y_train, y_pred)\n    \n    return accuracy",
          "correct_code": "from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\ndef train_and_evaluate(df):\n    X = df.drop('churn', axis=1)\n    y = df['churn']\n    \n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y\n    )\n    \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n    \n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    \n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    return accuracy",
          "hint": "í•™ìŠµì— ì‚¬ìš©í•œ ë°ì´í„°ë¡œ í‰ê°€í•˜ë©´ ëª¨ë¸ ì„±ëŠ¥ì´ ê³¼ëŒ€í‰ê°€ë©ë‹ˆë‹¤. ëª¨ë¸ì´ í•™ìŠµ ì¤‘ ë³´ì§€ ëª»í•œ ë°ì´í„°ë¡œ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤.",
          "solution_check": {
            "type": "multi_condition",
            "required_all": [
              "predict(X_test)",
              "accuracy_score(y_test"
            ],
            "forbidden": [
              "predict(X_train)",
              "accuracy_score(y_train, y_pred)"
            ]
          },
          "coaching": "ğŸ¯ í˜„ì—…: train ë°ì´í„°ë¡œ í‰ê°€í•˜ë©´ ëª¨ë¸ì´ 'ì´ë¯¸ ë³¸' ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥ë§Œ ì¸¡ì •ë˜ì–´ ê³¼ëŒ€í‰ê°€ë©ë‹ˆë‹¤. ë°˜ë“œì‹œ í•™ìŠµì— ì‚¬ìš©í•˜ì§€ ì•Šì€ test ë°ì´í„°ë¡œ í‰ê°€í•´ì•¼ ì‹¤ì œ ì„±ëŠ¥ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì´ train/test splitì„ í•˜ëŠ” ì´ìœ ì…ë‹ˆë‹¤."
        }
      ]
    }
  ]
}